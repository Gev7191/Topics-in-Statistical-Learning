---
title: "TSL Project"
author: "L. Insolia, J. Kim and G. Yeghikyan"    
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_notebook:
    # df_print: paged
    toc: yes
    toc_depth: '3' # up to three depths of headings (specified by #, ## and ###)
    highlight: tango
    keep_tex: yes
    number_sections: yes
    # theme: united
  # pdf document:
  editor_options:
    # toc: yes
    # toc_depth: 3
    chunk_output_type: inline
---


# General information #

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

We analyze a dataset published on [Kaggle](https://www.kaggle.com/etiennelq/french-employment-by-town).
It refers to french employment, salaries, population per town. 
The aim is to evaluate equality/inequalities in France, and geographical distribution of business according to their size.

Such data are collected by the INSEE.
Information regarding the number of firms in every french town, categorized by size
can be found [here](https://www.insee.fr/fr/metadonnees/definition/c1135). 
Information about salaries around french town per job categories, age and sex (expressed in average net amount per hour in euro) can be found [here](https://www.insee.fr/fr/statistiques/2522515).
Demographic information in France per town, age, sex and living mode
can be found [here](https://www.insee.fr/fr/statistiques/2863607).

Additional info about Population Data can be found [here](https://www.insee.fr/fr/statistiques/2863607#dictionnaire), 
it allows to add cat√©gorie socioprofessionnelle.




## Aim of the study ##
This project aims to explore existing patterns among French towns.

In particular, we are interested in:

* evaluating possible ineqialities: per towns/region, sex, age, etc.;
* predicting the ... using a regression model;
* reduce the dimensionality of ... performing a PCA;
* explore different algorithms to cluster male/females using ...



# General pre-processing phase #

Choose the working directory according to the user path: [NEEDED? probably not, delete it]
```{r}
# Jisu
# setwd("/Users/jisukim/TSL")
# Luca
# setwd("D:/Docs_D/PhD SNS/Classes/Topics in Statistical Learning/Project/git_TSL")
# Gevorg
# setwd("C:/Users/Gevorg/Desktop/TSL")
```

Import data:
```{r warning=FALSE}
# options(encoding = "UTF-8")  # for Mac [PROBABLY, to check]
# options(encoding = "ISO-8859-1")  # for Windows [PROBABLY NOT WORKING]
setwd("./data")
firms       <- read.csv("base_etablissement_par_tranche_effectif.csv", encoding = "UTF-8")
geo         <- read.csv("name_geographic_information.csv", encoding = "UTF-8")
salary      <- read.csv("net_salary_per_town_categories.csv", encoding = "UTF-8")
population  <- read.csv("population.csv", encoding = "UTF-8")
```

Check variable names:
```{r}
names(firms) 
names(population)
names(salary)
names(geo)
```

To better understand the data we assign meaningful names and drop some variables which are not needed (at the moment):
```{r} 
names(firms)[2:ncol(firms)] <-
  c("town", 
    "regNum",
    "deptNum",
    "total",
    "null",
    "firmsEmpl_1_5",
    "firmsEmpl_6_9",
    "firmsEmpl_10_19",
    "firmsEmpl_20_49",
    "firmsEmpl_50_99",
    "firmsEmpl_100_199",
    "firmsEmpl_200_499",
    "firmsEmpl_500plus")

names(salary)[2:ncol(salary)] <-
  c("town",
    "sal_general",    
    "sal_executive",
    "sal_midManager",
    "sal_employee",
    "sal_worker",
    "sal_Females",
    "sal_F_executive",
    "sal_F_midManager",
    "sal_F_employee",
    "sal_F_worker",
    "sal_Males",
    "sal_M_executive",
    "sal_M_midManager",
    "sal_M_employee",
    "sal_M_worker",
    "sal_18_25",
    "sal_26_50",
    "sal_51plus",
    "sal_F_18_25",
    "sal_F_26_50",
    "sal_F_51plus",
    "sal_M_18_25",
    "sal_M_26_50",
    "sal_M_51plus")

names(population)[5:7] <-
  c("ageCateg5",
    "sex",
    "peopleCategNum")

# Drop unnecessary columns (code/num and name represents same thing)
geo <- subset(geo, select = -c(EU_circo, code_rÈgion, numÈro_dÈpartement, prÈfecture, numÈro_circonscription, Èloignement))
# change names 
names(geo)[1:6] = 
  c("region", 
    "region_capital", 
    "department", 
    "town_name", 
    "postal_code", 
    "CODGEO")
```

According to the information provided, the CODGEO variable (in firms, salary and population) and code_insee (in geo) have to be merged.
However, for different reasons already identified by a kaggle user on [his kernel](https://www.kaggle.com/anqitu/insights-on-business-demographic-inequality) they do not.
To do so:
```{r}
firms$CODGEO <- sub("A", "0", firms$CODGEO)
firms$CODGEO <- sub("B", "0", firms$CODGEO)
salary$CODGEO <- sub("A", "0", salary$CODGEO)
salary$CODGEO <- sub("B", "0", salary$CODGEO)
population$CODGEO <- sub("A", "0", population$CODGEO)
population$CODGEO <- sub("B", "0", population$CODGEO)
```



# Analyze firms data #

## Pre-processing ##
```{r}

# preliminary checks
dim(firms)
names(firms)
head(firms)
str(firms)
summary(firms)

# converting CODGEO format
firms$CODGEO <- as.numeric(firms$CODGEO)

# Check for duplicated data
sum(duplicated.data.frame(firms))

# Categorize firms' size according to EU standard, but slightly different for medium and large firms (medium firms have <200 instead of <250 employees)
# http://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Enterprise_size
firms$micro   <- firms$firmsEmpl_1_5 + firms$firmsEmpl_6_9
firms$small   <- firms$firmsEmpl_10_19 + firms$firmsEmpl_20_49
firms$medium  <- firms$firmsEmpl_50_99 + firms$firmsEmpl_100_199
firms$large   <- firms$firmsEmpl_200_499 + firms$firmsEmpl_500plus

# Drop unnecessary (at the moment) columns 
firms <- subset(firms, select = c(CODGEO, town, total, micro, small, medium, large, null))

# check
head(firms)
summary(firms)

```

## EDA ##
  
```{r}
# there is an obs with more than 316K null data: we check if it is plausible
# get the highest 20 null values
str_firms <- sort(firms$null, decreasing = T)[1:20]
# get their indexes
str_firms_ind <- match(str_firms, firms$null)
# get the corresponding city
firms$town[str_firms_ind]
# hence, it seems reasonable..

# check the ratio of null for each town
summary(firms$null/firms$total)
# a lot of information is missing
# should we remove these data?
hist(firms$null/firms$total)

# evaluate the distribution of all the sizes 
# (log vs. ratio wrt total?)
hist(log(firms$total))
hist(log(firms$null))
hist(log(firms$micro))
hist(firms$micro/firms$total)
hist(log(firms$small))
hist(firms$small/firms$total)
hist(log(firms$medium))
hist(firms$medium/firms$total)
hist(log(firms$large))
hist(firms$large/firms$total)
```

PCA on firms data:
```{r}
firms_clean <- firms[firms$micro < 20000 & firms$large < 200,]
myPr <- prcomp(firms_clean[, 4:8], scale = TRUE)
#plot(scale(firms_clean$micro), scale(firms_clean$large))
#mean(firms_clean$micro)
#mean(firms_clean$large)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0)
#extract PC scores...
str(myPr)
#myPr$x #checking principal component scores
firms2 <- cbind(firms_clean, myPr$x[, 1:2])
head(firms2)
#plot with ggplot...
require(ggplot2)
ggplot(firms2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(firms_clean[, 4:8], firms2[,9:10])
```

## What we have learned ##

* More micro firms than small ones
* ... 

## How to use these data ##

We plan to use these for the following tasks:

* predict the salaries using such information as proxy for the competition in the job market;
* predict the total number of firms, using salary data;
* geo-spatial plot for firms' size 
* ... 


# Analyze geographical data #

## Pre-processing ##
```{r}

# preliminary checks
dim(geo)
names(geo)
head(geo)
str(geo)
summary(geo)

# spot "," instead of "." in longitude
newLong       <- as.character(geo$longitude)    # copy the vector
sum(grep(",", newLong))                         # total commas
ind_long_err  <- grep(",", newLong)             # indexing them
newLong       <- gsub(",", ".", newLong)        # substituting them with dots
indNA_Long    <- is.na(as.numeric((newLong)))   # spot NA
geo$longitude[indNA_Long]                       # verify that they were actually missing
geo$longitude <- as.numeric(newLong)            # overwrite the longitude variable with the new one

# Check for duplicated data (e.g., cities with different postal codes, that we dropped):
  # es. to verify it:  try on the initial dataset
  # sum(geo$nom_commune == "Paris")
  # ind_duplic <- geo$nom_commune == "Paris"
  # geo[ind_duplic,]
sum(duplicated.data.frame(geo)) 
# retaing unique postal cities
geo <- unique(geo, by = "CODGEO")

# check again
head(geo)
summary(geo)

```


Assign lat and long values for NAs units:
```{r}
require(ggmap)

# [ delete? ]
# # compare numbers of NA in latitude and longitude 
# sum(is.na(geo$latitude)) - sum(is.na(geo$longitude))
# # check if they match or not
# sum(!is.na(geo$latitude[indNA_Long]))
# # 64 obs are missing in longitude but not in latitude, hence 88 vice versa

# index of NAs and their total
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)

# NO MORE NEEDED BECAUSE IS A CSV FILE
    # # initialize variables
    # city_search = 0
    # res = as.data.frame(matrix(c(0, 0, 0), 1, 3))
    # names(res) = c("lon", "lat", "address")
    # 
    # # retrieve lat and long (Google API = 2500 request per day)
    # # my_iter = floor(sum(indNA_coord)/3)
    # for (i in 1:sum(indNA_coord)){
    # 
    #   # city searched
    #   city_search[i] = paste(c(as.character(NA_coord$town_name[i]), as.character(NA_coord$postal_code[i]), as.character(NA_coord$department[i]), "France"), sep=" ", collapse = ", ")
    #   
    #   # solution
    #   res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
    # 
    #   # retrieve still missing data, because of existing problems with API (up to 15 trials)
    #   j = 0
    #   while (any(is.na(res[i,])) & j < 25){
    #     res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
    #     j = j + 1
    #   }
    # }

    # # check the solution
    # sol = cbind(searched = city_search, res)

    # # save it as a csv file to save time
    # write.csv(retrieved_geo_NA[,2:3], "geo_NA_Final.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
    

# read the created csv
retrieved_geo_NA = read.csv("geo_NA_Final.csv", header = T, encoding = "UTF-8")
# get only long and lat and assign to original NA 
geo$latitude[indNA_coord] = retrieved_geo_NA[,2]
geo$longitude[indNA_coord] = retrieved_geo_NA[,1]

# there are 37 still missing units, which are towns located in old colonies far from Europe
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)
# exclude those towns
geo = geo[!indNA_coord,]

summary(geo)

```

## EDA ##

```{r}
#install.packages("ggplot2")
#install.packages("ggmap")
require(ggplot2)
require(ggmap)

# plot france (center: 2.213749 46.227638)
# fra_center = as.numeric(geocode("France"))
fra_center = c(2.213749, 46.227638)
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal")
FraMap

# plot all towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
FraMap +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.1) 

# delete non-European countries
ind_nonEur = geo$latitude < 30 | geo$latitude > 70 |geo$longitude < -20 | geo$longitude > 20
sum(ind_nonEur)
geo = geo[!ind_nonEur,]

# plot all European towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
FraMap +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.1) 

```



## What we have learned ##

Solved: 

* Why latitude is missing and not longitude?
* There are some duplications.

To do:
* What to do with non-Eropean towns?



## How to use these data ##

* Compare European towns vs. old colonies?
* Useful for all datasets/analyses


# Analyze salary data #

## Pre-processing ##

```{r}
# preliminary checks
dim(salary)
names(salary)
head(salary)
str(salary)
summary(salary)

# Drop unnecessary columns (town name repeats in other table, is it surely possible to merge them?)
names(salary)
# salary <- subset(salary, select = -c(town))

# Convert CODGEO to numeric
salary$CODGEO <- as.numeric(as.character(salary$CODGEO))

# Check for duplicated data
sum(duplicated.data.frame(salary))


```


## EDA ##

Univariate analysis comparing various job categories for both genders:  
```{r}

require(ggplot2)

#  number of units
n_sex <- length(salary$sal_Females)

# vector representing males and females
Label <- c(rep("M", n_sex*5), rep("F", n_sex*5))

# vector representing the variable considered
Variable <- c(rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex),
             rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex))

# merge these data
sal_sex = cbind.data.frame(Label = Label, 
             value = c(salary$sal_Males, salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
                       salary$sal_Females, salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker),
             Variable = Variable)

# plotting phase
p <-  ggplot(data = sal_sex, aes(x=Label, y=value)) +
      geom_boxplot(aes(fill = Label)) +
      # not color points replacing colour = group instead of colour=Label
      geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
      facet_wrap( ~ Variable, scales="free") +
      xlab("x-axis") + ylab("y-axis") + ggtitle("Gender comparison") +
      stat_boxplot(geom = "errorbar", width = 0.5)
      # p <- p + guides(fill=guide_legend(title="Legend"))
p

# excluding outliers
p2 <- ggplot(data = sal_sex, aes(x=Label, y=value)) +
      scale_y_continuous(limits = quantile(sal_sex$value, c(0, 0.9))) +
      geom_boxplot(aes(fill = Label)) +
      # not color points replacing colour = group instead of colour=Label
      geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
      facet_wrap( ~ Variable, scales="free") +
      xlab("x-axis") + ylab("y-axis") + ggtitle("Gender comparison excluding the last decile") +
      # p <- p + guides(fill=guide_legend(title="Legend"))
      stat_boxplot(geom = "errorbar", width = 0.5)
p2
```



Highlight bivariate relations using scatter matrices:
```{r}
# most general pairs
pairs(salary[c(3:8, 13, 18:20)])
# pairs highlighting genders' differences
pairs(salary[c(9:12, 14:17)])
```

Fit a regression model to predict the salaries of people in age 26-50 using as regressor 51+ years:
```{r, fig.keep='all'}

# fit and show OLS estimate
plot(salary$sal_26_50 ~ salary$sal_51plus)
fit_LM_26_50 = glm(salary$sal_26_50 ~ salary$sal_51plus, data = salary)
abline(fit_LM_26_50, lwd=3, col="red")

# diagnostics
summary(fit_LM_26_50)
plot(fit_LM_26_50)
```

Same as before but adding polynomials which are evauated using 10-folds cross validation:

```{r, fig.keep='all'}

require(boot)
set.seed(1)

# k-Fold Cross-Validation
cv.err.K = rep(0, 5)
cv.err.K = rbind(cv.err.K, cv.err.K)
for (i in 1:5){
  fit_LM_26_50.K = glm(sal_26_50 ~ poly(sal_51plus, i), data = salary)
  cv.err.K[,i] = cv.glm(salary, fit_LM_26_50.K, K = 10)$delta[1]
}

# plotting results
plot(cv.err.K[1,], type = 'l', col = 'red', xlab = "Polynomials' order", 
     ylab = "10-folds CV", main = "CV and adjusted CV for different polynomials")
lines(cv.err.K[2,], col = 'green')
points(which.min(cv.err.K), cv.err.K[1, which.min(cv.err.K)], col = "red", cex=2, pch=20)
legend('topright', legend = c('CV', 'Adj. CV'), col = c('red', 'green'), pch = 10)

```


Predicting sal_executive with 5 regressers using Lasso with CV and 10-folds CV:
[ PROBLEM FOR COLLINEARITY? use ridge?] [ add polynomials? ]
```{r, fig.keep='all'}

require(glmnet)
set.seed(1)

# crate an X matrix excluding intercept
# x = cbind(salary$sal_midManager, salary$sal_employee, salary$sal_worker, salary$sal_18_25, salary$sal_51plus)
# y = salary$sal_executive
# probably for ridge
names(salary)
y = salary$sal_26_50
x = as.matrix(cbind(salary[, c(4:8, 13, 18, 20)]))
names(x)

# grid for lambda values
grid = 10^seq(1, -5, length = 100)
lasso.mod = glmnet(x, y, alpha = 1, lambda = grid)
dim(coef(lasso.mod))
# the norms are increasing in value because of the shrinkage
lasso.mod$lambda[1]                  # lambda value
sqrt(sum(coef(lasso.mod)[-1,1]^2))   # L2 norm of its coeff
lasso.mod$lambda[51]
sqrt(sum(coef(lasso.mod)[-1,51]^2))
lasso.mod$lambda[90]
sqrt(sum(coef(lasso.mod)[-1,90]^2))
# predict values for a new lambda, e.g. OLS
OLS = predict(lasso.mod, s = 0, type = "coefficients")[1:nrow(coef(lasso.mod)),]

# split the date leaving the 10% for CV
train = sample(1:nrow(salary), floor(nrow(salary)*0.9))
test = -train
y.test = y[test]
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12)
err.i = rep("NA", length(grid))
for (i in 1:length(grid)){
  lasso.pred = predict(lasso.mod, s = grid[i], newx = x[test,])
  err.i[i] = mean((lasso.pred - y.test)^2)
}
plot(log(grid), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
     main = 'Test MSE among different Lambdas', ylim = c(0, 10))
bestlam = grid[which.min(err.i)]
points(log(grid)[bestlam], err.i[bestlam], col ="red", cex=2, pch=20)
# high values of lambda are like fitting just the intercept

# using 10 folds CV
set.seed (1)
cv.out = cv.glmnet(x[train ,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred - y.test)^2)


# using best subset
require(leaps)
dataBS = as.data.frame(cbind(y, x))
best.sub = regsubsets(y ~ x, data = dataBS, nvmax = nrow(coef(lasso.mod)))
best.sub.summary = summary(best.sub)
names(best.sub.summary)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
# built-in plots
?plot.regsubsets
par(mfrow=c(1,1))
plot(best.sub, scale = "r2")
plot(best.sub, scale = "adjr2")
plot(best.sub, scale = "Cp")
plot(best.sub, scale = "bic")
# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))

```




Model salaries for people aged 18-25, which seems more difficult to predict:
```{r, fig.keep='all'}

plot(y = salary$sal_18_25, x = salary$sal_51plus)
# there is a clear outlier
ind_out = which(salary$sal_18_25 == max(salary$sal_18_25))
# check if is so in all dimensions (apparently not)
col_col <- rep("black", nrow(salary))
col_col[ind_out] <- "red"
pairs(salary[, c(3:8, 13, 18:20)], col = col_col)

# evaluate an OLS fit
fit_LM_18_25 = lm(salary$sal_18_25 ~ salary$sal_51plus)
plot(y = salary$sal_18_25, x = salary$sal_51plus)
abline(fit_LM_18_25, lwd=3, col="red")

# # using more predictors
# fit_LM_2 = lm(salary$sal_M_18_25 ~ salary$sal_26_50 + salary$sal_51plus + salary$sal_general + salary$sal_executive + 
#                 salary$sal_midManager + salary$sal_employee + salary$sal_worker)
# 
# summary(fit_LM_2)
```

PCA for salary:
```{r}
myPr <- prcomp(salary[, 3:26], scale = TRUE)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0, cex = 0.5)
str(myPr)
#myPr$x #checking principal component scores
salary2 <- cbind(salary, myPr$x[, 1:2])
head(salary2)
#plot with ggplot...
#require(ggplot2)
ggplot(salary2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(salary[, 3:26], salary2[,27:28])
```


## What we have learned ##

* Check the outlier in sal_18_25: which city is it, etc.
* evaluate possible multicollinearity
* Try a regression with all predictors ans lasso

## How to use these data ##

* ...



# Analyze population data #

## Pre-processing ##


```{r}
# preliminary checks
names(population)
summary(population)

# Drop unnecessary columns (NIVGEO is the same for all)
population <- subset(population, select = -c(NIVGEO, LIBGEO))

# converting CODGEO to numeric
population$CODGEO <- as.numeric(population$CODGEO)

# Refactor sex and MOCO
population$MOCO <- factor(population$MOCO, levels = c(11,12,21,22,23,31,32),
            labels = c("children_living_with_two_parents", "children living with one parent",
                       "adults_living_in_couple_without_child", "adults_living_in_couple_with_children",
                       "adults_living_alone_with_children","persons not from family living in the home",
                       "persons_living_alone"))
population$SEXE <- factor(population$sex, levels = c(1,2), labels = c("Male", "Female"))
head(population)
# Take out rows with NB (number of people in this category) equal to 0
population <- population[population$peopleCategNum != 0,]

head(population)
summary(population)


```

## EDA ##

Compare age categories:
```{r}
require(ggplot2)
library(ggplot2)
#  number of units
n_cat <- length(population$CODGEO)
# extract unique categories
uniq_cat <- unique(population$ageCateg5!=5)
# vector representing sex for each category
Label <- c(rep(c('Male', 'Female'), n_cat))
# vector representing the variable considered
Variable <- rep(uniq_cat, n_cat/length(uniq_cat))
Value=population$peopleCategNum
# merge these data
pop_categ = cbind.data.frame(Label = Label, 
             value = Value,
             Variable = Variable)
p <- ggplot(data = pop_categ, aes(x=Label, y=value)) 
p <- p + geom_boxplot(aes(fill = Label))
# if you want color for points replace group with colour=Label
p <- p + geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75))
p <- p + facet_wrap( ~ Variable, scales="free")
p <- p + xlab("x-axis") + ylab("y-axis") + ggtitle("Category comparison")
# p <- p + guides(fill=guide_legend(title="Legend"))
p
```




```{r}
# Restructure population data to produce the demographic profile per town
# install.packages("plyr")
library(plyr)
population_per_town_data <- ddply(population, .(CODGEO), function(population) {
  data.frame(total_population = sum(population$peopleCategNum),
             male = sum(population[population$sex == "Male",]$peopleCategNum),
             female = sum(population[population$sex == "Female",]$peopleCategNum),
             child = sum(population[population$ageCateg5 %in% seq(0, 10, by=5),]$peopleCategNum),
             elderly = sum(population[population$ageCateg5 %in% seq(65, 80, by=5),]$peopleCategNum),
             workforce = sum(population[population$ageCateg5 %in% seq(15, 60, by=5),]$peopleCategNum) 
             )})

population_per_town_data$dependent <- population_per_town_data$child + population_per_town_data$elderly
population_per_town_data$sex_ratio <- ifelse(population_per_town_data$female==0, 0, population_per_town_data$male / population_per_town_data$female)
population_per_town_data$dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$dependent / population_per_town_data$workforce)
population_per_town_data$aged_dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$elderly / population_per_town_data$workforce)
population_per_town_data$child_dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$child / population_per_town_data$workforce)
summary(population_per_town_data)
```

```{r}
# Scale population to log
hist(population_per_town_data$total_population, ylim=c(0,40000), breaks = seq(0, 2500000, by=250000), xlab="", main = "", labels=T, col ="light blue")
population_per_town_data$total_population_log <- log10(population_per_town_data$total_population)

# Merge geo and pop
geo_pop_by_town <- merge(geo, population_per_town_data)
summary(geo_pop_by_town)
```

```{r}
# Plot "Distribution of Population for each Town"
#sc <- scale_colour_gradient(colours = myPalette(100), limits=c(min(merged_geo_pop_by_town_data$total_population_log), max(merged_geo_pop_by_town_data$total_population_log)))
#poppulation_distribution <- 
#  france_map + 
#  geom_point(aes(x=merged_geo_pop_by_town_data$longitude, y=merged_geo_pop_by_town_data$latitude, colour=merged_geo_pop_by_town_data$total_population_log), 
#             data=merged_geo_pop_by_town_data, alpha=0.8, size=0.6) + 
#  sc +
#  geom_text(aes(label = town_name, x = longitude, y = latitude), 
#            data = subset(merged_geo_pop_by_town_data, total_population_log %in% head(sort(total_population_log, decreasing=TRUE), 3)),
#            check_overlap = TRUE, size=7) +
#  labs(color='Total Population in Log') +
#  ggtitle("Distribution of Population for each Town") 
#poppulation_distribution
```


# Produce consistent datasets #


```{r}

# use only integer values
geo$CODGEO = as.integer(geo$CODGEO)  # already integer
population$CODGEO = as.integer(population$CODGEO)
firms$CODGEO = as.integer(firms$CODGEO)
salary$CODGEO = as.integer(salary$CODGEO)

# install.packages("dplyr")
library(dplyr)
dataset = c("population", "salary", "firms", "geo")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and make a new variable adding NEW
  nam <- paste(i, "NEW", sep = "")
  # counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # datasets different from i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(paste(i, "NEW", sep = "")))
  del_obs = paste("For", i, del_rows, "have been deleted.",
                  "They were the", round(del_prop, digits=2), "% of the total.", sep = " ")
  print(del_obs)
}


```

# Analysis #

## PCA ##

## Regression ##

## Clustering ##


