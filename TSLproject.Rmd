---
title: "TSL Project"
author: "L. Insolia, J. Kim and G. Yeghikyan"    
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_notebook:
    # df_print: paged
    toc: yes
    toc_depth: '3' # up to three depths of headings (specified by #, ## and ###)
    highlight: tango
    keep_tex: yes
    number_sections: yes
    # theme: united
  # pdf document:
  html_document:
    theme: united
    highlight: tango    
    toc: yes
    toc_depth: '3' 
  pdf_document:
    toc: yes
    toc_depth: '3'
    keep_tex: true
    latex_engine: pdflatex
editor_options:
  # toc: yes
  # toc_depth: 3
  chunk_output_type: inline
---


# General information #
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

We analyze a dataset published on [Kaggle](https://www.kaggle.com/etiennelq/french-employment-by-town).
It refers to french employment, salaries, population per town. 
The aim is to evaluate equality/inequalities in France, and geographical distribution of business according to their size.

Such data are collected by the INSEE.
Information regarding the number of firms in every french town, categorized by size
can be found [here](https://www.insee.fr/fr/metadonnees/definition/c1135). 
Information about salaries around french town per job categories, age and sex (expressed in average net amount per hour in euro) can be found [here](https://www.insee.fr/fr/statistiques/2522515).
Demographic information in France per town, age, sex and living mode
can be found [here](https://www.insee.fr/fr/statistiques/2863607).

Additional info about Population Data can be found [here](https://www.insee.fr/fr/statistiques/2863607#dictionnaire), 
it allows to add cat?gorie socioprofessionnelle.




## Aim of the study ##
This project aims to explore existing patterns among French towns.

In particular, we are interested in:

* evaluating possible inequalities: per towns/region, sex, age, etc.;
* predicting the ... using a regression model;
* reduce the dimensionality of ... performing a PCA;
* explore different algorithms to cluster male/females using ...



# General pre-processing phase #


Loading all libraries needed throughout the notebook:
```{r}
if(!require(plyr)){install.packages("plyr"); library(plyr)}
if(!require(dplyr)){install.packages("dplyr"); library(dplyr)}
if(!require(ggplot2)){install.packages("ggplot2"); library(ggplot2)}
if(!require(ggmap)){install.packages("ggmap"); library(ggmap)}
if(!require(ggbiplot)){
  if(!require(devtools)){
    install.packages("devtools")}
  install_github("vqv/ggbiplot")
  library(ggbiplot)}
if(!require(mclust)){install.packages("mclust"); library(mclust)} 
if(!require(corrplot)){install.packages("corrplot"); library(corrplot)}
if(!require(MASS)){install.packages("MASS"); library(MASS)}
if(!require(factoextra)){install.packages("factoextra"); library(factoextra)}
if(!require(rafalib)){install.packages("rafalib"); library(rafalib)}
if(!require(boot)){install.packages("boot"); library(boot)}
if(!require(glmnet)){install.packages("glmnet"); library(glmnet)}
```


Import the four datasets:
```{r warning=FALSE}
# set the encoding used [to test]
# options(encoding = "ISO-8859-1")  # default in Windows 

# loading data
setwd("./data")
firms       <- read.csv("base_etablissement_par_tranche_effectif.csv", encoding = "UTF-8")
geo         <- read.csv("name_geographic_information.csv", encoding = "UTF-8")
salary      <- read.csv("net_salary_per_town_categories.csv", encoding = "UTF-8")
population  <- read.csv("population.csv", encoding = "UTF-8")
```

Check variable names:
```{r}
names(firms) 
names(population)
names(salary)
names(geo)
```

To better understand the data we assign meaningful names and drop some variables which are not needed (at the moment):
```{r} 
names(firms)[2:ncol(firms)] <-
  c("town", 
    "regNum",
    "deptNum",
    "total",
    "null",
    "firmsEmpl_1_5",
    "firmsEmpl_6_9",
    "firmsEmpl_10_19",
    "firmsEmpl_20_49",
    "firmsEmpl_50_99",
    "firmsEmpl_100_199",
    "firmsEmpl_200_499",
    "firmsEmpl_500plus")

names(salary)[2:ncol(salary)] <-
  c("town",
    "sal_general",    
    "sal_executive",
    "sal_midManager",
    "sal_employee",
    "sal_worker",
    "sal_Females",
    "sal_F_executive",
    "sal_F_midManager",
    "sal_F_employee",
    "sal_F_worker",
    "sal_Males",
    "sal_M_executive",
    "sal_M_midManager",
    "sal_M_employee",
    "sal_M_worker",
    "sal_18_25",
    "sal_26_50",
    "sal_51plus",
    "sal_F_18_25",
    "sal_F_26_50",
    "sal_F_51plus",
    "sal_M_18_25",
    "sal_M_26_50",
    "sal_M_51plus")

names(population)[5:7] <-
  c("ageCateg5",
    "sex",
    "peopleCategNum")

# [KEEP DEPT. NUM?]
# [?loignement is the distance from where?]
names(geo)[c(2:11, 14)] =
  c("code_region",
    "region", 
    "region_capital",
    "number_depart",
    "department", 
    "prefecture",
    "circons",
    "town_name", 
    "postal_code", 
    "CODGEO",
    "eloignement")

# Drop unnecessary columns (code/num and name represents same thing) 
geo <- subset(geo, select = -c(EU_circo, code_region, number_depart, prefecture, circons, eloignement))
```

Check variable names again:
```{r}
names(firms) 
names(population)
names(salary)
names(geo)
```






# Analyze firms data #

## Pre-processing ##
```{r}

# preliminary checks
dim(firms)
names(firms)
head(firms)
str(firms)
summary(firms)

# converting CODGEO format
# firms$CODGEO <- as.numeric(firms$CODGEO)

# Check for duplicated data
sum(duplicated.data.frame(firms))

# Categorize firms' size according to EU standard, but slightly different for medium and large firms (medium firms have <200 instead of <250 employees)
# http://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Enterprise_size
firms$micro   <- firms$firmsEmpl_1_5 + firms$firmsEmpl_6_9
firms$small   <- firms$firmsEmpl_10_19 + firms$firmsEmpl_20_49
firms$medium  <- firms$firmsEmpl_50_99 + firms$firmsEmpl_100_199
firms$large   <- firms$firmsEmpl_200_499 + firms$firmsEmpl_500plus

# Drop unnecessary (at the moment) columns 
firms <- subset(firms, select = c(CODGEO, town, total, micro, small, medium, large, null))

# check
head(firms)
summary(firms)
str(firms)
# deptNum has to be factor?
```

## Descriptive statistics ##
  
```{r}
# there is an obs with more than 316K null data: we check if it is plausible
# get the highest 20 null values
str_firms <- sort(firms$null, decreasing = T)[1:20]
# get their indexes
str_firms_ind <- match(str_firms, firms$null)
# get the corresponding city
firms$town[str_firms_ind]
# hence, it seems reasonable..

# check the ratio of null for each town
summary(firms$null/firms$total)
# a lot of information is missing
# should we remove these data?
hist(firms$null/firms$total)
ggplot(data=firms, aes(firms$null/firms$total)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Null firms/total firms", y="Density") +
  ggtitle("Ratio between the number of null firms and total firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# evaluate the distribution of all the sizes 
ggplot(data=firms, aes(log(firms$total))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms (log scale)", y="Density") +
  ggtitle("Total number of firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# make the same kind of plot for them
hist(log(firms$null))
hist(log(firms$micro))
hist(log(firms$small))
hist(log(firms$medium))
hist(log(firms$large))

```

## PCA ##   

PCA on firms data:
```{r}

# pairs(firms[, 3:8])
# 
# salary_NEW = salary[, 3:ncol(salary)]
# # colnames(salary_NEW) = 1:32
# corrplot(cor(salary_NEW), method = "circle", title = "Correlation matrix for salary", 
#          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1,0)) 
# 
# 
# # firms_clean <- firms[firms$micro < 20000 & firms$large < 200,]
# firms_clean <- firms[firms$micro > 1 & firms$micro < 2500 & 
#                      firms$small > 1 & firms$small < 800 & 
#                      firms$medium > 1 & firms$medium < 500 & 
#                      firms$large > 1 & firms$large < 200 &
#                      firms$null > 1 & firms$null < 2000 ,]
# myPr <- prcomp(firms_clean[, 4:8], scale = TRUE)
# #plot(scale(firms_clean$micro), scale(firms_clean$large))
# #mean(firms_clean$micro)
# #mean(firms_clean$large)
# myPr
# summary(myPr)
# plot(myPr, type = "l")
# # biplot(myPr, scale = 0)
# #extract PC scores...
# str(myPr)
# #myPr$x #checking principal component scores
# firms2 <- cbind(firms_clean, myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])
# 
# # using ggbiplot
# ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
#   ggtitle("Biplot for the firms's size") +
#   theme(plot.title = element_text(hjust = 0.5))
```


```{r}

# get Paris in order to exclude it in the following
Paris = which.max(firms$total)

# Original scale
# Scatter matrix
pairs(firms[-Paris, 3:8], gap=0, main = "Scatter matrix for firms")
# Correlation matrix
corrplot(cor(firms[-Paris, 3:8]), method = "number", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0))

# Log scale
# Scatter matrix
pairs(firms[-Paris, 3:8], log = "xy", gap=0, main = "Scatter matrix for firms in log scale")
# Correlation matrix
firmsLog = log(firms[, 3:8]) 
firmsLog[firmsLog == -Inf] = 0
corrplot(cor(firmsLog), method = "number", title = "Correlation matrix for firms  in log scale", 
          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
          tl.col = "black", mar=c(0,0,1.5,0))

# PCA
myPr <- prcomp(firms[-which.max(firms$total), 3:8], scale = TRUE)
summary(myPr)
plot(myPr, type = "l")
# PCA
# myPr <- pca(firms[-which.max(firms$total), 3:8], nPcs = 6, scale = "uv") # cv = ..
# summary(myPr)
# plot(myPr, type = "l")
# source("https://bioconductor.org/biocLite.R")
# biocLite("pcaMethods")
# library(pcaMethods)
# slplot(myPr, scoresLoadings = c(T,T))

# using ggbiplot
ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
  ggtitle("Biplot for the firms's size") +
  theme(plot.title = element_text(hjust = 0.5))

# classic vs robust PC
# par(mfrow=c(2,1))
# require(graphics)
# library(rrcov)
# myPr<-PcaClassic(firms[-which.max(firms$total), 3:8], scale = TRUE, k = 6)
# biplot(myPr)
# v1 <- PcaHubert(firms[-which.max(firms$total), 3:8], scale = TRUE, mcd = T, k = 6)
# biplot(v1)
# plot(v1) # distance plot
# screeplot(v1)
```

## Cluster Analysis ##

```{r}
# firms_sampled <- firms[1:1000, ] # subsampling
# head(firms_sampled)
# firms_scaled <- scale(firms_sampled[, 3:8]) #scaling the data
# 
# head(firms_scaled)
# 
# firms_truncated <- firms_sampled[, 4:8]
# head(firms_truncated)
# plot(firms_sampled)
# # K-means clustering...
# 
# 
# fitK_scaled <- kmeans(firms_scaled, 4) 
# head(fitK_scaled)
# 
# fitK <- kmeans(firms_truncated, 5)
# head(fitK)
# str(fitK)
# plot(firms_sampled, col = fitK$cluster) #plotting data colored according to cluster membership
# 
# #choosing K---
# k <- list()
# for(i in 1:10){
#   k[[i]] <- kmeans(firms_truncated, i)
# }
# head(k)
# 
# betweenss_totalss <- list()
# for(i in 1:10){
#   betweenss_totalss[[i]] <- k[[i]]$betweenss/k[[i]]$totss
# }
# plot(1:10, betweenss_totalss, type ="b",
#      ylab = "Between SS / Total SS", xlab = "Clusters (k)") #calculating and plotting between SS to total SS ratio against number of clusters
# 
# for(i in 1:5) {
#   plot(firms, col = k[[i]]$cluster) #plotting data based on membership to clusters for k = 1 to 5 clusters
# }
# head(fitM)
# fitM <- Mclust(firms_truncated)
# plot(fitM)
# #Model-based clustering using mclust
# 
# head(clusters)
# plot(firms_sampled, col = clusters) # as we can see, it performs quite similar to the K-means 
# rect.hclust(fitH, k = 5, border = "red") # visualising dendrogam cut at k =5
# clusters <- cutree(fitH, 5)  # vector with cluster membership for each observation
# plot(fitH)
# d <- dist(firms_truncated)
# fitH <- hclust(d, "ward.D2")
# 
# #Hierarchical clustering---
# 

```


## What we have learned ##

* More micro firms than small ones
* ... 

## How to use these data ##

We plan to use these for the following tasks:

* predict the salaries using such informsation as proxy for the competition in the job market;
* predict the total number of firms, using salary data;
* geo-spatial plot for firms' size 
* ... 


# Analyze geographical data #

## Pre-processing ##
```{r}

# preliminary checks
dim(geo)
names(geo)
head(geo)
str(geo)
summary(geo)

# spot "," instead of "." in longitude
newLong       <- as.character(geo$longitude)    # copy the vector
sum(grep(",", newLong))                         # total commas
ind_long_err  <- grep(",", newLong)             # indexing them
newLong       <- gsub(",", ".", newLong)        # substituting them with dots
indNA_Long    <- is.na(as.numeric((newLong)))   # spot NA
geo$longitude[indNA_Long]                       # verify that they were actually missing
geo$longitude <- as.numeric(newLong)            # overwrite the longitude variable with the new one

# Check for duplicated data (e.g., cities with different postal codes, that we dropped):
  # es. to verify it:  try on the initial dataset
  # sum(geo$nom_commune == "Paris")
  # ind_duplic <- geo$nom_commune == "Paris"
  # geo[ind_duplic,]
sum(duplicated.data.frame(geo)) 
# retaing unique postal cities
geo <- unique(geo, by = "CODGEO")

# check again
head(geo)
summary(geo)

```


Assign lat and long values for NAs units:
```{r}

# index of NAs and their total
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)

# NO MORE NEEDED BECAUSE IS A CSV FILE
    # # initialize variables
    # city_search = 0
    # res = as.data.frame(matrix(c(0, 0, 0), 1, 3))
    # names(res) = c("lon", "lat", "address")
    # 
    # # retrieve lat and long (Google API = 2500 request per day)
    # # my_iter = floor(sum(indNA_coord)/3)
    # for (i in 1:sum(indNA_coord)){
    # 
    #   # city searched
    #   city_search[i] = paste(c(as.character(NA_coord$town_name[i]), as.character(NA_coord$postal_code[i]), as.character(NA_coord$department[i]), "France"), sep=" ", collapse = ", ")
    #   
    #   # solution
    #   res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
    # 
    #   # retrieve still missing data, because of existing problems with API (up to 15 trials)
    #   j = 0
    #   while (any(is.na(res[i,])) & j < 25){
    #     res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
    #     j = j + 1
    #   }
    # }

    # # check the solution
    # sol = cbind(searched = city_search, res)

    # # save it as a csv file to save time
    # write.csv(retrieved_geo_NA[,2:3], "geo_NA_Final.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
    

# read the created csv
setwd("./data")
retrieved_geo_NA = read.csv("geo_NA_Final.csv", header = T, encoding = "UTF-8")
# get only long and lat and assign to original NA 
geo$latitude[indNA_coord] = retrieved_geo_NA[,2]
geo$longitude[indNA_coord] = retrieved_geo_NA[,1]

# there are 37 still missing units, which are towns located in old colonies far from Europe
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)
# exclude those towns
geo = geo[!indNA_coord,]

summary(geo)

```

## Descriptive statistics ##

```{r}

# plot france (center: 2.213749 46.227638)
# fra_center = as.numeric(geocode("France"))
fra_center = c(2.213749, 46.227638)
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal")
FraMap

# plot all towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
FraMap +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.8, size=0.6) 

# delete non-European countries
ind_nonEur = geo$latitude < 30 | geo$latitude > 70 |geo$longitude < -20 | geo$longitude > 20
sum(ind_nonEur)
geo = geo[!ind_nonEur,]

# plot all European towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.8, size=0.6) 

```



## What we have learned ##

Solved: 

* Why latitude is missing and not longitude?
* There are some duplications.

To do:

* What to do with non-European towns?



## How to use these data ##

* Compare European towns vs. old colonies?
* Useful for all datasets/analyses


# Analyze salary data #

## Pre-processing ##

```{r}

# preliminary checks
dim(salary)
names(salary)
head(salary)
str(salary)
summary(salary)

# Drop unnecessary columns (town name repeats in other table, is it surely possible to merge them?)
names(salary)
# salary <- subset(salary, select = -c(town))

# Convert CODGEO to numeric
salary$CODGEO <- as.numeric(as.character(salary$CODGEO))

# Check for duplicated data
sum(duplicated.data.frame(salary))

```


## Descriptive statistics ##

Univariate analysis comparing salaries for both genders among various job categories:

```{r}

#  number of units
n_sex <- length(salary$sal_Females)

# vector representing males and females
Label <- c(rep("M", n_sex*5), rep("F", n_sex*5))

# vector representing the variable considered
Variable <- c(rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex),
             rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex))

# merge these data
sal_sex = cbind.data.frame(Label = Label, 
             value = c(salary$sal_Males, salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
                       salary$sal_Females, salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker),
             Variable = Variable)

# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  # not color points replacing colour = group instead of colour=Label
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different job positions") +
  theme(plot.title = element_text(hjust = 0.5)) +      stat_boxplot(geom = "errorbar", width = 0.5)
  # + guides(fill=guide_legend(title="Legend"))

# the same but excluding outliers
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  scale_y_continuous(limits = quantile(sal_sex$value, c(0, 0.9))) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + 
  ggtitle("Gender comparison for different job positions excluding the last decile") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```

Univariate analysis comparing salaries for both genders among various ages:

```{r}
# vector representing males and females
Label <- c(rep("M", n_sex*3), rep("F", n_sex*3))

# vector representing the variable considered
Variable <- c(rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex),
              rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex))

# merge these data
sal_sex <- cbind.data.frame(Label = Label, 
                           value = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus, 
                                     salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus),
                           Variable = Variable)

# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different ages") +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(c(5, 100)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


The income inequality between genders, age groups and working positions is clear.
In the following analyses the focus is on the salary ratio between womand and man among different
job positions:

```{r ratio F vs M accross jobs}
# Gender salary ratio and general level of income

# Overall mean salary: The higher the net mean income, the more skewed the ratio of salary between female and male is. only 2 towns have a ratio>1
# create overall F vs M ratio
salary$salary_ratio_FvsM <- salary$sal_Females / salary$sal_Males
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Overall salary ratio (females/males)", y="Density") + 
  labs(title = "Overall salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs overall mean salary
ggplot(salary, aes(x= sal_general, y=salary_ratio_FvsM)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Overall salary ratio(females/males)") + 
  labs(title = "Overall salary ratio between females and males vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm") 


# Executives mean salary: a bit better for females and less skewed
# create Executives F vs M ratio
salary$salary_ratio_FvsM_Exec <- salary$sal_F_executive / salary$sal_M_executive
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Executives salary ratio (females/males)", y="Density") + 
  labs(title = "Executives salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_Exec)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Executives salary ratio (females/males)") + 
  labs(title = "Executives salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Middle managers mean salary: ...
# create Middle managers F vs M ratio
salary$salary_ratio_FvsM_midManag <- salary$sal_F_midManager / salary$sal_M_midManager
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Middle managers salary ratio (females/males)", y="Density") + 
  labs(title = "Middle managers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_midManag)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Middle managers salary ratio (females/males)") + 
  labs(title = "Middle managers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Workers mean salary: ...
# create workers F vs M ratio
salary$salary_ratio_FvsM_worker <- salary$sal_F_worker / salary$sal_M_worker
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_worker)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Workers salary ratio (females/males)", y="Density") + 
  labs(title = "Workers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs workers mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_worker)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Workers salary ratio (females/males)") + 
  labs(title = "Workers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Employee mean salary: ...
# create Employee F vs M ratio
salary$salary_ratio_FvsM_employee <- salary$sal_F_employee / salary$sal_M_employee
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_employee)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Employee salary ratio (females/males)", y="Density") + 
  labs(title = "Employee salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs Employee mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_employee)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Employee salary ratio (females/males)") + 
  labs(title = "Employee salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```

Now we perform the same analysis for different age groups:

```{r ratio F vs M accross ages}

# 18-25 mean salary: are quite equal apart from some outliers and a quadratic trend
# create 18-25 F vs M ratio
salary$salary_ratio_FvsM_18_25 <- salary$sal_F_18_25 / salary$sal_M_18_25
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_18_25)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="18-25 salary ratio (females/males)", y="Density") + 
  labs(title = "18-25 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 18-25 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")
# scatter plot vs 18-25 mean salary for them
ggplot(salary, aes(x= sal_18_25, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="18-25 salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall 18-25 salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")
# [whic is better? themselve or general salary as predictor?]


# 26-50 mean salary: ...
# create 26-50 F vs M ratio
salary$salary_ratio_FvsM_26_50 <- salary$sal_F_26_50 / salary$sal_M_26_50
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_26_50)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="26-50 salary ratio (females/males)", y="Density") + 
  labs(title = "26-50 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_26_50)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="26-50 salary ratio (females/males)") + 
  labs(title = "26-50 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# 51+ mean salary: ...
# create 51+ F vs M ratio
salary$salary_ratio_FvsM_51plus <- salary$sal_F_51plus / salary$sal_M_51plus
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_51plus)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="51+ salary ratio (females/males)", y="Density") + 
  labs(title = "51+ salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_51plus)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="51+ salary ratio (females/males)") + 
  labs(title = "51+ salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```



## Linear models ##

Highlight bivariate relations using scatter matrices:
```{r bivariate plots}
# correlation matrix
salary_NEW = salary[, 3:ncol(salary)]
colnames(salary_NEW) = 1:32
corrplot(cor(salary_NEW), method = "circle", title = "Correlation matrix for salary", 
         diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0)) 

# most general pairs
pairs(salary[c(3:8, 13, 18:20)], gap=0, main = "Scatter matrix for some variables in salary")
# pairs highlighting genders' differences
pairs(salary[c(9:12, 14:17)], gap=0)
```


First we perform some easy task
fitting a regression model to predict the salaries of people in age 26-50 using as regressor 51+ years:

```{r, fig.keep='all'}
# fit and show OLS estimate
plot(salary$sal_26_50 ~ salary$sal_51plus)
fit_LM_26_50 = lm(salary$sal_26_50 ~ salary$sal_51plus, data = salary) 
abline(fit_LM_26_50, lwd=3, col="red")
# diagnostics
summary(fit_LM_26_50)
plot(fit_LM_26_50)

# Same as before but adding polynomials which are evauated using 10-folds cross validation
require(boot)
set.seed(1)
# k-Fold Cross-Validation
cv.err.K = rep(0, 5)
cv.err.K = rbind(cv.err.K, cv.err.K)
for (i in 1:5){
  fit_LM_26_50.K = glm(sal_26_50 ~ poly(sal_51plus, i), data = salary)
  cv.err.K[,i] = cv.glm(salary, fit_LM_26_50.K, K = 10)$delta[1]
}
# plotting results
plot(cv.err.K[1,], type = 'l', col = 'red', xlab = "Polynomials' order", 
     ylab = "10-folds CV", main = "CV and adjusted CV for different polynomials")
lines(cv.err.K[2,], col = 'green')
points(which.min(cv.err.K), cv.err.K[1, which.min(cv.err.K)], col = "red", cex=2, pch=20)
legend('topright', legend = c('CV', 'Adj. CV'), col = c('red', 'green'), pch = 10)

```


Predicting sal_executive Elastic NEt with CV and 10-folds CV, using some variables and thei 2nd and 3rd order polynomials:
```{r, fig.keep='all'}


set.seed(1)
# crate an X matrix excluding intercept
# x = cbind(salary$sal_midManager, salary$sal_employee, salary$sal_worker, salary$sal_18_25, salary$sal_51plus)
# y = salary$sal_executive
# probably for ridge
names(salary)
y = salary$sal_18_25 
x = as.matrix(cbind(salary[, c(4:8, 13, 19:20, 27:31)])) 
x = as.matrix(cbind(x, x*x*x))
names(x)

# grid for lambda values
# grid = 10^seq(1, -5, length = 100)
# lasso.mod = glmnet(x, y, alpha = 1, lambda = grid)
# dim(coef(lasso.mod))
# # the norms are increasing in value because of the shrinkage
# lasso.mod$lambda[1]                  # lambda value
# sqrt(sum(coef(lasso.mod)[-1,1]^2))   # L2 norm of its coeff
# lasso.mod$lambda[51]
# sqrt(sum(coef(lasso.mod)[-1,51]^2))
# lasso.mod$lambda[90]
# sqrt(sum(coef(lasso.mod)[-1,90]^2))
# # predict values for a new lambda, e.g. OLS
# OLS = predict(lasso.mod, s = 0, type = "coefficients")[1:nrow(coef(lasso.mod)),]

# split the date leaving the 10% for CV
train = sample(1:nrow(salary), floor(nrow(salary)*0.9))
test = -train
y.test = y[test]
grid = 10^seq(5, -5, length = 100) 
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12)
err.i = rep("NA", length(grid))
for (i in 1:length(grid)){
  lasso.pred = predict(lasso.mod, s = grid[i], newx = x[test,])
  err.i[i] = mean((lasso.pred - y.test)^2)
}
plot(log(grid), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
     main = 'Test MSE among different Lambdas', ylim = c(0, 10))
bestlam = grid[which.min(err.i)]
points(log(grid)[bestlam], err.i[bestlam], col ="red", cex=2, pch=20)
# high values of lambda are like fitting just the intercept

# using 10 folds CV
set.seed (1)
cv.out = cv.glmnet(x[train ,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred - y.test)^2)

lasso.coef = predict(lasso.mod, type = "coefficients", s=bestlam)[1:ncol(x),] 
lasso.coef[lasso.coef!=0] 

# using best subset
require(leaps)
dataBS = as.data.frame(cbind(y, x))
best.sub = regsubsets(y ~ x, data = dataBS, nvmax = nrow(coef(lasso.mod)))
best.sub.summary = summary(best.sub)
names(best.sub.summary)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
# built-in plots
?plot.regsubsets
par(mfrow=c(1,1))
plot(best.sub, scale = "r2")
plot(best.sub, scale = "adjr2")
plot(best.sub, scale = "Cp")
plot(best.sub, scale = "bic")
# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))

```


Code for the presentation


```{r}

# BRUTTA COPIA PER PRESENTAZIONE 
plot.regsubsets2 <-  
  function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
     "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
  { 
    obj <- x 
    lsum <- summary(obj) 
    par(mar = c(7, 5, 6, 3) + 0.1) 
    nmodels <- length(lsum$rsq) 
    np <- obj$np 
    propscale <- FALSE 
    sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
                     nomatch = 0) 
    if (sscale == 0)  
      stop(paste("Unrecognised scale=", scale)) 
    if (propscale)  
      stop(paste("Proportional scaling only for probabilities")) 
    yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
    up <- switch(sscale, -1, -1, 1, 1) 
    index <- order(yscale * up) 
    colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
                                                           1e-04)), -log(pmax(yscale, 1e-04))) 
    image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
                       NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
          x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
          col = col) 
    laspar <- par("las") 
    on.exit(par(las = laspar)) 
    par(las = 2) 
    axis(1, at = 1:np, labels = labels, ...) # I modified this line 
    axis(2, at = 1:nmodels, labels = signif(yscale[index], 2)) 
    if (!is.null(main))  
      title(main = main) 
    box() 
    invisible(NULL) 
  } 
 
 
require(glmnet) 
set.seed(1) 
 
# crate an X matrix excluding intercept 
# x = cbind(salary$sal_midManager, salary$sal_employee, salary$sal_worker, salary$sal_18_25, salary$sal_51plus) 
# y = salary$sal_executive 
# probably for ridge 
names(salary) 
y = salary$sal_18_25 
x = as.matrix(cbind(salary[, c(4:8, 13, 19:20, 27:31)])) 
x = as.matrix(cbind(x, x*x, x*x*x)) 
names(x) 
 
# # grid for lambda values 
# grid = 10^seq(3, -2, length = 100) 
# lasso.mod = glmnet(x, y, alpha = 1, lambda = grid) 
# dim(coef(lasso.mod)) 
# # the norms are increasing in value because of the shrinkage 
# lasso.mod$lambda[1]                  # lambda value 
# sqrt(sum(coef(lasso.mod)[-1,1]^2))   # L2 norm of its coeff 
# lasso.mod$lambda[51] 
# sqrt(sum(coef(lasso.mod)[-1,51]^2)) 
# lasso.mod$lambda[90] 
# sqrt(sum(coef(lasso.mod)[-1,90]^2)) 
# # predict values for a new lambda, e.g. OLS 
# OLS = predict(lasso.mod, s = 0, type = "coefficients")[1:nrow(coef(lasso.mod)),] 
 
# split the date leaving the 10% for CV 
train = sample(1:nrow(salary), floor(nrow(salary)*0.9)) 
test = -train 
y.test = y[test] 
grid = 10^seq(3, -2, length = 100) 
lasso.mod = glmnet(x[train,], y[train], alpha = 0.5, lambda = grid, thresh = 1e-12) 
err.i = rep("NA", length(grid)) 
for (i in 1:length(grid)){ 
  lasso.pred = predict(lasso.mod, s = grid[i], newx = x[test,]) 
  err.i[i] = mean((lasso.pred - y.test)^2) 
} 
plot(log(grid), err.i, xlab = 'log Lambda', ylab = 'test set MSE',  
     main = 'Test MSE among different Lambdas', ylim = c(0, 2)) 
bestlam = grid[which.min(err.i)] 
points(log(grid)[bestlam], err.i[bestlam], col ="red", cex=2, pch=20) 
# high values of lambda are like fitting just the intercept 
 
# using 10 folds CV 
set.seed (1) 
cv.out = cv.glmnet(x[train ,], y[train], alpha = 0.5) 
plot(cv.out) #, cex.names=0.5) 
# mtext("My 'Title' in a strange place", line=15) 
mtext(expression("Best lambda for salary 18-25 using elastic net \n           with alpha=0.5 and 10-folds CV"), outer=TRUE,  cex=1.2, line=-3) 
bestlam = cv.out$lambda.min 
bestlam 
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test ,]) 
mean((lasso.pred - y.test)^2) 
 
lasso.coef = predict(lasso.mod, type = "coefficients", s=bestlam)[1:ncol(x),] 
lasso.coef[lasso.coef!=0] 
 
 
# using best subset 
require(leaps) 
dataBS = as.data.frame(cbind(y, x)) 
# salary[, c(4:8, 13, 19:20, 27:31)]) 
# assign names 
namesdataBS =names(dataBS) 
lll = ((dim(dataBS)[2]-1)/3) 
for (i in 1:lll+1){ 
  str = namesdataBS[i] 
  namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1] 
} 
namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll)) 
namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll)) 
 
names(dataBS) = namesdataBS 
 
best.sub = regsubsets(y ~ ., data = dataBS, nvmax = nrow(coef(lasso.mod))) 
best.sub.summary = summary(best.sub) 
names(best.sub.summary) 
# manual plotting 
par(mfrow =c(2,2)) 
# rsq 
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
ind_Rsq = which.max(best.sub.summary$rsq) 
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# adjRsq 
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
ind_adjRsq = which.max(best.sub.summary$adjr2) 
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# Cp 
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
ind_Cp = which.min(best.sub.summary$cp) 
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# bic 
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
ind_bic = which.min(best.sub.summary$bic) 
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# mtext("My 'Title' in a strange place", line=15) 
mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
 
# built-in plots 
?plot.regsubsets 
par(mfrow=c(1,1)) 
plot(best.sub, scale = "r2") 
plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.7) 
mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.2, line=-3.5) 
plot(best.sub, scale = "Cp") 
plot(best.sub, scale = "adjr2") 
# retrieve the model with min BIC 
coefficients(best.sub, which.min(best.sub.summary$bic))
 
```



Model salaries for people aged 18-25, which seems more difficult to predict:
```{r, fig.keep='all'}

plot(y = salary$sal_18_25, x = salary$sal_51plus)
# there are clear outliers
ind_out = salary$sal_18_25 > 13
# check if is so in all dimensions (apparently not)
col_col <- rep("black", nrow(salary))
col_col[ind_out] <- "red"
# pairs(salary[, c(3:8, 13, 18:20)], col = col_col)

# use log scale
y = log(salary$sal_midManager[!ind_out])
x = log(salary$sal_51plus[!ind_out])
hist(y, 30)
plot(y = y, x = x)

# evaluate an OLS fit
fit_LM_18_25 = lm(y ~ x)
summary(fit_LM_18_25) 
plot(y = y, x = x)
abline(fit_LM_18_25, lwd=3, col="red")

# using more predictors
x = cbind.data.frame(salary$sal_26_50, salary$sal_51plus, salary$sal_general, salary$sal_executive, salary$sal_midManager, salary$sal_employee, salary$sal_worker)
x = x[!ind_out,]
yx = cbind.data.frame(y, x, x*x, x*x*x, log(x))
# assign right names
namesdataBS = c("y", names(x))
lll = ((dim(yx)[2]-1)/4) 
# delete first _ in each name
for (i in 1:lll+1){ 
  str = namesdataBS[i] 
  namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1] 
} 
# assign new names
namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll)) 
namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll)) 
namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll)) 
names(yx) = namesdataBS 

# LM fit
fit_LM_2 = lm(y ~ ., data = yx)
summary(fit_LM_2)

# Elastic net with 10-folds CV
# using 10 folds CV
  # install.packages("doParallel")
  # library(doParallel)
  # cl <- makeCluster(4)
  # registerDoParallel(cl)
x = as.matrix(yx[, 2:ncol(yx)])
par(mfrow =c(3,2))
for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
  set.seed (3)
  cv.out = cv.glmnet(x[train ,], y[train], alpha = j)
  plot(cv.out)
  title(paste("alpha = ", j), line = 2.5)
}


# split the date leaving the 20% for CV
train = sample(1:nrow(x), floor(nrow(x)*0.8))
test = -train
y.test = y[test]
x = as.matrix(x)
par(mfrow =c(3,2))
for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
  set.seed (3)
  lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
  err.i = rep("NA", length(lasso.mod$lambda))
  for (i in 1:length(lasso.mod$lambda)){
    lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,])
    err.i[i] = mean((lasso.pred - y.test)^2)
  }
  plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
       main = 'Test MSE among different Lambdas', type = "b", col = j)
}
bestlam = which.min(err.i)
points(log(grid)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)


# using best subset
require(leaps)
dataBS = as.data.frame(cbind(y, x))
best.sub = regsubsets(y ~ x, data = dataBS, nvmax = nrow(coef(lasso.mod)))
best.sub.summary = summary(best.sub)
names(best.sub.summary)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
# built-in plots
?plot.regsubsets
par(mfrow=c(1,1))
plot(best.sub, scale = "r2")
plot(best.sub, scale = "adjr2")
plot(best.sub, scale = "Cp")
plot(best.sub, scale = "bic")
# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))
names(coefficients(best.sub, which.min(best.sub.summary$bic)))


```

ANOVA model for salary:
```{r}

# create response variable
sal_y = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus,
        salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
        salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus,
        salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker)

n_sal_y = length(sal_y)             # length response variable
n_cat = length(salary$sal_M_18_25)  # length of each category (i.e., original vectors)

# create sex dummy variable, 1 for males and 0 for females
sal_sex = rep(0, n_sal_y)   # full regressors
sal_sex[1:n_sal_y/2] = 1    # assign males

# create age dummy variables, 18-25 years old is the base case
sal_age = cbind(rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# 26-50 y.o.
sal_age[(n_cat+1):(n_cat*2), 1] = 1     # males
sal_age[(n_cat*8+1):(n_cat*9), 1] = 1   # females
# 51+ y.o.
sal_age[(n_cat*2+1):(n_cat*3), 2] = 1   # males
sal_age[(n_cat*9+1):(n_cat*10), 2] = 1  # females

# create job type dummy variables, worker is the base case
sal_job = cbind(rep(0, n_sal_y), rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# executives
sal_job[(n_cat*3+1):(n_cat*4), 1] = 1     # males
sal_job[(n_cat*10+1):(n_cat*11), 1] = 1   # females
# middle managers
sal_job[(n_cat*4+1):(n_cat*5), 2] = 1     # males
sal_job[(n_cat*11+1):(n_cat*12), 2] = 1   # females
# employee
sal_job[(n_cat*5+1):(n_cat*6), 3] =   1   # males
sal_job[(n_cat*12+1):(n_cat*13), 3] = 1   # females

# final data set 
data_ANOVA = cbind.data.frame(response = sal_y, sex = sal_sex, age = sal_age, job = sal_job)
names(data_ANOVA)
# show regressors' shape
imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# sub sample to avoid correlation
set.seed(20)
subs = sample(1:n_sal_y, size = round(n_sal_y*0.2))
data_ANOVA = data_ANOVA[subs,]
# show randomized data
imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 

# plot response variable
hist(data_ANOVA[,1], 30)
hist(log(data_ANOVA[,1]), 30)
hist(sqrt(data_ANOVA[,1]), 30)
hist(data_ANOVA[,1]^-1, 30)
sal_y = data_ANOVA[,1]^-1  # also suggested by Box-Cox transformation
ggplot(data=data.frame(data_ANOVA[,1]), aes(data_ANOVA[,1]^-1)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 40) + 
  geom_density(col="black") + 
  labs(x="salary^-1", y="Density") + 
  ggtitle("Transformation of salary found using Box-Cox transformation") + 
  theme(plot.title = element_text(hjust = 0.5)) 

# ANOVA model
sal_ANOVA = lm(sal_y ~ sal_sex[subs] + sal_age[subs] + sal_job[subs] + sal_sex[subs]:sal_age[subs] + sal_sex[subs]:sal_job[subs])
summary(sal_ANOVA)
anova((sal_ANOVA))

# box-cox transformation suggested to use y^-1
boxcox(sal_ANOVA)

# adding contrasts?
# add CV and LASSO!
```


PCA for salary:
```{r}
myPr <- prcomp(salary[, 3:26], scale = TRUE)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0, cex = 0.5)
str(myPr)
#myPr$x #checking principal component scores
salary2 <- cbind(salary, myPr$x[, 1:2])
head(salary2)
#plot with ggplot...
#require(ggplot2)
ggplot(salary2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(salary[, 3:26], salary2[,27:28])

ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 1, varname.adjust = 1) + 
  ggtitle("Biplot for the firms's size") + 
  theme(plot.title = element_text(hjust = 0.5)) 
  
```


## What we have learned ##

* Check the outlier in sal_18_25: which city is it, etc.
* evaluate possible multicollinearity
* Try a regression with all predictors ans lasso

## How to use these data ##

* ...


# Analyze population data #

## Pre-processing ##


```{r}
# preliminary checks
names(population)
summary(population)

# Drop unnecessary columns (NIVGEO is the same for all)
population <- subset(population, select = -c(NIVGEO, LIBGEO))

# converting CODGEO to numeric
population$CODGEO <- as.numeric(as.character(population$CODGEO))


# Refactor sex and MOCO
population$MOCO <- factor(population$MOCO, levels = c(11,12,21,22,23,31,32),
                          labels = c("children_living_with_two_parents", "children living with one parent",
                                     "adults_living_in_couple_without_child", "adults_living_in_couple_with_children",
                                     "adults_living_alone_with_children","persons not from family living in the home",
                                     "persons_living_alone"))
population$sex <- factor(population$sex, levels = c(1,2), labels = c("Male", "Female"))
head(population)
# Take out rows with NB (number of people in this category) equal to 0
# population <- population[population$peopleCategNum != 0,]

head(population)
summary(population)


```

## Descriptive statistics ##

```{r}
# #Compare age categories:
# #library(ggplot2)
# #  number of units
# n_cat <- length(population$CODGEO)
# # extract unique categories
# uniq_cat <- unique(population$ageCateg5!=5)
# # vector representing sex for each category
# Label <- c(rep(c('Male', 'Female'), n_cat))
# # vector representing the variable considered
# Variable <- rep(uniq_cat, n_cat/length(uniq_cat))
# Value=population$peopleCategNum
# merge these data
#pop_categ = cbind.data.frame(Label = Label, 
#             value = Value,
#             Variable = Variable)
#p <- ggplot(data = pop_categ, aes(x=Label, y=value)) 
#p <- p + geom_boxplot(aes(fill = Label))
# if you want color for points replace group with colour=Label
#p <- p + geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75))
#p <- p + facet_wrap( ~ Variable, scales="free")
#p <- p + xlab("x-axis") + ylab("y-axis") + ggtitle("Category comparison")
# p <- p + guides(fill=guide_legend(title="Legend"))
#p
```


```{r}
# Restructure population data to produce the demographic profile per town
# install.packages("plyr")
library(plyr)
population_per_town_data <- ddply(population, .(CODGEO), function(population) {
  data.frame(total_population = sum(population$peopleCategNum),
             male = sum(population[population$sex == "Male",]$peopleCategNum),
             female = sum(population[population$sex == "Female",]$peopleCategNum),
             child = sum(population[population$ageCateg5 %in% seq(0, 10, by=5),]$peopleCategNum),
             elderly = sum(population[population$ageCateg5 %in% seq(65, 80, by=5),]$peopleCategNum),
             workforce = sum(population[population$ageCateg5 %in% seq(15, 60, by=5),]$peopleCategNum) 
  )})

population_per_town_data$dependent <- population_per_town_data$child + population_per_town_data$elderly
population_per_town_data$sex_ratio <- ifelse(population_per_town_data$female==0, 0, population_per_town_data$male / population_per_town_data$female)
population_per_town_data$dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$dependent / population_per_town_data$workforce)
population_per_town_data$aged_dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$elderly / population_per_town_data$workforce)
population_per_town_data$child_dependency_ratio <- ifelse(population_per_town_data$workforce==0, 0, population_per_town_data$child / population_per_town_data$workforce)

summary(population_per_town_data)
```

```{r}
# Scale population to log
hist(population_per_town_data$total_population, ylim=c(0,40000), breaks = seq(0, 2500000, by=250000), xlab="", main = "", labels=T, col ="light blue")
population_per_town_data$total_population_log <- log10(population_per_town_data$total_population)

# Merge geo and pop
# geo_pop_by_town <- merge(geo, population_per_town_data)
# summary(geo_pop_by_town)


# Plot "Distribution of Population for each Town"
#myPalette(low = "white", high = c("green", "red"), mid=NULL, k =50)-Need "GLAD" package
# sc <- scale_colour_gradientn(colours =palette(rainbow(8)), limits=c(min(geo_pop_by_town$total_population_log), max(geo_pop_by_town$total_population_log)))
# poppulation_distribution <- 
#   FraMap + 
#   geom_point(aes(x=geo_pop_by_town$longitude, y=geo_pop_by_town$latitude, colour=geo_pop_by_town$total_population_log), 
#              data=geo_pop_by_town, alpha=0.8, size=0.6) + 
#   sc +
#   #geom_text(aes(label = town_name, x = longitude, y = latitude), 
#             #data = subset(geo_pop_by_town, total_population_log %in% head(sort(total_population_log, decreasing=TRUE), 3)), check_overlap = TRUE, size=7) +
# 
#   labs(color='Total Population in Log') +
#   ggtitle("Distribution of Population for each Town") 
# poppulation_distribution
```

```{r}
# Group population data by department because of small size of some towns and the given geojson file of department
# pop_by_department <- ddply(geo_pop_by_town, .(department), function(geo_pop_by_town) {
#   data.frame(total_population = sum(geo_pop_by_town$total_population),
#              male = sum(geo_pop_by_town$male),
#              female = sum(geo_pop_by_town$female),
#              child = sum(geo_pop_by_town$child),
#              elderly = sum(geo_pop_by_town$elderly),
#              dependent = sum(geo_pop_by_town$dependent),
#              workforce = sum(geo_pop_by_town$workforce) 
#   )})
# 
# pop_by_department$dependency_ratio <- pop_by_department$dependent / pop_by_department$workforce
# pop_by_department$aged_dependency_ratio <- pop_by_department$elderly / pop_by_department$workforce
# pop_by_department$child_dependency_ratio <- pop_by_department$child / pop_by_department$workforce
# summary(pop_by_department)
```

```{r}
# Scale population to log
# pop_by_department$total_population_log <- log10(pop_by_department$total_population)
# 
# # Merge geo and pop
# geo_pop_by_department <- merge(geo, pop_by_department)
# summary(geo_pop_by_department)
```


```{r}
# Plot "Distribution of Population for each department"
#myPalette(low = "white", high = c("green", "red"), mid=NULL, k =50)-Need "GLAD" package
# sc <- scale_colour_gradientn(colours =palette(rainbow(8)), limits=c(min(geo_pop_by_department$total_population_log), max(geo_pop_by_department$total_population_log)))
# pop_distribution_department <- 
#   FraMap + 
#   geom_point(aes(x=geo_pop_by_department$longitude, y=geo_pop_by_department$latitude, colour=geo_pop_by_department$total_population_log), 
#              data=geo_pop_by_department, alpha=0.8, size=0.6) + 
#   sc +
#   #geom_text(aes(label = town_name, x = longitude, y = latitude), 
#             #data = subset(geo_pop_by_department, total_population_log %in% head(sort(total_population_log, decreasing=TRUE), 3)), check_overlap = TRUE, size=7) +
#   labs(color='Total Population in Log') + 
#   ggtitle("Distribution of Population for each department") 
# pop_distribution_department
```


```{r}
population_data2 <- ddply(population, .(sex, ageCateg5), function(population) {
  data.frame(total_population = sum(population$peopleCategNum))
  })
pop_pyramid <- ggplot(data = population_data2, 
       mapping = aes(x = ageCateg5, fill = sex, 
                     y = ifelse(test = sex == "Male", 
                                yes = -total_population, no = total_population))) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(population_data2$total_population) * c(-1,1)) + ggtitle("Pyramid of Population") + theme(plot.title = element_text(hjust = 0.5)) + labs(x= "Age")+ labs(y = "Population") + coord_flip()
print(pop_pyramid)
```


```{r}
#Plot dependency ratio
# sc <- scale_colour_gradientn(colours =palette(rainbow(5)), limits=c(min(geo_pop_by_town$dependency_ratio), max(geo_pop_by_town$dependency_ratio)))
# dependency_ratio_plot<- 
#   FraMap + 
#   geom_point(aes(x=geo_pop_by_town$longitude, y=geo_pop_by_town$latitude, colour=geo_pop_by_town$dependency_ratio),
#              data=geo_pop_by_town, alpha=0.8, size=0.6) + 
#   sc +
#   labs(color='dependency ratio') +
#   ggtitle("Dependency ratio of each town") 
# dependency_ratio_plot
```


## PCA ##




# Produce consistent datasets #

The CODGEO variables (code_insee in the original geo data) have to be merged.
However, for different reasons already identified by other kaggle users they need some pre-processing.
To do so, some already known mistakes are corrected:

```{r}
firms$CODGEO      <- sub("A", "0", firms$CODGEO)
firms$CODGEO      <- sub("B", "0", firms$CODGEO)
salary$CODGEO     <- sub("A", "0", salary$CODGEO)
salary$CODGEO     <- sub("B", "0", salary$CODGEO)
population$CODGEO <- sub("A", "0", population$CODGEO)
population$CODGEO <- sub("B", "0", population$CODGEO)
geo$CODGEO        <- sub("A", "0", geo$CODGEO)
geo$CODGEO        <- sub("B", "0", geo$CODGEO)

# [Check if there are more]
```


Then all CODGEO are trasformed to integers and four new datasets are created retaining only the common CODGEO:
```{r}
# use only integer values
geo$CODGEO = as.integer(geo$CODGEO)  # already integer
population$CODGEO = as.integer(population$CODGEO)
firms$CODGEO = as.integer(firms$CODGEO)
salary$CODGEO = as.integer(salary$CODGEO)

# store datasets' names to loop on them
dataset = c("population", "salary", "firms", "geo")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and create a new variable concateneting "NEW" at the end
  nam <- paste(i, "NEW", sep = "")
  # initialize counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # for each dataset different from the i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(i))
  del_obs = paste("For", i, del_rows, "units have been deleted.",
                  "They were the", round(del_prop*100, digits=2), "% of the total.", sep = " ")
  print(paste(del_obs))
}

print(paste("The new dataset has", nrow(salaryNEW), "units and", 
      ncol(salaryNEW)+ncol(populationNEW)+ncol(firmsNEW)+ncol(geoNEW), "features."))
```


Create population of the same length and merge the data:
```{r}
# 
# popNEWnames = c("CODGEO", 
#                 c(unique(as.character(populationNEW$MOCO))), 
#                 c(unique(as.character(populationNEW$ageCateg5))),
#                 c(unique(as.character(populationNEW$sex))),
#                 "Pop")
# matrix("NA", dim(populationNEW[1], ))
head(population)
summary(population)
# general size of a town
popNEW = ddply(populationNEW, .(CODGEO), summarize, popTot=sum(peopleCategNum))
head(popNEW)

# merging
population$CODGEO = as.integer(population$CODGEO)
newDat = merge(firmsNEW, popNEW, by="CODGEO")
names(newDat)
head(newDat)
newDat = merge(newDat, salaryNEW, by="CODGEO")
names(newDat)
head(newDat)
newDat = merge(newDat, geoNEW, by="CODGEO")
names(newDat)
head(newDat)
newDat = subset(newDat, select = -town.y)

```


# Analysis #

General:
```{r}
corrplot(cor(newDat[, 3:20]), method = "circle", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", type = "lower", mar=c(0,0,1.5,0))
corrplot(cor(newDat[, 21:41]), method = "circle", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, type = "lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0))
```


Firms and geo:
```{r}
#grouping Regions according to CODGEO
geoNEW_sub <- geoNEW[, c("region", "CODGEO")] 
firms_regions <- merge(geoNEW_sub, firmsNEW, by = "CODGEO") #merging firms and geo datasets
firms_regions_total <- aggregate(firms_regions$total, by = list(region = firms_regions$region), FUN = sum) #total number of firms per region
firms_regions_micro <- aggregate(firms_regions$micro, by = list(region = firms_regions$region), FUN = sum) #number of micro firms per region
firms_regions_small <- aggregate(firms_regions$small, by = list(region = firms_regions$region), FUN = sum) #number of small firms per region
firms_regions_medium <- aggregate(firms_regions$medium, by = list(region = firms_regions$region), FUN = sum) #number of small firms per region
firms_regions_large <- aggregate(firms_regions$large, by = list(region = firms_regions$region), FUN = sum) #number of small firms per region

colnames(firms_regions_total) <- c("regions", "Total")
colnames(firms_regions_micro) <- c("regions", "Micro")
colnames(firms_regions_small) <- c("regions", "Small")
colnames(firms_regions_medium) <- c("regions", "Medium")
colnames(firms_regions_large) <- c("regions", "Large")

firms_regions_total$Percentage_total <- (firms_regions_total$Total/sum(firms_regions_total$Total))*100 #adding percentage column
firms_regions_micro$Percentage_micro <- (firms_regions_micro$Micro/sum(firms_regions_micro$Micro))*100
firms_regions_small$Percentage_small <- (firms_regions_small$Small/sum(firms_regions_small$Small))*100
firms_regions_medium$Percentage_medium <- (firms_regions_medium$Medium/sum(firms_regions_medium$Medium))*100
firms_regions_large$Percentage_large <- (firms_regions_large$Large/sum(firms_regions_large$Large))*100

ggplot(firms_regions_total, aes(x=regions,y = Percentage_total)) +
  geom_bar(col="black", fill = "blue", alpha = .5, stat = "identity") 
  
ggplot(firms_regions_micro, aes(x=regions,y = Percentage_micro)) +
  geom_bar(col="black", fill = "blue", alpha = .5, stat = "identity") 
  
ggplot(firms_regions_small, aes(x=regions,y = Percentage_small)) +
  geom_bar(col="black", fill = "blue", alpha = .5, stat = "identity")

ggplot(firms_regions_medium, aes(x=regions,y = Percentage_medium)) +
  geom_bar(col="black", fill = "blue", alpha = .5, stat = "identity")

ggplot(firms_regions_large, aes(x=regions,y = Percentage_large)) +
  geom_bar(col="black", fill = "blue", alpha = .5, stat = "identity")
```
## PCA ##
```{r}

```
## Regression ##
```{r}




```
## Clustering ##
```{r}

```