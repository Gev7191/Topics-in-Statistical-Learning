---
title: "TSL Project"
author: "Luca Insolia, Jisu Kim and Gevorg Yeghikyan"    
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_notebook:
    toc: yes
    toc_depth: '3' 
    highlight: tango
    keep_tex: yes
    number_sections: yes
  html_document:
    theme: united
    highlight: tango    
    toc: yes
    toc_depth: '3' 
#   pdf_document: # ADD PDF!
#     toc: yes
#     toc_depth: '3'
#     keep_tex: true
#     latex_engine: pdflatex
# editor_options:
#   # toc: yes
#   # toc_depth: 3
#   chunk_output_type: inline
---


# General information 

We analyze a dataset published on [Kaggle](https://www.kaggle.com/etiennelq/french-employment-by-town).
It refers to french employment, salaries, population per town. 
The aim is to evaluate equality/inequalities in France, and geographical distribution of business according to their size.

Such data are collected by the INSEE.
Information regarding the number of firms in every french town, categorized by size
can be found [here](https://www.insee.fr/fr/metadonnees/definition/c1135). This dataset contains about 35000 units/per town.

Information about salaries around french town per job categories, age and sex (expressed in average net amount per hour in euro) can be found [here](https://www.insee.fr/fr/statistiques/2522515). This dataset contains about 5000 units/per town. 

Demographic information in France per town, age, sex and living mode
can be found [here](https://www.insee.fr/fr/statistiques/2863607). This dataset contains about 8 million units/per town. Additional info about Population Data can be found [here](https://www.insee.fr/fr/statistiques/2863607#dictionnaire). 

These datasets have been pre-processed and put together. The final dataset contains 58 variables and 5022 observations. 


## Aim of the study 

This project aims to explore structure of French labour market. 
In particular, we are interested in:

* evaluating possible inequalities: per towns/region, sex, age, job categories etc.;
* discover geographical distribution of business according to their size
* predicting the ... using a regression model;
* reduce the dimensionality of ... performing a PCA;
* explore different algorithms to cluster male/females using ...


## Plan for the study 

1. Unsupervised learning: 
* PCA
* Clustering methods (K-means/Hierarchical)
2. Supervised learning:
* GLM
* Linear/Quadratic Discriminant Analysis
* KNN
* Cross-validation
* Bootstrap
* Subset selection 
* Shrinkage methods
* Dimension reduction methods
3. Description of population demographics in France
4. Structure of the french labour market
5. ...
6. Future works


## Loading tools

Loading all libraries needed throughout the notebook:
```{r loading libraries}

# data manipulation
if(!require(psych)){install.packages("psych"); library(psych)} 
if(!require(plyr)){install.packages("dplyr"); library(plyr)}
if(!require(dplyr)){install.packages("dplyr"); library(dplyr)}
# plotting
if(!require(RColorBrewer)){install.packages("RColorBrewer"); library(RColorBrewer)} # colors
if(!require(zoo)){install.packages("zoo"); library(zoo)} # colors
if(!require(ggplot2)){install.packages("ggplot2"); library(ggplot2)}
if(!require(ggfortify)){install.packages("ggfortify"); library(ggfortify)}
if(!require(ggmap)){install.packages("ggmap"); library(ggmap)}
if(!require(ggbiplot)){
  if(!require(devtools)){
    install.packages("devtools")}
  install_github("vqv/ggbiplot")
  library(ggbiplot)}
if(!require(corrplot)){install.packages("corrplot"); library(corrplot)}
if(!require(zoo)){install.packages("zoo"); library(zoo)}
if(!require(plotly)){install.packages("plotly"); library(plotly)} # interactive plotting
if(!require(lattice)){install.packages("lattice"); library(lattice)}
if(!require(MVA)){install.packages("MVA"); library(MVA)}
if(!require(ape)){install.packages("ape"); library(ape)}
if(!require(KernSmooth)){install.packages("KernSmooth"); library(KernSmooth)}
if(!require(rgl)){install.packages("rgl"); library(rgl)}
if(!require(car)){install.packages("car"); library(car)}
# Clustering
if(!require(mclust)){install.packages("mclust"); library(mclust)}
if(!require(dbscan)){install.packages("dbscan"); library(dbscan)}
# plot design matrix
if(!require(rafalib)){install.packages("rafalib"); library(rafalib)}
# cross validation
if(!require(boot)){install.packages("boot"); library(boot)}
# elastic net
if(!require(glmnet)){install.packages("glmnet"); library(glmnet)}
# group Lasso
if(!require(gglasso)){install.packages("gglasso"); library(gglasso)}
# robust fit
if(!require(robustbase)){install.packages("robustbase"); library(robustbase)}
# robust Lasso 
if(!require(robustHD)){install.packages("robustHD"); library(robustHD)}
if(!require(enetLTS)){install.packages("enetLTS"); library(enetLTS)}
# box-cox tranformation (in ANOVA) and standardized residuals
if(!require(MASS)){install.packages("MASS"); library(MASS)}
# best subset selection
if(!require(leaps)){install.packages("leaps"); library(leaps)}
# plot for PCA
# if(!require(factoextra)){install.packages("factoextra"); library(factoextra)}

```

Import the datasets:
```{r loading datasets, warning=FALSE}

setwd("./data")
firms       <- read.csv("base_etablissement_par_tranche_effectif.csv", encoding = "UTF-8")
geo         <- read.csv("name_geographic_information.csv", encoding = "UTF-8")
salary      <- read.csv("net_salary_per_town_categories.csv", encoding = "UTF-8")
population  <- read.csv("population.csv", encoding = "UTF-8")
educ        <- read.csv("level_education.csv", sep =";", encoding = "UTF-8")
categ_socio <- read.csv("Categorie_socioprofessionnelle.csv", sep =";", encoding = "UTF-8")
status_work <- read.csv("Emplois_lieu_travail.csv", sep =";", encoding = "UTF-8")
ineq        <- read.csv("Comparateur_territoires.csv", sep =";", encoding = "UTF-8")
commune     <- read.csv("insee_commune.csv", encoding = "UTF-8")

```


# Pre-processing

## Firms data
 
Assign meaningful names and check the modified data:
```{r pre processing firms}

names(firms) 
names(firms)[2:ncol(firms)] <-
  c("town", 
    "regNum",
    "deptNum",
    "total",
    "null",
    "firmsEmpl_1_5",
    "firmsEmpl_6_9",
    "firmsEmpl_10_19",
    "firmsEmpl_20_49",
    "firmsEmpl_50_99",
    "firmsEmpl_100_199",
    "firmsEmpl_200_499",
    "firmsEmpl_500plus")

# preliminary checks
names(firms)
head(firms)
str(firms)
summary(firms)

# Check for duplicated data: there is no
sum(duplicated.data.frame(firms))

```

Categorize firms' size according to 
[EU standard](http://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Enterprise_size), 
but in a slightly different form for medium and large firms 
(i.e., medium firms have <200 instead of <250 employees):
```{r modify firms}

# merge variables
firms$micro   <- firms$firmsEmpl_1_5 + firms$firmsEmpl_6_9
firms$small   <- firms$firmsEmpl_10_19 + firms$firmsEmpl_20_49
firms$medium  <- firms$firmsEmpl_50_99 + firms$firmsEmpl_100_199
firms$large   <- firms$firmsEmpl_200_499 + firms$firmsEmpl_500plus

# Drop unnecessary (at the moment) columns 
firms <- subset(firms, select = c(CODGEO, town, total, micro, small, medium, large, null))

# check
summary(firms)

# there is an obs with more than 316K null data: we check if it is plausible
# get the highest 20 null values
str_firms <- sort(firms$null, decreasing = T)[1:20]
# get their indexes
str_firms_ind <- match(str_firms, firms$null)
# get the corresponding city
firms$town[str_firms_ind]
# they are the largest cities, hence it seems reasonable..

```

## Geographical data 

Assign names and remove some variables:
```{r pre processing geo}

names(geo)
names(geo)[c(2:11, 14)] =
  c("code_region",
    "region", 
    "region_capital",
    "number_depart",
    "department", 
    "prefecture",
    "circons",
    "town_name", 
    "postal_code", 
    "CODGEO",
    "eloignement")

# drop unnecessary columns (code/num and name represents same thing) 
geo <- subset(geo, select = -c(EU_circo, code_region, number_depart, prefecture, circons, eloignement, town_name))

# preliminary checks
names(geo)
head(geo)
str(geo)
summary(geo)

```

Correct typos for longitude data and keep just the unique CODGEO to avoid towns with multiple postal codes:
```{r fix geo}

# spot "," instead of "." in longitude
newLong       <- as.character(geo$longitude)    # copy the vector
sum(grep(",", newLong))                         # total commas
ind_long_err  <- grep(",", newLong)             # indexing them
newLong       <- gsub(",", ".", newLong)        # substituting them with dots
indNA_Long    <- is.na(as.numeric((newLong)))   # spot NA
# geo$longitude[indNA_Long]                       # verify that they were actually missing
geo$longitude <- as.numeric(newLong)            # overwrite the longitude variable with the new one

# Check for duplicated data (e.g., cities with different postal codes, that we dropped):
  # e.g., to verify it,  try on the initial dataset:
  # sum(geo$nom_commune == "Paris")
  # ind_duplic <- geo$nom_commune == "Paris"
  # geo[ind_duplic,]
sum(duplicated.data.frame(geo)) 
# retaing unique postal cities
geo <- geo[!duplicated(geo$CODGEO),]

# check again
summary(geo)

```

Assign latitude and longitude values for missing data (almost 3000).
The code used to retrieve them using Google API has been commented and its result is loaded.
```{r assign NA in geo, warning=FALSE}

# index of NAs and their total
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)

# code used to retrieve the NA using Google API, which have been saved in a csv file
# 
# # initialize variables
# city_search = 0
# res = as.data.frame(matrix(c(0, 0, 0), 1, 3))
# names(res) = c("lon", "lat", "address")
# 
# # retrieve lat and long (Google API = 2500 request per day)
# # my_iter = floor(sum(indNA_coord)/3)
# for (i in 1:sum(indNA_coord)){
# 
#   # city searched
#   city_search[i] = paste(c(as.character(NA_coord$town_name[i]), as.character(NA_coord$postal_code[i]), as.character(NA_coord$department[i]), "France"), sep=" ", collapse = ", ")
#   
#   # solution
#   res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
# 
#   # retrieve still missing data, because of existing problems with API (up to 15 trials)
#   j = 0
#   while (any(is.na(res[i,])) & j < 25){
#     res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
#     j = j + 1
#   }
# }

# # check the solution
# sol = cbind(searched = city_search, res)

# # save it as a csv file to save time
# write.csv(retrieved_geo_NA[,2:3], "geo_NA_Final.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
    

# read the created csv
setwd("./data")
retrieved_geo_NA = read.csv("geo_NA_Final.csv", header = T, encoding = "UTF-8")
# get only long and lat and assign to original NA 
geo$latitude[indNA_coord] = retrieved_geo_NA[,2]
geo$longitude[indNA_coord] = retrieved_geo_NA[,1]

# there are 37 still missing units, which are towns located in old colonies far from Europe
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)
# exclude those towns
geo = geo[!indNA_coord,]

summary(geo)

```

Remove DOM-TOM towns (i.e. old colonies far from Europe)
```{r}

# delete non-European countries
ind_nonEur = geo$latitude < 30 | geo$latitude > 70 |geo$longitude < -20 | geo$longitude > 20
sum(ind_nonEur)
geo = geo[!ind_nonEur,]

```


## Salary data 

Assign meaningful names and check the data:
```{r pre processing salary}

names(salary)
names(salary)[2:ncol(salary)] <-
  c("town",
    "sal_general",    
    "sal_executive",
    "sal_midManager",
    "sal_employee",
    "sal_worker",
    "sal_Females",
    "sal_F_executive",
    "sal_F_midManager",
    "sal_F_employee",
    "sal_F_worker",
    "sal_Males",
    "sal_M_executive",
    "sal_M_midManager",
    "sal_M_employee",
    "sal_M_worker",
    "sal_18_25",
    "sal_26_50",
    "sal_51plus",
    "sal_F_18_25",
    "sal_F_26_50",
    "sal_F_51plus",
    "sal_M_18_25",
    "sal_M_26_50",
    "sal_M_51plus")

# preliminary checks
names(salary)
head(salary)
str(salary)
summary(salary)

# Check for duplicated data: there are no
sum(duplicated.data.frame(salary))

# drop unnecessary variable
salary <-subset(salary, select = -c(town))

```

## Population data 

Rename the variables for population and exclude the unnecessary ones:
```{r pre processing population}

#names(population)
names(population)[5:7] <-
  c("ageCateg5",
    "sex",
    "peopleCategNum")

# drop unnecessary columns (NIVGEO is the same for all)
population <- subset(population, select = -c(NIVGEO, LIBGEO))

# Refactor sex and MOCO
population$MOCO <- factor(population$MOCO, levels = c(11,12,21,22,23,31,32),
                          labels = c("children_living_with_two_parents", 
                                     "children_living_with_one_parent",
                                     "adults_living_in_couple_without_child",
                                     "adults_living_in_couple_with_children",
                                     "adults_living_alone_with_children",
                                     "persons_not_from_family_living_in_the_home",
                                     "persons_living_alone"))
population$sex <- factor(population$sex, levels = c(1,2), labels = c("Male", "Female"))

# check again
summary(population)

```

Understand the distribution of population according to different age categories using population pyramide
(performed now because later the dataset will be modified):
```{r Population pyramide}

# need the initial shape of data to construct the pyramide 
# Population pyramide
population_data2 <- ddply(population, .(sex, ageCateg5), function(population) {
  data.frame(total_population = sum(population$peopleCategNum))
  })
pop_pyramid <- ggplot(data = population_data2,
       mapping = aes(x = ageCateg5, fill = sex,
                     y = ifelse(test = sex == "Male",
                                yes = -total_population, no = total_population))) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(population_data2$total_population) * c(-1,1)) + 
  ggtitle("Pyramid of Population") + theme(plot.title = element_text(hjust = 0.5)) + 
  labs(x= "Age")+ labs(y = "Population") + coord_flip() # + scale_fill_brewer(palette = "Set1")
pop_pyramid

```

Re-organize population dataset and set CODGEO as units and create new variables using the available variables.
```{r Re-organize population dataset by CODGEO}

# Re-organize the data by creating new variables: "total population", "male", "female", "child", "elderly" and "workforce".
population <- ddply(population, .(CODGEO), function(population) {
  data.frame(total_population = sum(population$peopleCategNum),
             male = sum(population[population$sex == "Male",]$peopleCategNum),
             female = sum(population[population$sex == "Female",]$peopleCategNum),
             child = sum(population[population$ageCateg5 %in% seq(0, 10, by=5),]$peopleCategNum),
             elderly = sum(population[population$ageCateg5 %in% seq(65, 80, by=5),]$peopleCategNum),
             workforce = sum(population[population$ageCateg5 %in% seq(15, 60, by=5),]$peopleCategNum) 
  )})

# Calculate ratios using the existing variables
population$dependent <- population$child + population$elderly
population$sex_ratio <- ifelse(population$female==0, 0, population$male / population$female)
population$dependency_ratio <- ifelse(population$workforce==0, 0, population$dependent / population$workforce)
population$aged_dependency_ratio <- ifelse(population$workforce==0, 0, population$elderly / population$workforce)
population$child_dependency_ratio <- ifelse(population$workforce==0, 0, population$child / population$workforce)

# total population=65mil, which is reasonable
sum(population$total_population)

```

## Education data

Assign meaningful names and build some useful indicators:
```{r}

names(educ)[3:ncol(educ)] <-
  c("Age15_NoDip_M","Age15_NoDip_F", "Age15_Sec_M", "Age15_Sec_F", "Age15_Hi_M","Age15_Hi_F", "Age15_Univ_M",
    "Age15_Univ_F", "Age20_NoDip_M", "Age20_NoDip_F", "Age20_Sec_M","Age20_Sec_F", "Age20_Hi_M", "Age20_Hi_F",
    "Age20_Univ_M", "Age20_Univ_F","Age25_NoDip_M", "Age25_NoDip_F","Age25_Sec_M","Age25_Sec_F", "Age25_Hi_M",
    "Age25_Hi_F","Age25_Univ_M","Age25_Univ_F", "Age30_NoDip_M", "Age30_NoDip_F", "Age30_Sec_M", "Age30_Sec_F",
    "Age30_Hi_M", "Age30_Hi_F","Age30_Univ_M","Age30_Univ_F", "Age35_NoDip_M", "Age35_NoDip_F", "Age35_Sec_M",
    "Age35_Sec_F", "Age35_Hi_M", "Age35_Hi_F", "Age35_Univ_M","Age35_Univ_F", "Age40_NoDip_M", "Age40_NoDip_F",
    "Age40_Sec_M","Age40_Sec_F", "Age40_Hi_M", "Age40_Hi_F", "Age40_Univ_M","Age40_Univ_F", "Age45_NoDip_M",
    "Age45_NoDip_F","Age45_Sec_M","Age45_Sec_F", "Age45_Hi_M", "Age45_Hi_F", "Age45_Univ_M","Age45_Univ_F",
    "Age50_NoDip_M", "Age50_NoDip_F","Age50_Sec_M","Age50_Sec_F", "Age50_Hi_M", "Age50_Hi_F", "Age50_Univ_M",
    "Age50_Univ_F", "Age55_NoDip_M", "Age55_NoDip_F","Age55_Sec_M", "Age55_Sec_F", "Age55_Hi_M", "Age55_Hi_F",
    "Age55_Univ_M", "Age55_Univ_F", "Age60_NoDip_M", "Age60_NoDip_F","Age60_Sec_M","Age60_Sec_F", "Age60_Hi_M",
    "Age60_Hi_F", "Age60_Univ_M", "Age60_Univ_F", "Age65_NoDip_M", "Age65_NoDip_F","Age65_Sec_M","Age65_Sec_F",
    "Age65_Hi_M", "Age65_Hi_F", "Age65_Univ_M", "Age65_Univ_F")

# Aggregate variables by grouping all the education level categories by gender
# total number of females with no diploma
educ$female_NoDip <- rowSums(cbind(educ$Age15_NoDip_F, educ$Age20_NoDip_F, educ$Age25_NoDip_F, educ$Age30_NoDip_F,
                                   educ$Age35_NoDip_F,educ$Age40_NoDip_F,educ$Age45_NoDip_F, educ$Age50_NoDip_F,
                                   educ$Age55_NoDip_F, educ$Age60_NoDip_F, educ$Age65_NoDip_F))

# total number of males with no diploma
educ$male_NoDip <- rowSums(cbind(educ$Age15_NoDip_M, educ$Age20_NoDip_M, educ$Age25_NoDip_M, educ$Age30_NoDip_M,
                                 educ$Age35_NoDip_M,educ$Age40_NoDip_M,educ$Age45_NoDip_M, educ$Age50_NoDip_M,
                                 educ$Age55_NoDip_M, educ$Age60_NoDip_M, educ$Age65_NoDip_M))

# total number of females with secondary education level
educ$female_Sec <- rowSums(cbind(educ$Age15_Sec_F, educ$Age20_Sec_F, educ$Age25_Sec_F, educ$Age30_Sec_F,
                                 educ$Age35_Sec_F,educ$Age40_Sec_F,educ$Age45_Sec_F, educ$Age50_Sec_F, 
                                 educ$Age55_Sec_F, educ$Age60_Sec_F, educ$Age65_Sec_F))

# total number of males with secondary education level
educ$male_Sec <- rowSums(cbind(educ$Age15_Sec_M, educ$Age20_Sec_M, educ$Age25_Sec_M, educ$Age30_Sec_M,
                               educ$Age35_Sec_M,educ$Age40_Sec_M,educ$Age45_Sec_M, educ$Age50_Sec_M, 
                               educ$Age55_Sec_M, educ$Age60_Sec_M, educ$Age65_Sec_M))

# total number of females with high-school education level
educ$female_Hi <- rowSums(cbind(educ$Age15_Hi_F, educ$Age20_Hi_F, educ$Age25_Hi_F, educ$Age30_Hi_F,
                                educ$Age35_Hi_F,educ$Age40_Hi_F,educ$Age45_Hi_F, educ$Age50_Hi_F, 
                                educ$Age55_Hi_F, educ$Age60_Hi_F, educ$Age65_Hi_F))

# total number of males with high-school education level
educ$male_Hi <- rowSums(cbind(educ$Age15_Hi_M, educ$Age20_Hi_M, educ$Age25_Hi_M, educ$Age30_Hi_M,
                              educ$Age35_Hi_M,educ$Age40_Hi_M,educ$Age45_Hi_M, educ$Age50_Hi_M, 
                              educ$Age55_Hi_M, educ$Age60_Hi_M, educ$Age65_Hi_M))

# total number of females with university degree
educ$female_Univ <- rowSums(cbind(educ$Age15_Univ_F, educ$Age20_Univ_F, educ$Age25_Univ_F, educ$Age30_Univ_F,
                                  educ$Age35_Univ_F,educ$Age40_Univ_F,educ$Age45_Univ_F, educ$Age50_Univ_F,
                                  educ$Age55_Univ_F, educ$Age60_Univ_F, educ$Age65_Univ_F))

# total number of males with university degree
educ$male_Univ <- rowSums(cbind(educ$Age15_Univ_M, educ$Age20_Univ_M, educ$Age25_Univ_M, educ$Age30_Univ_M,
                                educ$Age35_Univ_M,educ$Age40_Univ_M,educ$Age45_Univ_M, educ$Age50_Univ_M,
                                educ$Age55_Univ_M, educ$Age60_Univ_M, educ$Age65_Univ_M))

# drop all the unnecessary variables 
educ <- subset(educ, select= c(CODGEO,
                               female_NoDip, male_NoDip, 
                               female_Sec, male_Sec, 
                               female_Hi, male_Hi, 
                               female_Univ, male_Univ))

# total number of population without a diploma
educ$nodip <-rowSums(cbind(educ$female_NoDip, educ$male_NoDip))

# total number of population with secondary level of education
educ$sec <-rowSums(cbind(educ$female_Sec, educ$male_NoDip))

# total number of population with highschool diploma
educ$high <-rowSums(cbind(educ$female_Hi, educ$male_Hi))

# total number of poopulation with university diploma
educ$univ <- rowSums(cbind(educ$female_Univ, educ$male_Univ))

```


## Demographic/social profiles data

Rename the original variables 
```{r pre-processing demographic/social profiles data}

# rename variables
names(categ_socio)[3:ncol(categ_socio)] <-
  c("M_immi_agri",
    "F_immi_agri",
    "M_NoImmi_agri",
    "F_NoImmi_agri",
    "M_immi_comm",
    "F_immi_comm",
    "M_NoImmi_comm",
    "F_NoImmi_comm",
    "M_immi_exec",
    "F_immi_exec",
    "M_NoImmi_exec",
    "F_NoImmi_exec",
    "M_immi_midman",
    "F_immi_midman",
    "M_NoImmi_midman",
    "F_NoImmi_midman",
    "M_immi_emp",
    "F_immi_emp",
    "M_NoImmi_emp",
    "F_NoImmi_emp",
    "M_immi_worker",
    "F_immi_worker",
    "M_NoImmi_worker",
    "F_NoImmi_worker",
    "M_immi_retired",
    "F_immi_retired",
    "M_NoImmi_retired",
    "F_NoImmi_retired",
    "M_immi_noAct",
    "F_immi_noAct",
    "M_NoImmi_noAct",
    "F_NoImmi_noAct")

# drop LIBGEO
categ_socio <- subset(categ_socio, select=-c(LIBGEO))

```

Create the total number of immigrants and natives per town, also separeted by gender.
```{r Categ_socio-rename variables }

# total number of male immigrants per town 
categ_socio$male_immig <-rowSums(cbind(categ_socio$M_immi_agri, categ_socio$M_immi_comm, categ_socio$M_immi_emp,  
                                       categ_socio$M_immi_exec, categ_socio$M_immi_midman,categ_socio$M_immi_noAct, 
                                       categ_socio$M_immi_retired, categ_socio$M_immi_worker))
 
# total number of female immigrants per town
categ_socio$female_immig <- rowSums(cbind(categ_socio$F_immi_agri,categ_socio$F_immi_comm, categ_socio$F_immi_emp, 
                                          categ_socio$F_immi_exec, categ_socio$F_immi_midman, 
                                          categ_socio$F_immi_noAct, categ_socio$F_immi_retired, categ_socio$F_immi_worker))

# total number immigrants per town
categ_socio$total_immig <- categ_socio$male_immig + categ_socio$female_immig

# working male immigrants per town 
categ_socio$male_working_immig <- categ_socio$male_immig - categ_socio$M_immi_retired 
 
# working female immigrants per town  
categ_socio$female_working_immig <- categ_socio$female_immig - categ_socio$F_immi_retired 
 
# total number of male natives per town 
categ_socio$male_native <- rowSums(cbind(categ_socio$M_NoImmi_agri, categ_socio$M_NoImmi_comm, categ_socio$M_NoImmi_exec, categ_socio$M_NoImmi_emp, categ_socio$M_NoImmi_midman, categ_socio$M_NoImmi_noAct, categ_socio$M_NoImmi_retired, categ_socio$M_NoImmi_worker))

# total number of female natives per town 
categ_socio$female_native <- rowSums(cbind(categ_socio$F_NoImmi_agri, categ_socio$F_NoImmi_comm, categ_socio$F_NoImmi_exec, categ_socio$F_NoImmi_emp, categ_socio$F_NoImmi_midman, categ_socio$F_NoImmi_noAct, categ_socio$F_NoImmi_retired, categ_socio$F_NoImmi_worker))

# total number of natives per town 
categ_socio$total_native <- categ_socio$male_native + categ_socio$female_native

# drop unnecessary variables 
categ_socio <- subset(categ_socio, select = c(CODGEO, male_immig, female_immig, total_immig, male_working_immig, 
                                               female_working_immig, male_native, female_native, total_native)) 

```


## Work status data

Rename the original variables
```{r pre-processing work status data}

# rename variables
names(status_work)[3:ncol(status_work)] <-
  c("Less20_M_wagearner_Full", "Less20_M_wagearner_Half", "Less20_M_Indp1_Full", "Less20_M_Indp1_Half",
    "Less20_M_empl_Full", "Less20_M_empl_Half", "Less20_M_trans_Full", "Less20_M_trans_Half",
    "Less20_F_wagearner_Full", "Less20_F_wagearner_Half", "Less20_F_Indp1_Full", "Less20_F_Indp1_Half",
    "Less20_F_empl_Full", "Less20_F_empl_Half", "Less20_F_trans_Full", "Less20_F_trans_Half",
    "t4_M_wagearner_Full", "t4_M_wagearner_Half", "t4_M_Indp1_Full", "t4_M_Indp1_Half", "t4_M_empl_Full",
    "t4_M_empl_Half", "t4_M_trans_Full", "t4_M_trans_Half", "t4_F_wagearner_Full", "t4_F_wagearner_Half",
    "t4_F_Indp1_Full", "t4_F_Indp1_Half", "t4_F_empl_Full", "t4_F_empl_Half", "t4_F_trans_Full", "t4_F_trans_Half",
    "t9_M_wagearner_Full", "t9_M_wagearner_Half", "t9_M_Indp1_Full", "t9_M_Indp1_Half", "t9_M_empl_Full",
    "t9_M_empl_Half", "t9_M_trans_Full", "t9_M_trans_Half", "t9_F_wagearner_Full", "t9_F_wagearner_Half",
    "t9_F_Indp1_Full", "t9_F_Indp1_Half", "t9_F_empl_Full", "t9_F_empl_Half", "t9_F_trans_Full", "t9_F_trans_Half",
    "T4_M_wagearner_Full", "T4_M_wagearner_Half", "T4_M_Indp1_Full", "T4_M_Indp1_Half", "T4_M_empl_Full",
    "T4_M_empl_Half", "T4_M_trans_Full", "T4_M_trans_Half", "T4_F_wagearner_Full", "T4_F_wagearner_Half",
    "T4_F_Indp1_Full", "T4_F_Indp1_Half", "T4_F_empl_Full", "T4_F_empl_Half", "T4_F_trans_Full", "T4_F_trans_Half",
    "T9_M_wagearner_Full", "T9_M_wagearner_Half", "T9_M_Indp1_Full", "T9_M_Indp1_Half", "T9_M_empl_Full",
    "T9_M_empl_Half", "T9_M_trans_Full", "T9_M_trans_Half", "T9_F_wagearner_Full", "T9_F_wagearner_Half",
    "T9_F_Indp1_Full", "T9_F_Indp1_Half", "T9_F_empl_Full", "T9_F_empl_Half", "T9_F_trans_Full", "T9_F_trans_Half",
    "FF_M_wagearner_Full", "FF_M_wagearner_Half", "FF_M_Indp1_Full", "FF_M_Indp1_Half", "FF_M_empl_Full",
    "FF_M_empl_Half", "FF_M_trans_Full", "FF_M_trans_Half", "FF_F_wagearner_Full", "FF_F_wagearner_Half",
    "FF_F_Indp1_Full", "FF_F_Indp1_Half", "FF_F_empl_Full", "FF_F_empl_Half", "FF_F_trans_Full", "FF_F_trans_Half",
    "F9_M_wagearner_Full", "F9_M_wagearner_Half", "F9_M_Indp1_Full", "F9_M_Indp1_Half", "F9_M_empl_Full",
    "F9_M_empl_Half", "F9_M_trans_Full", "F9_M_trans_Half", "F9_F_wagearner_Full", "F9_F_wagearner_Half",
    "F9_F_Indp1_Full", "F9_F_Indp1_Half", "F9_F_empl_Full", "F9_F_empl_Half", "F9_F_trans_Full", "F9_F_trans_Half",
    "f4_M_wagearner_Full", "f4_M_wagearner_Half", "f4_M_Indp1_Full", "f4_M_Indp1_Half", "f4_M_empl_Full",
    "f4_M_empl_Half", "f4_M_trans_Full", "f4_M_trans_Half", "f4_F_wagearner_Full", "f4_F_wagearner_Half",
    "f4_F_Indp1_Full", "f4_F_Indp1_Half", "f4_F_empl_Full", "f4_F_empl_Half", "f4_F_trans_Full", "f4_F_trans_Half",
    "f9_M_wagearner_Full", "f9_M_wagearner_Half", "f9_M_Indp1_Full", "f9_M_Indp1_Half", "f9_M_empl_Full",
    "f9_M_empl_Half", "f9_M_trans_Full", "f9_M_trans_Half", "f9_F_wagearner_Full", "f9_F_wagearner_Half",
    "f9_F_Indp1_Full", "f9_F_Indp1_Half", "f9_F_empl_Full", "f9_F_empl_Half", "f9_F_trans_Full", "f9_F_trans_Half",
    "s4_M_wagearner_Full", "s4_M_wagearner_Half", "s4_M_Indp1_Full", "s4_M_Indp1_Half", "s4_M_empl_Full",
    "s4_M_empl_Half", "s4_M_trans_Full", "s4_M_trans_Half", "s4_F_wagearner_Full", "s4_F_wagearner_Half",
    "s4_F_Indp1_Full", "s4_F_Indp1_Half", "s4_F_empl_Full", "s4_F_empl_Half", "s4_F_trans_Full", "s4_F_trans_Half",
    "s9_M_wagearner_Full", "s9_M_wagearner_Half", "s9_M_Indp1_Full", "s9_M_Indp1_Half", "s9_M_empl_Full",
    "s9_M_empl_Half", "s9_M_trans_Full", "s9_M_trans_Half", "s9_F_wagearner_Full", "s9_F_wagearner_Half",
    "s9_F_Indp1_Full", "s9_F_Indp1_Half", "s9_F_empl_Full", "s9_F_empl_Half", "s9_F_trans_Full", "s9_F_trans_Half")

```

Create aggregate statistics and drop unnecessary variables
```{r Status drop and aggregation}

# Simplify the variables by summing up by categories
# total number of wage earners for males and females.
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('wagearner', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$wagearner_M=rowSums(status_work[,nam.M])
status_work$wagearner_F=rowSums(status_work[,nam.F]) 
                                               
# same but for independent workers by gender
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Indp1', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$independent_M=rowSums(status_work[,nam.M]) 
status_work$independent_F=rowSums(status_work[,nam.F])

# same but for government transfer receivers
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('trans', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$transfer_M=rowSums(status_work[,nam.M])  
status_work$transfer_F=rowSums(status_work[,nam.F])

# same but for employers
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('empl', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$employer_M=rowSums(status_work[,nam.M])
status_work$employer_F=rowSums(status_work[,nam.F])
 
# same but for full-time contracts
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Full', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$full_M=rowSums(status_work[,nam.M])
status_work$full_F=rowSums(status_work[,nam.F])

# same but for half-time contracts
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Half', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
#names(status_work[nam.M])
#names(status_work[nam.F])
status_work$half_M=rowSums(status_work[,nam.M])
status_work$half_F=rowSums(status_work[,nam.F])

# total of each variables
status_work$full <- status_work$full_M + status_work$full_F
status_work$half <- status_work$half_M + status_work$half_F
status_work$wagearner <- status_work$wagearner_M + status_work$wagearner_F
status_work$independent <- status_work$independent_M + status_work$independent_F
status_work$transfer <- status_work$transfer_F + status_work$transfer_M 
status_work$employer <- status_work$employer_F + status_work$employer_M 

# drop unnecessary variables
status_work <- subset(status_work, select = c(CODGEO,
                                              wagearner_M, wagearner_F, 
                                              independent_M, independent_F, 
                                              transfer_M, transfer_F, 
                                              employer_M, employer_F, 
                                              full_M, full_F,
                                              half_M, half_F,
                                              full, half,
                                              wagearner, independent, transfer, employer))
# Full variable means individuals with full time contracts. Half means individuals with part time contract. 
# "transfer" is individuals receiving transfer payments. 
# "independent" is like auto-entrepreneurs. 

```


## Inequality data

Drop unnecessary variables and rename the other ones
```{r pre-processing inequality data}

names(ineq)

# drop unnecessary variables
ineq <- subset(ineq, select = -c(LIBGEO, P09_POP, NAISD16, DECESD16, P14_RP_PROP, P09_EMPLT,ETTOT15, ETAZ15, ETBE15, ETFZ15, ETGU15, ETGZ15, ETOQ15, ETTEF115, ETTEFP1015))

# rename variables 
names(ineq)[4:ncol(ineq)] <-
  c("pop_2014",
    "Superficie",
    "birth09_14",
    "death09_14",
    "households14",
    "housing14",
    "princ_resid14",
    "sec_resid14",
    "vac_resid14",
    "tax_house14",
    "shared_tax_house14",
    "median_living14",
    "lev_ineq14",
    "empl",
    "emp_sal",
    "pop15_64",
    "unemp15_64",
    "act15_64")

# drop more unnecessary variables 
ineq <- subset(ineq, select = -c(princ_resid14, sec_resid14, vac_resid14, pop_2014, households14, birth09_14, 
                                death09_14, tax_house14, shared_tax_house14)) 
 
# unemployment rate 
ineq$unemp_rate <- ineq$unemp15_64/ineq$pop15_64 

```


## Commune data

Drop unnecessary variables and rename the other ones
```{r}
names(commune)

#Drop unnecessary variables
commune <-subset(commune, select=-c(libgeo, scorefiscal, dep, reg, evolutionpop, nbpropritaire, 
                                    nblogementsecondaireetoccasionne, scoreurbanit, scorevargion, scorepib,  
                                    nbrsidencesprincipales, indicefiscalpartiel, nboccupantsrsidenceprincipale, 
                                    depmoyennesalaireshoraires, depmoyennesalairescadrehoraires, 
                                    depmoyennesalairesprofintermdiai, depmoyennesalairesemployhoraires, 
                                    depmoyennesalairesouvrihoraires, valeurajoutergionale, 
                                    nbindustriesdesbiensintermdiaire, nbdecommerce, nbdeservicesauxparticuliers, 
                                    nbinstitutiondeeducationsantacti, scorevargion, scorepib, 
                                    moyennerevenusfiscauxdpartementa, moyennerevenusfiscauxrgionaux, 
                                    regmoyennesalaireshoraires, regmoyennesalairescadrehoraires, 
                                    regmoyennesalairesprofintermdiai, regmoyennesalairesemployhoraires, 
                                    regmoyennesalairesouvrihoraires, tauxtudiants, dynamiquedmographiqueinsee, 
                                    capacitfisc, capacitfiscale, moyennerevnusfiscaux, nblogement, fidlit)) 


# rename variables  
names(commune) <-c("CODGEO", "orient_econ", "demo_index", "population", "nb_household", 
                    "nb_female", "nb_male", "nb_minor", "nb_major", "nb_students",  
                    "nb_firms_service", "nb_firms_commerce", "nb_firms_construction", 
                    "nb_firms_ind", "nb_created_firms", "nb_created_ind", 
                    "nb_created_construction", "nb_created_commerce",
                    "nb_created_services", "urbanRural", "nb_active", "nb_active_employees",
                    "nb_active_non_employees", "dymanic_demo_prof", "profitRate", 
                    "regional_GDP", "demo_environment", "evol_pop_score", 
                    "evol_entrepreneurial_score", "demo_envir2") 

# drop more variables 
commune <-subset(commune, select=-c(demo_index, population, profitRate, demo_envir2, demo_environment)) 

```


# Produce consistent datasets

The CODGEO variables have to be merged.
However, for different reasons already identified by other kaggle users they need some pre-processing.
To do so, some already known mistakes are corrected:

```{r fix and match CODGEO}

firms$CODGEO       <- sub("A", "0", firms$CODGEO)
firms$CODGEO       <- sub("B", "0", firms$CODGEO)
salary$CODGEO      <- sub("A", "0", salary$CODGEO)
salary$CODGEO      <- sub("B", "0", salary$CODGEO)
population$CODGEO  <- sub("A", "0", population$CODGEO)
population$CODGEO  <- sub("B", "0", population$CODGEO)
geo$CODGEO         <- sub("A", "0", geo$CODGEO)
geo$CODGEO         <- sub("B", "0", geo$CODGEO)
educ$CODGEO        <- sub("A", "0", educ$CODGEO)
educ$CODGEO        <- sub("B", "0", educ$CODGEO)
categ_socio$CODGEO <- sub("A", "0", categ_socio$CODGEO)
categ_socio$CODGEO <- sub("B", "0", categ_socio$CODGEO)
ineq$CODGEO        <- sub("A", "0", ineq$CODGEO)
ineq$CODGEO        <- sub("B", "0", ineq$CODGEO)
status_work$CODGEO <- sub("A", "0", status_work$CODGEO)
status_work$CODGEO <- sub("B", "0", status_work$CODGEO)
commune$CODGEO     <- sub("A", "0", commune$CODGEO)
commune$CODGEO     <- sub("B", "0", commune$CODGEO)

# Then all CODGEO are trasformed to integers and four new datasets are created retaining only the common CODGEO:
# use only integer values
geo$CODGEO         <- as.integer(geo$CODGEO)  
population$CODGEO  <- as.integer(population$CODGEO)
firms$CODGEO       <- as.integer(firms$CODGEO)
salary$CODGEO      <- as.integer(salary$CODGEO)
status_work$CODGEO <- as.integer(status_work$CODGEO)  
ineq$CODGEO        <- as.integer(ineq$CODGEO)
educ$CODGEO        <- as.integer(educ$CODGEO)
categ_socio$CODGEO <- as.integer(categ_socio$CODGEO)
commune$CODGEO     <- as.integer(commune$CODGEO)

# store datasets' names to loop on them
dataset = c("population", "salary", "firms", "geo", "ineq", "commune", "educ", "categ_socio", "status_work")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and create a new variable concateneting "NEW" at the end
  nam <- paste(i, "NEW", sep = "")
  # initialize counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # for each dataset different from the i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(i))
  del_obs = paste("For", i, del_rows, "units have been deleted.",
                  "They were the", round(del_prop*100, digits=2), "% of the total.", sep = " ")
  print(paste(del_obs))
}

print(paste("The new dataset has", nrow(salaryNEW), "units and", 
      ncol(salaryNEW)+ncol(populationNEW)+ncol(firmsNEW)+ncol(geoNEW)+
        ncol(status_workNEW)+ncol(ineqNEW)+ncol(educNEW)+ncol(categ_socioNEW)+ncol(communeNEW), "features."))

```

Overwrite new datesets into the old ones and remove the latter ones from memory
```{r take unique datasets}

firms       <- firmsNEW
geo         <- geoNEW
salary      <- salaryNEW
population  <- populationNEW
educ        <- educNEW
categ_socio <- categ_socioNEW
status_work <- status_workNEW
ineq        <- ineqNEW
commune     <- communeNEW

rm(list=c("firmsNEW", "geoNEW", "salaryNEW", "populationNEW", "educNEW", "categ_socioNEW", "status_workNEW", "ineqNEW", "communeNEW"))

```


# Descriptive statistics 

## Firms data

Check the distribution of the null firms (i.e., unknown sizes) and analyze
firms' distribution per town:
```{r firms distribution}

# check the ratio of null firms for each town
summary(firms$null/firms$total)
# a lot of information is missing, should we remove these data?
ggplot(data=firms, aes(firms$null/firms$total)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Null firms/total firms", y="Count") +
  ggtitle("Ratio between the number of null firms and total firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# distribution for the firms with unknown size 
ggplot(data=firms, aes(log(firms$null))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of unknown size (log scale)", y="Density") +
  ggtitle("Number of firms of unknown size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the total number of firms 
ggplot(data=firms, aes(log(firms$total))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms (log scale)", y="Density") +
  ggtitle("Total number of firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# distribution for the number of micro size firms 
ggplot(data=firms, aes(log(firms$micro))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of micro size (log scale)", y="Density") +
  ggtitle("Number of firms of micro size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of small size firms 
ggplot(data=firms, aes(log(firms$small))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of small size (log scale)", y="Density") +
  ggtitle("Number of firms of small size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of medium size firms 
ggplot(data=firms, aes(log(firms$medium))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of medium size (log scale)", y="Density") +
  ggtitle("Number of firms of medium size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of large size firms 
ggplot(data=firms, aes(log(firms$large))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of large size (log scale)", y="Density") +
  ggtitle("Number of firms of large size per town") +
  theme(plot.title = element_text(hjust = 0.5))

```

Check correlation among the variables:
```{r firms correlation}
corrplot(cor(firms[,3:8]))
```


### What have learned

Solved problems: 

* Use the EU firms' categorization.

Some problematic aspects:

* Some towns have 100% of null firms, should we remove these data?
* These variables are strongly correlated, should we use just the total amount in the following parts?

We could use these data for the following tasks:

* predict the salaries using such information as proxy for the competition in the job market;
* predict the total number of firms, using salary data;
* geo-spatial plot for firms' size 



## Geographic data

Plot available towns on a map:
```{r geo map}

# center of France, obtained using:
# fra_center = as.numeric(geocode("France"))
fra_center = c(2.213749, 46.227638)
# plot all European towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.2, size=0.01) 

# Try to cll google API until there is no error
# 
# FraMap = NA
# while (any(is.na(FraMap))){
#   FraMap = tryCatch({
#       ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal")
#   }, error = function(e) {
#     message("cannot open URL")  
#     FraMap = NA
#   })
# }

```

### What we have learned 

Solved problems: 

* Missing latitude and longitude;
* Duplications due to multiple postal codes in the same town;
* Exclude non-European towns;

These information are useful to plot any future dataset/analysis.
In addition, they could be useful to compare European towns vs. old colonies.


## Salary data 

Univariate analysis comparing salaries for both genders among various job categories:

```{r boxplots works}

#  number of units
n_sex <- length(salary$sal_Females)
# vector representing males and females
Label <- c(rep("M", n_sex*5), rep("F", n_sex*5))
# vector representing the variable considered
Variable <- c(rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex),
             rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex))
# merge these data
sal_sex = cbind.data.frame(Label = Label, 
             value = c(salary$sal_Males, salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
                       salary$sal_Females, salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker),
             Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  # not color points replacing colour = group instead of colour=Label
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different job positions") +
  theme(plot.title = element_text(hjust = 0.5)) +      stat_boxplot(geom = "errorbar", width = 0.5)
  # + guides(fill=guide_legend(title="Legend"))

# the same but excluding outliers
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  scale_y_continuous(limits = quantile(sal_sex$value, c(0, 0.9))) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + 
  ggtitle("Gender comparison for different job positions excluding the last decile") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


Univariate analysis comparing salaries for both genders among various ages:

```{r boxplots ages}

# vector representing males and females
Label <- c(rep("M", n_sex*3), rep("F", n_sex*3))
# vector representing the variable considered
Variable <- c(rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex),
              rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex))
# merge these data
sal_sex <- cbind.data.frame(Label = Label, 
                           value = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus, 
                                     salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus),
                           Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different ages") +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(c(5, 100)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


The income inequality between genders, age groups and working positions is clear.
In the following analyses the focus is on the salary ratio between women and men among different
job positions:
```{r ratio F vs M accross jobs}

# Gender salary ratio and general level of income

# Overall mean salary: The higher the net mean income, the more skewed the ratio of salary between female and male is. Only 2 towns have a ratio>1
# create overall F vs M ratio
salary$salary_ratio_FvsM <- salary$sal_Females / salary$sal_Males
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Overall salary ratio (females/males)", y="Density") + 
  labs(title = "Overall salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs overall mean salary
ggplot(salary, aes(x= sal_general, y=salary_ratio_FvsM)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Overall salary ratio(females/males)") + 
  labs(title = "Overall salary ratio between females and males vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm") 


# Executives mean salary: a bit better the situation for females in this case and less skewed
# create Executives F vs M ratio
salary$salary_ratio_FvsM_Exec <- salary$sal_F_executive / salary$sal_M_executive
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Executives salary ratio (females/males)", y="Density") + 
  labs(title = "Executives salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_Exec)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Executives salary ratio (females/males)") + 
  labs(title = "Executives salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Middle managers mean salary: ....
# create Middle managers F vs M ratio
salary$salary_ratio_FvsM_midManag <- salary$sal_F_midManager / salary$sal_M_midManager
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Middle managers salary ratio (females/males)", y="Density") + 
  labs(title = "Middle managers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_midManag)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Middle managers salary ratio (females/males)") + 
  labs(title = "Middle managers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Workers mean salary: ...
# create workers F vs M ratio
salary$salary_ratio_FvsM_worker <- salary$sal_F_worker / salary$sal_M_worker
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_worker)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Workers salary ratio (females/males)", y="Density") + 
  labs(title = "Workers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs workers mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_worker)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Workers salary ratio (females/males)") + 
  labs(title = "Workers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Employee mean salary: ...
# create Employee F vs M ratio
salary$salary_ratio_FvsM_employee <- salary$sal_F_employee / salary$sal_M_employee
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_employee)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Employee salary ratio (females/males)", y="Density") + 
  labs(title = "Employee salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs Employee mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_employee)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Employee salary ratio (females/males)") + 
  labs(title = "Employee salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Now the focus is on the salary ratio between women and men among different age groups:
```{r ratio F vs M accross ages}

# 18-25 mean salary: are quite equal apart from some outliers and a quadratic trend
# create 18-25 F vs M ratio
salary$salary_ratio_FvsM_18_25 <- salary$sal_F_18_25 / salary$sal_M_18_25
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_18_25)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="18-25 salary ratio (females/males)", y="Density") + 
  labs(title = "18-25 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 18-25 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")
# scatter plot vs 18-25 mean salary for them
ggplot(salary, aes(x= sal_18_25, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="18-25 salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall 18-25 salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "loess")


# 26-50 mean salary: ...
# create 26-50 F vs M ratio
salary$salary_ratio_FvsM_26_50 <- salary$sal_F_26_50 / salary$sal_M_26_50
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_26_50)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="26-50 salary ratio (females/males)", y="Density") + 
  labs(title = "26-50 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_26_50)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="26-50 salary ratio (females/males)") + 
  labs(title = "26-50 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# 51+ mean salary: ...
# create 51+ F vs M ratio
salary$salary_ratio_FvsM_51plus <- salary$sal_F_51plus / salary$sal_M_51plus
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_51plus)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="51+ salary ratio (females/males)", y="Density") + 
  labs(title = "51+ salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_51plus)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="51+ salary ratio (females/males)") + 
  labs(title = "51+ salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Highlight bivariate relations:
```{r bivariate plots}

# correlation matrix
corrplot(cor(salary[, 3:ncol(salary)]), method = "circle", title = "Correlation matrix for salary data", 
         diag = T, tl.cex=0.5, type="lower", 
         tl.col = "black", mar=c(0,0,1.5,0)) 

# most general pairs
pairs(salary[c(3:8, 13, 18:20)], gap=0, main = "Scatter matrix of the main variables in salary data", cex = 0.6)
# pairs highlighting genders' differences
pairs(salary[c(9:12, 14:17)], gap=0, main = "Scatter matrix of job categories for both genders", cex = 0.6)

```

### What we have learned

....



## Population data 

Plot population data across different towns:
```{r plotting some population data in log10}

# Histogram of total population per town in log
ggplot(data=population, aes(ifelse(total_population!=0, log10(total_population), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 total population", y="Density") +
  ggtitle("Histogram of total population per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of dependency ratio per town
ggplot(data=population, aes(ifelse(dependency_ratio!=0, log10(dependency_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 dependency ratio", y="Density") +
  ggtitle("Histogram of dependency ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of sex ratio per town
ggplot(data=population, aes(ifelse(sex_ratio!=0, log10(sex_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 sex ratio", y="Density") +
  ggtitle("Histogram of sex ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

```

Understand which towns have the highest concentration of population and ratios.
```{r geo plot for each town}

# Merge geography and population data
# geo_population <- merge(geo, population, by="CODGEO")
geo_population <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(population$CODGEO),],
                                   population[unique(population$CODGEO) %in% unique(geo$CODGEO),])

# France map
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")

# Plot "Distribution of total population for each town" 
sc <- scale_colour_gradientn(colours =palette(rainbow(3)), limits=c(log10(min(geo_population$total_population)), 
                                                                    log10(max(geo_population$total_population)))) 
population_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude,
                 colour=log10(geo_population$total_population)), 
             data=geo_population, alpha=0.2, size=0.01) +
             sc + labs(color='') + ggtitle("Distribution of Population for each town")
population_distribution

# Plot "Distribution of aged dependency ratio for each town"
sc <- scale_colour_gradientn(colours =palette(rainbow(4)), limits=c(min(geo_population$aged_dependency_ratio),
                                                                  max(geo_population$aged_dependency_ratio)))
aged_ratio_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude,
                 colour=geo_population$aged_dependency_ratio),
             data=geo_population, alpha=1, size=1) +
             sc + labs(color='') + ggtitle("Aged dependency ratio per town")
aged_ratio_distribution

# which city has the highest/lowest aged dependency ratio 
geo_population[which.min(geo_population$aged_dependency_ratio),c('CODGEO')]  
geo_population[which.max(geo_population$aged_dependency_ratio),c('CODGEO')]  
 
# get list of top 10 highest aged dependency ratio 
Rank = sort(population$aged_dependency_ratio, TRUE) 
Rank10 = sort(Rank[1:10], TRUE) 
sol = rep(0, 10) 
for (i in 1:10){ 
    # maybe this one was right (LUCA)
    # sol[i] = paste(geo_population$CODGEO[geo_population$aged_dependency_ratio %in% Rank10[i]] ) 
    sol[i] = paste(geo_population$region[population$aged_dependency_ratio %in% Rank10[i]] )
} 

# remove the dataset created to save memory
rm(list = c("geo_population", "Rank", "Rank10")) 

```


### What we have learned 

We can observe that in urban cities, the aged dependcy ratio is lower compared to non-urban cities. 
The city with the lowest aged dependency ratio is La Fert-sous-Jouarre and the highest is L'Aiguillon-sur-Mer. 

## Education data

```{r Descriptive statistics-Education} 
 
# Difference between male and female's education level (with University degrees) 
summary(educ$male_Univ) 
summary(educ$female_Univ) 
summary(educ$male_NoDip) 
summary(educ$female_NoDip) 
 
# Merge geo and educ data to include longitude and latitude in the data. 
geo_educ <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(educ$CODGEO),], 
                                   educ[unique(educ$CODGEO) %in% unique(geo$CODGEO),]) 
 
# check the ratio of individuals with high and low education level for each town. 
summary(educ$univ/educ$nodip) 
 
# Histogram of the ratio of individuals with high and low education level for each town 
ggplot(data=educ, aes(log(educ$univ/educ$nodip))) + 
  geom_histogram(aes(y=..density..), col="black", fill="blue", alpha=0.3, bins= 50)+ 
                   geom_density(col="black") + 
                   labs(x="log. University degree/No diploma", y="Density") + 
                   ggtitle("Ratio between the number of individuals with no diploma and  
                           individuals with University degree per town") + 
                   theme(plot.title =element_text(hjust= 0.5)) 
 
# Adjust scales before plotting 
sc <- scale_colour_gradientn(colours = c("#56ddc5", "#F8766D", "#00BA38"), limits=c(min(educ$univ/educ$nodip), max(educ$univ/educ$nodip))) 
 
# Map ratio  
educ_ratio_NoDipUniv <- 
  FraMap + 
  geom_point(aes(x=geo_educ$longitude, y=geo_educ$latitude, 
                 colour=educ$univ/educ$nodip), 
             data=geo_educ, alpha=1, size=1) + 
             sc + labs(color='Ratio') + ggtitle("Education level ratio for each town") 
educ_ratio_NoDipUniv 
 
# Statistics of shape for the data 
# library(psych) 
describe(educ$univ/educ$nodip) 
 
# remove the dataset created to save memory 
rm(list=c("geo_educ", "educ_ratio_NoDipUniv")) 
 
``` 
 
### What we have learned 
 
In average 960 males have university degree and 1100 females have university degree.  
In average 904 males have no diploma and 1242 females have no diploma. 
Highest ratio in Paris=78251=Yvelines 
 

## Demographic/social profiles data

```{r Descriptive statistics Demographic profiles}
glimpse(categ_socio)

# merge geo and categ_socio
geo_categ_socio <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(categ_socio$CODGEO),],
                                   categ_socio[unique(categ_socio$CODGEO) %in% unique(geo$CODGEO),])

# find which cities have the most and least number of immigrants
categ_socio[which.max(categ_socio$total_immig),c('CODGEO')] 
categ_socio[which.min(categ_socio$total_immig),c('CODGEO')] 

# get list of top 10 cities with the most number of immigrants
Rank = sort(categ_socio$total_immig, TRUE)
Rank10 = sort(Rank[1:10], TRUE)
sol = rep(0, 10)
for (i in 1:10){
    sol[i] = paste(geo_categ_socio$region[categ_socio$total_immig %in% Rank10[i]] )
}

rm(geo_categ_socio)
```

### What we have learned

City with the least number of immigrants is Rennes and the most is Paris.
="le-de-France"              "Provence-Alpes-Cte d'Azur" "Rhne-Alpes"                "Midi-Pyrnes"             
"Provence-Alpes-Cte d'Azur" "Alsace"                     "Languedoc-Roussillon"       "le-de-France"             
"le-de-France"              "le-de-France"             


## Work status data

```{r descriptive stats status_work}

summary(status_work)
names(status_work) 

# merge geo and status_work
# geo_status_work <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(status_work$CODGEO),],
#                                    status_work[unique(status_work$CODGEO) %in% unique(geo$CODGEO),])
geo_status_work <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(status_work$CODGEO),], 
                                    status_work[unique(status_work$CODGEO) %in% unique(geo$CODGEO),]) 


# which town has the most/least people receiving government transfers
status_work[which.max(status_work$transfer),c('CODGEO')] 
status_work[which.min(status_work$transfer),c('CODGEO')] 

# get list of top 10 cities with the people receiving government transfers
Rank = sort(status_work$transfer, TRUE)
Rank10 = sort(Rank[1:10], TRUE)
sol = rep(0, 10)
for (i in 1:10){
    sol[i] = paste(geo_status_work$CODGEO[status_work$transfer %in% Rank10[i]])
}

# which town has the most/least people with part time contract.
status_work[which.max(status_work$half),c('CODGEO')] 
status_work[which.min(status_work$half),c('CODGEO')] 

# get list of top 10 cities with the most part-time contract.
Rank = sort(status_work$half, TRUE)
Rank10 = sort(Rank[1:10], TRUE)
sol = rep(0, 10)
for (i in 1:10){
    sol[i] = paste(status_work$CODGEO[status_work$half %in% Rank10[i]] ) 
}


# Histogram (Females with full time contract)
describe(status_work$full_F)    # Skewness

ggplot(data=status_work, aes(ifelse(full_F!=0, log(full_F), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="", y="Density") +
  ggtitle("Female workers with full time contract") + 
  theme(plot.title = element_text(hjust = 0.5))

# Histogram (males with full time contract) 
describe(status_work$full_M)    # Skewness 

ggplot(data=status_work, aes(ifelse(full_M!=0, log(full_M), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="", y="Density") +
  ggtitle("Male workers with full time contract") + 
  theme(plot.title = element_text(hjust = 0.5))

# Histogram: Contract type ratio
describe(status_work$half/status_work$full)    # Skewness

ggplot(data=status_work, aes(status_work$half/status_work$full)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Contract type ratio (temporary/full time)", y="Density") +
  ggtitle("Contract type ratio") +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r Box plot}
#  number of units
n_sex <- length(status_work$wagearner_F)

# vector representing males and females
Label <- c(rep("M", n_sex*6), rep("F", n_sex*6))

# vector representing the variable considered
Variable <- c(rep("Wagearner", n_sex), 
             rep("Independent", n_sex),
             rep("Transfer", n_sex),
             rep("Employer", n_sex),
             rep("Full",n_sex),
             rep("Half", n_sex))
             
# merge these data
status_sex = cbind.data.frame(Label = Label, 
             value = c(status_work$wagearner_M, status_work$wagearner_F, status_work$independent_M,
                       status_work$independent_F, status_work$transfer_M, status_work$transfer_F,
                       status_work$employer_M, status_work$employer_F, status_work$full_M, status_work$full_F,
                       status_work$half_M, status_work$half_F),
             Variable = Variable)

# plotting phase
ggplot(data = status_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("# of individuals") + ggtitle("Gender comparison") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)


# the same but excluding outliers
ggplot(data = status_sex, aes(x=Label, y=value)) +
  scale_y_continuous(limits = quantile(status_sex$value, c(0, 0.9))) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("# of individuals") + 
  ggtitle("Gender comparison excluding the last decile") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)
```

### What we have learned

Paris is the city where they have the most individuals receiving government transfers.
Lyon is the city where they have the least individuals receiving government transfers.
="le-de-France"              "Provence-Alpes-Cte d'Azur" "Provence-Alpes-Cte d'Azur" "Rhne-Alpes"               
"Midi-Pyrnes"              "Aquitaine"                  "Languedoc-Roussillon"       "Pays de la Loire"          
"Rhne-Alpes"                "Alsace"                    

Paris has most individuals with half time contract. 
Rouen has the least. 
= "le-de-France"              "Rhne-Alpes"                "Provence-Alpes-Cte d'Azur" "Midi-Pyrnes"             
"Pays de la Loire"           "Nord-Pas-de-Calais"         "Alsace"                     "Languedoc-Roussillon"      
"Aquitaine"                  "Bretagne"    

In average, more females receive transfers than males. 
In average, more females have part-time contract than males. (In average, 581 females have part-time contract (in each town) and 178 males have part-time contract. (More males have full-time contract.)

More females work as wage-earners than males.
=All these facts can be related to vulnerability of females in the labour market. Many studies show that females are mostly working as part-time workers and lower job categories. On top of this, more females receive transfers.
We can also observe this in our data that more males are registered as independent workers or employers. 

## Inequality data

```{r descriptive statistics Inequality data}
glimpse(ineq)

# merge geo and ineq
geo_ineq <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(ineq$CODGEO),],
                                   ineq[unique(ineq$CODGEO) %in% unique(geo$CODGEO),])

# which city has the highest/lowest unemployment rate
ineq[which.max(ineq$unemp_rate),c('CODGEO')] 
ineq[which.min(ineq$unemp_rate),c('CODGEO')] 

# get list of top 10 cities with the highest unemployment rate.
Rank = sort(ineq$unemp_rate, TRUE)
Rank10 = sort(Rank[1:10], TRUE)
sol = rep(0, 10)
for (i in 1:10){
    sol[i] = paste(ineq$CODGEO[ineq$unemp_rate %in% Rank10[i]] ) 
}

# which is the smallest/biggest city (area)
ineq[which.max(ineq$Superficie),c('CODGEO')] 
ineq[which.min(ineq$Superficie),c('CODGEO')] 

# which city has the highest/lowest median living level
ineq[which.max(ineq$median_living14),c('CODGEO')] 
ineq[which.min(ineq$median_living14),c('CODGEO')] 

# Adjust scales before plotting
sc <- scale_colour_gradientn(colours = c("#56ddc5", "#ff3db7"), limits=c(min(ineq$unemp_rate), max(ineq$unemp_rate)))

# Map ratio 
ineq_unempl_distribution <-
  FraMap +
  geom_point(aes(x=geo_ineq$longitude, y=geo_ineq$latitude,
                 colour=geo_ineq$unemp_rate),
             data=geo_ineq, alpha=1, size=1) +
             sc + labs(color='Unemployment rate') + ggtitle("Unemployment rate for each town")
ineq_unempl_distribution

# remove the dataset created to save memory
rm(list=c("geo_ineq", "ineq_unempl_distribution"))

```

### What we have learned

Bordeaux has the highest unemployment rate.
Nantes has the lowest unemployment rate. 
="Aquitaine"            "Languedoc-Roussillon" "Nord-Pas-de-Calais"   "Haute-Normandie"      "Guadeloupe"          
"Corse"                "Nord-Pas-de-Calais"   "Corse"                "Rhne-Alpes"          "Nord-Pas-de-Calais" 

## Commune data

```{r descriptive statistics Commune data}
glimpse(commune)

# change the factor variables to numeric
aaa = as.numeric(commune$urbanRural)

# Merge geo and commune data to include longitude and latitude in the data.
geo_commune <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(commune$CODGEO),],
                                   commune[unique(commune$CODGEO) %in% unique(geo$CODGEO),])


# Adjust scales before plotting
sc <- scale_colour_gradientn(colours = palette(rainbow(3)), limits=c(min(aaa), max(aaa)))

# Map ratio 
 urbanrural<-
  FraMap +
  geom_point(aes(x=geo_commune$longitude, y=geo_commune$latitude,
                 colour=aaa),
             data=geo_commune, alpha=1, size=0.5) +
             sc + labs(color='') + ggtitle("Urban and rural cities")
urbanrural

rm(list = c("geo_commune", "Rank", "Rank10", "aaa"))
```


# Create an unique dataset

Create an unique dataset:
```{r Merge the data and create csv file}

# merging
newDat = merge(firms, population, by="CODGEO")
newDat = merge(newDat, salary, by="CODGEO")
newDat = merge(newDat, geo, by="CODGEO")
newDat = merge(newDat, status_work, by="CODGEO")
newDat = merge(newDat, ineq, by="CODGEO")
newDat = merge(newDat, educ, by="CODGEO")
newDat = merge(newDat, categ_socio, by="CODGEO")
newDat = merge(newDat, commune, by="CODGEO")

# check
names(newDat)
head(newDat)

```

Save/load final dataset (just to avoid running all the code) 
```{r Save/load final dataset, warning=FALSE} 
 
# #remove unused factor levels on newDat 
# for (i in 1:ncol(newDat)){ 
#   if (is.factor(newDat[,i])){ 
#     newDat[,i] = droplevels.factor(newDat[,i]) 
#   } 
# } 
# # save the NewDat.csv file created (which has to be moved in the data folder) 
# write.table(newDat, "newDat.csv", quote = TRUE, row.names=FALSE, col.names = TRUE, 
#           fileEncoding = "UTF-8", sep = ",") 
 
# load the final dataset 
setwd("./data")
newDat <- read.csv("newDat.csv", encoding = "UTF-8", header = TRUE)

# check (changing newDat loaded with newDat2), the only differences are in the order of 10e-15 
# identical(newDat, newDat2) 
 
```

Spatial plot of the data to spot possible patterns. 
A sub-sample will be retained when needed in order to reduce 
geo-spatial correlations. 
```{r newDat spatial plot} 
# center of France, obtained using: 
# fra_center = as.numeric(geocode("France")) 
fra_center = c(2.213749, 46.227638) 
 
ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") + 
  geom_point(aes(x=longitude, y=latitude), data=newDat, col="orange", alpha=0.2, size=0.01)  
 
``` 
 

# Unsupervised Learning

## Multi-Dimensional Scaling (MDS)

### Preliminary multivariate salary data visualisation to better conduct the unsupervised learning  
```{r}
newDat2 = newDat[!ind_nonEur,]
salary2 = salary[!ind_nonEur,]
region <- newDat2$region

# add region variable to the salary dataset
salary_with_dep <- cbind(region, salary2)
#check
#ncol(salary_with_dep)
#head(salary_with_dep)
# extract continuous variables
salary_variables <- salary_with_dep[, 3:34]

#scatterplot to visually see the correlations between variables
pairs(salary_variables[1:8], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#scatterplot between variables of major interest
pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#bivariate densities among general, males and females salaries
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, 
       breaks[-1], y, col="grey", ...)
}


pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")],
      diag.panel = panel.hist,
      panel = function (x,y) {
        data <- data.frame(cbind(x,y))
        par(new = TRUE)
        den <- bkde2D(data, 
                      bandwidth=sapply(data,dpik))
        contour(x = den$x1, y = den$x2, 
                z = den$fhat, axes = FALSE)
      })

#scale the data
salary_variables_scaled <- as.data.frame(scale(salary_variables))
#divide sal_general into 4 parts
salary_levels <- with(salary_variables_scaled, 
                  equal.count(sal_general,4))
#make a 3D plot of salary_ratio_FvsM against sal_26_50 and sal_employee given salary_levels
plot(cloud(salary_ratio_FvsM ~ sal_26_50 * sal_employee | salary_levels, 
           panel.aspect = 0.5,
           data = salary_variables_scaled))

#plot sal_worker against sal_midManager given 3 partitions of sal_general
plot(xyplot(sal_worker ~ sal_midManager| cut(sal_general, 3), 
            data = salary_variables, 
            layout = c(3, 1), 
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))

#multidimesnional plot of midManager salary against worker salary, given 4 partitions of F/M salary ratio, and grayscale intensities of general salary
M_F_salary_ratio  <- with(salary_variables, equal.count(salary_ratio_FvsM, 4))
general_salary.ord <- with(salary_variables, rev(order(sal_general)))
salary_variables.ordered <- salary_variables[general_salary.ord,]
sal_general.breaks <- with(salary_variables.ordered, 
                     do.breaks(range(sal_general),50))
salary_variables.ordered$color<-level.colors(salary_variables.ordered$sal_general,
                                   at=sal_general.breaks,
                                   col.regions=grey.colors)
plot(xyplot(sal_worker ~ sal_midManager | M_F_salary_ratio, 
            data = salary_variables.ordered,
            aspect = "iso", 
            groups = color, 
            cex = 1, col = "black",
       panel = function(x, y, groups, ..., subscripts) {
           fill <- groups[subscripts]
           panel.grid(h = -1, v = -1)
           panel.xyplot(x, y, pch = 21, 
                        fill = fill, ...)
       },
       legend =
       list(right =
            list(fun = draw.colorkey,
                 args=list(key=list(col=gray.colors,
                                    at = sal_general.breaks),
                           draw = FALSE))),
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))


#plot(cloud(sal_general ~ sal_worker * salary_ratio_FvsM | sal_26_50, 
#           data = salary_variables,
#           zlim = rev(range(salary_variables$sal_general)),
#           screen = list(z = 5, x = -3), 
#           panel.aspect = 0.9,
#           xlab = "Worker salary", 
#           ylab = "M_F_salary_ratio", 
#           zlab = "General salary"))



```

### Preliminary multivariate firms data visualisation to better conduct the unsupervised learning 
```{r}
# add region variable to the salary dataset
firms2 = firms[!ind_nonEur,]
firms_with_dep <- cbind(region, firms2)
#check
#ncol(salary_with_dep)
#head(salary_with_dep)
# extract continuous variables
firms_variables <- firms_with_dep[, 4:8]

#scatterplot to visually see the correlations between variables
pairs(firms_variables[, 2:5], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)


#bivariate densities among general, males and females salaries
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, 
       breaks[-1], y, col="grey", ...)
}


pairs(firms_variables[, 2:5],
      diag.panel = panel.hist,
      panel = function (x,y) {
        data <- data.frame(cbind(x,y))
        par(new = TRUE)
        den <- bkde2D(data, 
                      bandwidth=sapply(data,dpik))
        contour(x = den$x1, y = den$x2, 
                z = den$fhat, axes = FALSE)
      })

#scale the data
firms_variables_scaled <- as.data.frame(scale(firms_variables))
#divide firms total into 4 parts
firms_levels <- with(firms_variables_scaled, 
                  equal.count(total,4))
#make a 3D plot of large against micro and small given firms_levels
plot(cloud(large ~ micro * small | firms_levels, 
           panel.aspect = 0.5,
           data = firms_variables))

#plot sal_worker against sal_midManager given 3 partitions of sal_general
plot(xyplot(medium ~ large| cut(total, 3), 
            data = firms_variables, 
            layout = c(3, 1), 
            xlab = "medium firms", 
            ylab = "large firms"))

#multidimesnional plot of large firms against micro firms, given 4 partitions of medium, and grayscale intensities of total
#medium_firms  <- with(firms_variables, equal.count(medium, 4))
#total.ord <- with(firms_variables, rev(order(total)))
#firms_variables.ordered <- firms_variables[total.ord,]
#total.breaks <- with(firms_variables.ordered, 
                     #do.breaks(range(total),10))
#firms_variables.ordered$color<-level.colors(firms_variables.ordered$total,
                                  # at=total.breaks,
                                   #col.regions=grey.colors)
#plot(xyplot(large ~ micro | medium, 
            #data = firms_variables.ordered,
            #aspect = "iso", 
            #groups = color, 
            #cex = 1, col = "black",
       #panel = function(x, y, groups, ..., subscripts) {
           #fill <- groups[subscripts]
          #panel.grid(h = -1, v = -1)
           #panel.xyplot(x, y, pch = 11, 
                       #fill = fill, ...)
       #},
       #legend =
       #list(right =
            #list(fun = draw.colorkey,
                 #args=list(key=list(col=gray.colors,
                                    #at = total.breaks),
                           #draw = FALSE))),
            #xlab = "micro firms", 
            #ylab = "large firms"))


#plot(cloud(sal_general ~ sal_worker * salary_ratio_FvsM | sal_26_50, 
#           data = salary_variables,
#           zlim = rev(range(salary_variables$sal_general)),
#           screen = list(z = 5, x = -3), 
#           panel.aspect = 0.9,
#           xlab = "Worker salary", 
#           ylab = "M_F_salary_ratio", 
#           zlab = "General salary"))


```


### Multi-dimensional scaling (MDS) on salary data
```{r}
# produces a list with covariance matrix
# for each region as component:
salary_var <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
                     function(i) var(salary_with_dep[i,3:34]))
str(salary_var)

#remove regions with null values
salary_var <- salary_var[-c(12, 15, 19, 20, 28)]
str(salary_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(salary_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(salary_var[[1]])
# creates common covariance matrix S
s <- 0
for (v in c(1:23)) S <- S + regions_count_sub$Freq[v] * as.matrix(salary_var[[v]]) 
S <- S / 5021

# finds center of each variable (sal_general, etc)
# for each region
salary_cen <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
    function(i) apply(salary_with_dep[i,-c(1:2)], 2, mean))
salary_cen
salary_cen <- salary_cen[-c(12, 15, 19, 20, 28)]
str(salary_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
salary_cen <- matrix(unlist(salary_cen), 
    nrow = length(salary_cen), byrow = TRUE)
salary_cen

# compute the mahalanobis distances:
salary_mah <- apply(salary_cen, 1, 
    function(cen) mahalanobis(salary_cen, cen, S))
salary_mah



# run it on two dimensions
salary_mds <- cmdscale(salary_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(salary_mds) * 1.2
plot(salary_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(salary_mds, labels = levels(salary_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
salary_mds <- cmdscale(salary_mah, k = nrow(salary_mah) - 1, 
         eig = TRUE)

#only 9 of the first 21 eigenvalues are > 0
# x <- salary_mds$points[,1]
# y <- salary_mds$points[,2]
# st <- mst(salary_mah)
# plot(x, y, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = range(x)*1.2, type = "n")
# for (i in 1:nrow(salary_mah)) {
#   w1 <- which(st[i, ] == 1)
#   segments(x[i], y[i], x[w1], y[w1])
# }
# text(x, y, labels = colnames(salary_mah), 
#      cex = 1.5)

```

### Multi-dimensional scaling (MDS) on firms data
```{r}
firms_with_dep <- firms_with_dep[, -9]
# produces a list with covariance matrix
# for each region as component:
firms_var <- tapply(1:nrow(firms_with_dep), firms_with_dep$region, 
                     function(i) var(firms_with_dep[i,4:8]))
#str(firms_var)

#remove regions with null values
firms_var <- firms_var[-c(12, 15, 19, 20, 28)]
#check
str(firms_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(firms_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(firms_var[[1]])

# creates common covariance matrix S
s <- 0
for (v in c(1:5)) S <- S + regions_count_sub$Freq[v] * as.matrix(firms_var[[v]]) 
S <- S / 5021

# finds center of each variable (micro, etc)
# for each region
firms_cen <- tapply(1:nrow(firms_with_dep), firms_with_dep$region, 
    function(i) apply(firms_with_dep[i,-c(1:3)], 2, mean))
firms_cen
firms_cen <- firms_cen[-c(12, 15, 19, 20, 28)]
str(firms_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
firms_cen <- matrix(unlist(firms_cen), 
    nrow = length(firms_cen), byrow = TRUE)
firms_cen

# compute the mahalanobis distances:
firms_mah <- apply(firms_cen, 1, 
    function(cen) mahalanobis(firms_cen, cen, S))
firms_mah



# run it on two dimensions
firms_mds <- cmdscale(firms_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(firms_mds) * 1.2
plot(firms_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(firms_mds, labels = levels(firms_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
firms_mds <- cmdscale(firms_mah, k = nrow(firms_mah) - 1, 
         eig = TRUE)

#only 5 of the first 21 eigenvalues are > 0
# x <- firms_mds$points[,1]
# y <- firms_mds$points[,2]
# st <- mst(firms_mah)
# plot(x, y, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = range(x)*1.2, type = "n")
# for (i in 1:nrow(firms_mah)) {
#   w1 <- which(st[i, ] == 1)
#   segments(x[i], y[i], x[w1], y[w1])
# }
# text(x, y, labels = colnames(firms_mah), 
#      cex = 1.5)
```

### Multi-Dimensional Scaling for merged dataset
```{r}
# NewDat_PCA_with_dep <- cbind(geo[1], newDat_pca_scores)
# # produces a list with covariance matrix
# # for each region as component:
# NewDat_PCA_var <- tapply(1:nrow(NewDat_PCA_with_dep), NewDat_PCA_with_dep$region, 
#                      function(i) var(NewDat_PCA_with_dep[i,2:4]))
# 
# 
# #remove regions with null values
# NewDat_PCA_var <- NewDat_PCA_var[-c(11, 12, 15, 19, 20, 28)]
# #check
# #str(firms_var)
# 
# 
# 
# #create dataframe with the counts of towns per region
# regions_count <- data.frame(table(firms_with_dep$region))
# regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
# #check total sum
# row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
# sum(regions_count_sub$Freq)
# 
# 
# 
# # initializes common covariance matrix var:
# S <- regions_count_sub$Freq[1] * as.matrix(NewDat_PCA_var[[1]])
# 
# # creates common covariance matrix S
# s <- 0
# for (v in c(1:5)) S <- S + regions_count_sub$Freq[v] * as.matrix(NewDat_PCA_var[[v]]) 
# S <- S / 5024
# 
# # finds center of each variable (micro, etc)
# # for each region
# NewDat_cen <- tapply(1:nrow(NewDat_PCA_with_dep), NewDat_PCA_with_dep$region, 
#     function(i) apply(NewDat_PCA_with_dep[i,-1], 2, mean))
# NewDat_cen
# NewDat_cen <- NewDat_cen[-c(11, 12, 15, 19, 20, 28)]
# str(NewDat_cen)
# 
# # create a matrix out of each components, each
# # mean measurement for all variables by region
# NewDat_cen <- matrix(unlist(NewDat_cen), 
#     nrow = length(NewDat_cen), byrow = TRUE)
# NewDat_cen
# 
# # compute the mahalanobis distances:
# NewDat_mah <- apply(NewDat_cen, 1, 
#     function(cen) mahalanobis(NewDat_cen, cen, S))
# NewDat_mah
# 
# 
# 
# # run it on two dimensions
# NewDat_mds <- cmdscale(NewDat_mah)
# 
# # draw a scatterplot of two-dimensional solution
# # from classical MDS applied to Mahalanobis
# # distances:
# lim <- range(NewDat_mds) * 1.2
# plot(NewDat_mds, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = lim, ylim = lim, type = "n")
# text(NewDat_mds, labels = levels(NewDat_PCA_with_dep$region), 
#      cex = 0.5)
# 
# # But here, as the two-dimensional fit may not
# # explain what is needed to represent the observed
# # distances, we shall investigate the solution 
# # in a little more detail using the "minimum
# # spanning tree".
# 
# # The minimum spanning tree is defined as follows. 
# # Suppose n points are given (possibly in many
# # dimensions). Then a tree spanning these points 
# # (that is, a spanning tree) is any set of straight 
# # line segments joining pairs of points such that
# # - 0 closed loops occur,
# # - every point is visited at least one time, and
# # - the tree is connected (has paths between each
# # pair of points)
# 
# # We use function mst() from package ape to plot
# # minimum spanning tree on the two-dimensional
# # scaling solution:
# 
# #
# # run classic MDS function on distances
# NewDat_mds <- cmdscale(NewDat_mah, k = nrow(NewDat_mah) - 1, 
#          eig = TRUE)
# 
# #only 5 of the first 21 eigenvalues are > 0
# # x <- firms_mds$points[,1]
# # y <- firms_mds$points[,2]
# # st <- mst(firms_mah)
# # plot(x, y, xlab = "Coordinate 1", 
# #      ylab = "Coordinate 2",
# #      xlim = range(x)*1.2, type = "n")
# # for (i in 1:nrow(firms_mah)) {
# #   w1 <- which(st[i, ] == 1)
# #   segments(x[i], y[i], x[w1], y[w1])
# # }
# # text(x, y, labels = colnames(firms_mah), 
# #      cex = 1.5)
```

## PCA

### PCA on salary data and delta M-F
```{r}

pca_salary <- prcomp(salary_variables)
autoplot(pca_salary, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary, type = "l")
#pca_salary
#summary(pca_salary)

#create dataset with M-F differences
salary_var_delta <- data.frame("M-F" = salary_variables$sal_Males -                                                           salary_variables$sal_Females,
                     "Executive M-F" = salary_variables$sal_M_executive -
                                         salary_variables$sal_F_executive,
                    "midManager M-F" = salary_variables$sal_M_midManager -
                                         salary_variables$sal_F_midManager,
                      "employee M-F" = salary_variables$sal_M_employee -
                                         salary_variables$sal_F_employee,
                        "worker M-F" = salary_variables$sal_M_worker -
                                         salary_variables$sal_F_worker,
                         "18-25 M-F" = salary_variables$sal_M_18_25 -
                                         salary_variables$sal_F_18_25,
                         "26-50 M-F" = salary_variables$sal_M_26_50 -
                                         salary_variables$sal_F_26_50,
                       "51 plus M-F" = salary_variables$sal_M_51plus -
                                         salary_variables$sal_F_51plus)
pca_salary_delta <- prcomp(salary_var_delta)
autoplot(pca_salary_delta, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary_delta, type = "l")
#pca_salary_delta
#summary(pca_salary_delta)

```

### PCA on firms data
```{r}
firms_variables_normalized <- firms_variables[]/commune$population

pca_firms <- prcomp(firms_variables_normalized)
autoplot(pca_firms, data = firms_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_firms, type = "l")
#pca_firms
#summary(pca_firms)


```


### PCA on merged dataset
```{r}
drops <- c("CODGEO","town", "total", "null", "total_population", "region", "region_capital", "department", "town_name", "postal_code", "latitude", "longitude", "LIBGEO.x", "LIBGEO.y", "REG", "DEP", "pop_2014", "transfer_M", "transfer_F", "M_immi_agri", "F_immi_agri", "male_immig", "tax_house14", "shared_tax_house14", "median_living14", "lev_ineq14")


newDat_variables <- newDat2[ , !(names(newDat2) %in% drops)]
newDat_variables <- select_if(newDat_variables, is.numeric)
newDat_variables <- sapply(newDat_variables, as.numeric)
newDat_variables <- as.data.frame(newDat_variables)



names(which(sapply(newDat_variables, anyNA)))

#str(newDat_variables)
newDat_variables_dep <- cbind(geo[1], newDat_variables)

#pca_newDat <- prcomp(na.omit(newDat_variables), scale=TRUE)
pca_newDat <- prcomp(newDat_variables)
str(pca_newDat)


autoplot(pca_newDat, data = newDat_variables_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_newDat, type = "l")

#pca_newDat

```
### PCA on selected variables
```{r}
nums <- unlist(lapply(commune, is.numeric)) 
commune_selected <- commune[, nums]

commune_selected <- commune_selected[, 3:21]

commune_selected_normalized <- commune_selected[]/commune$population

unemp_percent <- as.data.frame((ineq$unemp15_64/ineq$pop15_64)*100)
names(unemp_percent)[1] <- "unemp_percent"

selected_variables <- cbind(firms_variables_normalized, salary_variables, commune_selected_normalized, unemp_percent)

pca_selected_variables <- prcomp(selected_variables)
autoplot(pca_selected_variables, data = geo, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_selected_variables, type = "l")

```


## Cluster Analysis

### Cluster Analysis on salary
```{r}


#create dataframe with first two principal components scores as variables
salary_pca_scores<- as.data.frame(pca_salary$x[,1:2])
salary_pca_scores <- as.data.frame(scale(salary_pca_scores))
sapply(salary_pca_scores, var)
#scatterplot between PC1 and PC2 to see if there are outliers
# pairs(salary_pca_scores, 
#       panel = function (x, y, ...) {
#           points(x, y, ...)
#           abline(lm(y ~ x), col = "red")
#       }, pch = ".", cex = 0.5)

#after visually identifying thresholds, get rid of outliers
#outliers <- subset(salary_pca_scores, PC1 > 8 | PC2 > 13)
#outliers_names <- unlist(as.numeric(rownames(outliers)))
#salary_pca_scores <- salary_pca_scores[-outliers_names,]
#renumber rows
#row.names(salary_pca_scores) <- 1:nrow(salary_pca_scores)

#sample data
#set.seed(123)
#salary_scores_sampled <- sample_n(salary_pca_scores, 1000)

#renumber rows
#row.names(salary_scores_sampled) <- 1:nrow(salary_scores_sampled)

#create distance matrix for hclust
#dm <- dist(salary_scores_sampled)
#round to two decimals
#dm <- round(dm, 2)

# Below we show the cluster solutions for
# salary data. The top row gives the cluster
# dendograms along with the cutoff used to
# derive the classes presented (in the space
# of the two principal components) in the
# bottom row:


# salary_pc <- princomp(dm, cor = TRUE)
# layout(matrix(1:6, nr = 2), height = c(2, 1))
# plot(cs <- hclust(dm, method = "single"), 
#      main = "Single")
# abline(h = 3.8, col = "lightgrey")
# xlim <- range(salary_pc$scores[,1])
# plot(salary_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(cs, h = 3.8)
# text(salary_pc$scores[,1:2], labels = lab, 
#      cex=0.6)
# plot(cc <- hclust(dm, method = "complete"), 
#      main = "Complete")
# abline(h = 7, col = "lightgrey")
# plot(salary_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(cc, h = 7)  
# text(salary_pc$scores[,1:2], labels = lab, 
#      cex=0.6)     
# plot(ca <- hclust(dm, method = "average"), 
#      main = "Average")
# abline(h = 7.8, col = "lightgrey")
# plot(salary_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(ca, h = 7.8)                             
# text(salary_pc$scores[,1:2], labels = lab, 
#      cex=0.6)  

# restore layout 
layout(matrix(1, nr = 1)) 
 
# K-means clustering
n <- nrow(salary_pca_scores)
wss <- rep(0, 22)
wss[1] <- (n - 1) * sum(sapply(salary_pca_scores, var))
for (i in 2:22)
    wss[i] <- sum(kmeans(salary_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:22, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 5 groups 
plot(salary_pca_scores, 
     pch = kmeans(salary_pca_scores, 
                  centers = 5)$cluster)

#now we have to see whether these groups correspond to geographical regions...

```
###clustering salary delta M-F
```{r}
salary_delta_pca_scores <- as.data.frame(pca_salary_delta$x[,1:3])

n <- nrow(salary_delta_pca_scores)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(salary_delta_pca_scores, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(salary_delta_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")



#plots
plot(salary_delta_pca_scores, 
     pch = kmeans(salary_delta_pca_scores, 
                  centers = 6)$cluster)

plot3d(salary_delta_pca_scores, col=kmeans(salary_delta_pca_scores, 
                  centers = 6)$cluster)

scatter3d(x = salary_delta_pca_scores$PC1, y = salary_delta_pca_scores$PC2, z = salary_delta_pca_scores$PC3, groups = as.factor(kmeans(salary_delta_pca_scores, 
                  centers = 6)$cluster),
          grid = FALSE, surface = FALSE)

clusters <- kmeans(salary_delta_pca_scores, 
                  centers = 6)

FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(clusters$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on unemployment rate")
salary_geo


salary_delta_cluster <- cbind(salary_var_delta, clusters$cluster)
names(salary_delta_cluster)[9] <- "cluster"
sal_delta_by_cluster <- aggregate(salary_delta_cluster$M.F, by=list(salary_delta_cluster$cluster), FUN=mean)[2]
names(sal_delta_by_cluster)[1] <- "Salary_difference"
```

### Finding towns with large difference in male-female salaries
```{r}
#detecting extreme cities in PCA
quantiles<-tapply(salary_delta_pca_scores$PC1,newDat$town,quantile)
minq <- sapply(newDat$town, function(x) quantiles[[x]]["25%"])
maxq <- sapply(newDat$town, function(x) quantiles[[x]]["75%"])


towns <- as.data.frame(newDat$town)
k <- salary_delta_pca_scores[which(salary_delta_pca_scores$PC1<minq | salary_delta_pca_scores$PC1>maxq), ]

#find towns with largedifference 
extreme_town_list <- towns[which(salary_delta_pca_scores$PC1<minq | salary_delta_pca_scores$PC1>maxq), ]

extreme_towns <- as.data.frame(extreme_town_list)


#detecting extreme cities in salary delta
quantiles<-tapply(salary_var_delta$M.F,newDat$town,quantile)
minq <- sapply(newDat$town, function(x) quantiles[[x]]["25%"])
maxq <- sapply(newDat$town, function(x) quantiles[[x]]["75%"])

town <- towns[which(salary_var_delta$M.F<minq | 
                                    salary_var_delta$M.F>maxq), ]

extreme_towns_m_f <- as.data.frame(town)

geo_extreme <- subset(newDat, town %in% extreme_towns_m_f$town)
geo_extreme <- geo_extreme[, c("town", "latitude", "longitude")]

FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo_extreme$longitude, y=geo_extreme$latitude),
             data=geo_extreme, col="red", alpha=0.5, size=0.5) + labs(color='') + ggtitle("Towns with large difference in male-female salaries")
salary_geo

#As you can see, the towns with large salry differences are concetrated around large cities such as Paris and Lyon.

```
###clustering towns based on unemployed %
```{r}
unemp_percent <- as.data.frame((ineq$unemp15_64/ineq$pop15_64)*100)
names(unemp_percent)[1] <- "unemp_percent"

n <- nrow(unemp_percent)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(unemp_percent, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(unemp_percent,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 6
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

clusters <- kmeans(unemp_percent, 6)

unemp_percent <- cbind(unemp_percent, clusters$cluster)
names(unemp_percent)[2] <- "cluster"
unemp_by_cluster <- aggregate(unemp_percent$unemp_percent, by=list(unemp_percent$cluster), FUN=mean)[2]
names(unemp_by_cluster)[1] <- "unemp_rate(%)" 

FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(clusters$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on unemployment rate")
salary_geo



```

### Clustering based on Location Quotient
```{r}
industry <- commune[, c("nb_firms_service", "nb_firms_commerce", "nb_firms_construction")]

lq_service <- (industry$nb_firms_service/(industry$nb_firms_service+industry$nb_firms_commerce+industry$nb_firms_construction))/(sum(industry$nb_firms_service)/sum(industry))
lq_service <- as.data.frame((lq_service))


lq_commerce <- (industry$nb_firms_commerce/(industry$nb_firms_service+industry$nb_firms_commerce+industry$nb_firms_construction))/(sum(industry$nb_firms_commerce)/sum(industry))
lq_commerce <- as.data.frame((lq_commerce))

lq_construction <- (industry$nb_firms_construction/(industry$nb_firms_service+industry$nb_firms_commerce+industry$nb_firms_construction))/(sum(industry$nb_firms_construction)/sum(industry))
lq_construction <- as.data.frame((lq_construction))

industry_lq <- cbind(industry, lq_service, lq_commerce, lq_construction)
#create dataframe with the counts of towns per region
regions_count <- data.frame(table(salary_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)

```


###Salary clusters 
```{r}
#create distance matrix for hclust
dm <- dist(salary_variables)
#round to two decimals
dm <- round(dm, 2)
 
# K-means clustering
n <- nrow(salary_variables)
wss <- rep(0, 22)
wss[1] <- (n - 1) * sum(sapply(salary_variables, var))
for (i in 2:22)
    wss[i] <- sum(kmeans(salary_variables,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:22, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 5 groups 
# plot(salary_pca_scores, 
#      pch = kmeans(salary_pca_scores, 
#                   centers = 5)$cluster)

#now we have to see whether these groups correspond to geographical regions...
```


### Mapping the salary clusters
```{r}
 
#now we have to see whether these groups correspond to geographical regions...
clusters <- kmeans(salary_pca_scores, 5)
salary_with_dep$cluster <- clusters$cluster
geo$cluster <- clusters$cluster

salary_pca_scores$cluster <- clusters$cluster
ggplot(salary_pca_scores, aes(x = PC1, y = PC2, colour = cluster)) +
  geom_point()
 
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(geo$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on salary")
salary_geo
 
 
#salary_with_dep$cluster <- clusters$cluster
#data("world.cities")
#France <- world.cities %>% filter(country.etc == "France")
#EPS <- 0.15
#clusters <- dbscan(France[, 4:5], eps = EPS)
#+France$cluster <- clusters$cluster
 
#groups  <- France %>% filter(cluster != 0)
#noise  <- France %>% filter(cluster == 0)
 
#ggplot(France, aes(x = long, y = lat, alpha = 0.5)) + 
#  geom_point(aes(fill = "grey"), noise) +
#  geom_point(aes(colour = as.factor(cluster)), groups,
 #            size = 3) +
#  coord_map() +
#  theme(legend.position = "none")

clusters <- kmeans(salary_variables, 6)
salary_with_dep$cluster <- clusters$cluster
geo$cluster <- clusters$cluster
 
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(geo$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on salary")
salary_geo

salary_cl <- cbind(salary_variables, clusters$cluster)
names(salary_cl)[33] <- "cluster"
salary_cluster <- aggregate(salary_cl$sal_general, by=list(salary_cl$cluster), FUN=mean)[2]
names(salary_cluster)[1] <- "salary" 

 
```


### Clustering on merged dataset
```{r}
#create dataframe with first three principal components scores as variables
newDat_pca_scores<- as.data.frame(pca_newDat$x[,1:3])
newDat_pca_scores <- as.data.frame(scale(newDat_pca_scores))
sapply(newDat_pca_scores, var)

#sample data
#set.seed(123)
#newDat_scores_sampled <- sample_n(newDat_variables, 1000)

#renumber rows
#row.names(newDat_scores_sampled) <- 1:nrow(newDat_scores_sampled)

#create distance matrix for hclust
dm <- dist(newDat_variables)
#round to two decimals
dm <- round(dm, 2)

# Below we show the cluster solutions for
# the merged data. The top row gives the cluster
# dendograms along with the cutoff used to
# derive the classes presented (in the space
# of the three principal components) in the
# bottom row:


# newDat_pc <- princomp(dm, cor = TRUE)
# layout(matrix(1:6, nr = 2), height = c(2, 1))
# plot(cs <- hclust(dm, method = "single"), 
#      main = "Single")
# abline(h = 3.8, col = "lightgrey")
# xlim <- range(newDat_pc$scores[,1])
# plot(newDat_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(cs, h = 3.8)
# text(newDat_pc$scores[,1:2], labels = lab, 
#      cex=0.6)
# plot(cc <- hclust(dm, method = "complete"), 
#      main = "Complete")
# abline(h = 7, col = "lightgrey")
# plot(newDat_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(cc, h = 7)  
# text(newDat_pc$scores[,1:2], labels = lab, 
#      cex=0.6)     
# plot(ca <- hclust(dm, method = "average"), 
#      main = "Average")
# abline(h = 7.8, col = "lightgrey")
# plot(newDat_pc$scores[,1:2], type = "n", 
#      xlim = xlim, ylim = xlim,
#      xlab = "PC1", ylab = "PC2")
# lab <- cutree(ca, h = 7.8)                             
# text(newDat_pc$scores[,1:2], labels = lab, 
#      cex=0.6)  
# 
# # restore layout 
# layout(matrix(1, nr = 1)) 
 
# K-means clustering
n <- nrow(newDat_pca_scores)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(newDat_pca_scores, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(newDat_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 11
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 6 groups 
plot(newDat_pca_scores, 
     pch = kmeans(newDat_pca_scores, 
                  centers = 6)$cluster)

#now we have to see whether these groups correspond to geographical regions...
```
### Clustering on selected variables
```{r}
selected_pca_scores<- as.data.frame(pca_selected_variables$x[,1:3])

# K-means clustering
n <- nrow(selected_variables)
wss <- rep(0, 22)
wss[1] <- (n - 1) * sum(sapply(selected_variables, var))
for (i in 2:22)
    wss[i] <- sum(kmeans(selected_variables,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 7
plot(1:22, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

clusters <- kmeans(selected_variables, centers = 7)
scatter3d(x = selected_pca_scores$PC1, y = selected_pca_scores$PC2, z = selected_pca_scores$PC3, groups = as.factor(clusters$cluster),
          grid = FALSE, surface = FALSE)

FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(clusters$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on selected variables")
salary_geo

#plot3d(selected_pca_scores, col=clusters$cluster)
# newDat_matrix <- as.matrix(selected_pca_scores)
# kNNdistplot(newDat_matrix, k=5)
# abline(h=1, col="red")
# 
# set.seed(1234)
# db = dbscan(newDat_matrix, 1, 50)
# db
# 
# # newDat_pca_scores$cluster <- db$cluster
# # newDat_pca_scores$cluster <- newDat_pca_scores$cluster + 1
# plot3d(selected_pca_scores[,1:3], col=db$cluster+1)
# 
# 
# cl <- hdbscan(newDat_pca_scores, minPts = 49)
# cl
# plot3d(newDat_pca_scores[,1:3], col=cl$cluster+1)
# 
# cluster <- sNNclust(newDat_pca_scores, 20, 10, 10)
# cluster
# plot3d(newDat_pca_scores[,1:3], col=cluster$cluster+1)
# 
# cluster <- jpclust(newDat_pca_scores, 15, 10)
# cluster
# plot3d(newDat_pca_scores[,1:3], col=cluster$cluster+1)
```
```{r}
# install.packages(c("rgl", "car"))
# library("car")
scatter3d(x = selected_pca_scores$PC1, y = selected_pca_scores$PC2, z = selected_pca_scores$PC3, groups = as.factor(cluster$cluster+1),
          grid = FALSE, surface = FALSE)

```
###clustering in commune
```{r}
commune_trimmed <- commune[,c("nb_firms_service","nb_firms_commerce","nb_firms_construction")]
commune_trimmed$active_empl_prop <- commune$nb_active_employees/commune$population
commune_trimmed$nb_firms_service <- commune$nb_firms_service/commune$population
commune_trimmed$nb_firms_commerce <- commune$nb_firms_commerce/commune$population
commune_trimmed$nb_firms_construction <- commune$nb_firms_construction/commune$population
#commune_trimmed <- as.data.frame(scale(commune_trimmed))

names(which(sapply(commune_trimmed, anyNA)))
str(commune_trimmed)
#str(newDat_variables)
#newDat_variables_dep <- cbind(geo[1], newDat_variables)

# i <- sapply(commune_trimmed, is.factor)
# commune_trimmed[i] <- lapply(commune_trimmed[i], as.character)
# str(commune_trimmed)

newDat_matrix <- as.matrix(commune_trimmed)
kNNdistplot(newDat_matrix, k=5)
abline(h=0.01, col="red")

set.seed(1234)
db = dbscan(newDat_matrix, 0.01, 50)
db

cl <- hdbscan(commune_trimmed, minPts = 15)
cl

#K means

n <- nrow(commune_trimmed)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(commune_trimmed, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(commune_trimmed,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 11
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 6 groups 
plot(commune_trimmed, 
     pch = kmeans(commune_trimmed, 
                  centers = 6)$cluster)


```

###PCA on commune
```{r}
commune_pca <- prcomp(commune_trimmed)
commune_pca
commune_pca_scores<- as.data.frame(commune_pca$x[,1:3])

autoplot(commune_pca, data = geo, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(commune_pca, type = "l")

plot3d(commune_pca[,1:2], col=cluster$cluster+1)

```


```{r}
# K-means clustering
n <- nrow(newDat_variables)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(newDat_variables, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(newDat_variables,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")
```
```{r}
#now we have to see whether these groups correspond to geographical regions...
clusters <- kmeans(newDat_variables, 10)
newDat_variables_dep$cluster <- clusters$cluster
geo$cluster <- clusters$cluster

FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
newDat_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(geo$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on merged Dataset predictors")
newDat_geo
```


### Mapping the new clusters 
```{r}
#now we have to see whether these groups correspond to geographical regions...
clusters <- kmeans(newDat_pca_scores, 6)
newDat_variables_dep$cluster <- clusters$cluster
geo$cluster <- clusters$cluster

newDat_pca_scores$cluster <- clusters$cluster
library(rgl)
plot3d(newDat_pca_scores[,1:3], col=newDat_pca_scores$cluster)
 
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
newDat_geo <-
  FraMap +
  geom_point(aes(x=geo$longitude, y=geo$latitude, colour= factor(geo$cluster)),
             data=geo, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on merged Dataset predictors")
newDat_geo

```
```{r}
newDat_pca_scores$cluster <- clusters$cluster
library(rgl)
plot3d(newDat_pca_scores[,1:3], col=newDat_pca_scores$cluster)
```



# Supervised Learning

## ANOVA for salary

To check if the difference in the mean in salary betweem males and females is statistically significant,
a t-test is performed
```{r salary difference t test}

t.test(salary$sal_Males, salary$sal_Females, alternative = c("two.sided"))

```

This solution can also be otbained fitting a linear model with a dummy variable catching the sex effect,
which corresponds to an ANOVA test
```{r salary difference t test as an ANOVA}

# create the reponse attaching the salary for each sex
sal_y = c(salary$sal_Males, salary$sal_Females)
# create the dummy variable controlling for the corresponding sex
dummy = c(rep(1, length(salary$sal_Males)), rep(0, length(salary$sal_Females)))

# the t value is exactly the same as the one obtained in the previous t-test
sal_ANOVA = lm(sal_y ~ dummy)
summary(sal_ANOVA)

```

To improve such analysis some more aspects are now considered.
In particualar, the model considered in an ANOVA, where the
predictors take in account each gender for different age and job levels,
plus their interaction terms.

First of all the dataset is created and a subsample of it is considered, 
in order to avoid strong correlation among units
```{r salary construct full ANOVA model}

# create response variable
sal_y = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus,
          salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
          salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus,
          salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker)

n_sal_y = length(sal_y)             # length response variable
n_cat = length(salary$sal_M_18_25)  # length of each category (i.e., original vectors)

# create sex dummy variable, 1 for males and 0 for females
sal_sex = rep(0, n_sal_y)   # full regressors
sal_sex[1:n_sal_y/2] = 1    # assign males

# create age dummy variables, 18-25 years old is the base case
sal_age = cbind(rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# 26-50 y.o.
sal_age[(n_cat+1):(n_cat*2), 1] = 1     # males
sal_age[(n_cat*8+1):(n_cat*9), 1] = 1   # females
# 51+ y.o.
sal_age[(n_cat*2+1):(n_cat*3), 2] = 1   # males
sal_age[(n_cat*9+1):(n_cat*10), 2] = 1  # females

# create job type dummy variables, worker is the base case
sal_job = cbind(rep(0, n_sal_y), rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# executives
sal_job[(n_cat*3+1):(n_cat*4), 1] = 1     # males
sal_job[(n_cat*10+1):(n_cat*11), 1] = 1   # females
# middle managers
sal_job[(n_cat*4+1):(n_cat*5), 2] = 1     # males
sal_job[(n_cat*11+1):(n_cat*12), 2] = 1   # females
# employee
sal_job[(n_cat*5+1):(n_cat*6), 3] =   1   # males
sal_job[(n_cat*12+1):(n_cat*13), 3] = 1   # females

# final data set
data_ANOVA = cbind.data.frame(response = sal_y, sex = sal_sex, age = sal_age, job = sal_job)
# names(data_ANOVA)
# show regressors' shape
imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")
axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl."))
box()
# sub sample to avoid correlation, (unbalanced dataset)
  # set.seed(20)
  subs = sample(1:n_sal_y, size = round(n_sal_y*0.4))
  data_ANOVA = data_ANOVA[subs,]

# show randomized data
imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA after sub-sampling")
axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl."))
box()

```

Run the ANOVA model, tranforming the response variable using Box-Cox transformation
```{r salary ANOVA model}

# plot the transformed response variable (suggested by Box-Cox transformation)
sal_y = data_ANOVA[,1]^-1  
ggplot(data=data.frame(data_ANOVA[,1]), aes(sal_y)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 40) +
  geom_density(col="black") +
  labs(x="salary^-1", y="Density") +
  ggtitle("Transformation of salary found using Box-Cox transformation") +
  theme(plot.title = element_text(hjust = 0.5))

# take the identified subsample for dummy variables
sex = sal_sex[subs]
age = sal_age[subs,]
job = sal_job[subs,]
# ANOVA model
sal_ANOVA = lm(sal_y ~ sex + age + job + sex:age + sex:job)
summary(sal_ANOVA)
anova(sal_ANOVA)
aov(sal_y ~ sex + age + job + sex:age + sex:job)
plot(sal_ANOVA, 1)

# box-cox transformation suggested to use y^-1
boxcox(sal_ANOVA)
title("Box-Cox transformation of the response")

# comparison a full model with a reduced one excluding one interaction term
sal_ANOVA2 = lm(sal_y ~ sex + age + job + sex:age)
anova(sal_ANOVA, sal_ANOVA2)

```

To check if the model can be sparsified the group-Lasso is applied,
which shows that the interaction term sex:job is the least important.
```{r salary group-Lasso}

# define the considered model, which includes interaction terms
form <- model.matrix(sal_y ~ (sex + age + job)^2)
# remove interaction age:job
colnames(form)
form = form[,2:12]

# define groups' labels
grp = c(1,2,2,3,3,3,4,4,5,5,5)

#Group-Lasso with 10-folds Cross Validation and LAD cost function
fit.cv=cv.gglasso(x=form, y=sal_y, group=grp, nfolds=10, pred.loss = "L2")

# plot CV for Lambda
plot(fit.cv, main = "10-folds CV for group-Lasso in ANOVA", cex = 0.1)
# mtext(expression("10-folds CV for group-Lasso in ANOVA"), outer=TRUE,  cex=1, line=-0.2)

# extract coefficients
# coef.mat = fit.cv$gglasso.fit$beta
# plot separately each coefficient
cols = brewer.pal(5,name="Set1")
plot(fit.cv$gglasso.fit, col = cols, lwd = 3, main = "Coefficients vs.Lambda values")
legend('bottomright', legend = paste("Group", 1:5), col=cols[1:5], cex = 0.65, pch = 1, lwd=4, bty ="n")
# plot coefficients' norms (not working)
# plot(fit.cv$gglasso.fit, group=1, col = brewer.pal(5,name="Set1"), lwd = 2)
# legend('bottomright', legend = paste("Group", 1:5), col=cols[1:5], cex = 0.65, pch = 1, lwd=4, bty ="n")
 
#Pick the best Lambda
lmbda=fit.cv$lambda.1se
(coefs=coef.gglasso(object=fit.cv$gglasso.fit,s=lmbda))
#At best lambda get coefficients and fitted values
plt=sal_y-predict.gglasso(object=fit.cv$gglasso.fit,newx=form,s=lmbda,type='link')
plot(plt, ylab="residuals", xlab="index", main="Plot of residuals")
abline(0, 0, col= "red")

```

## Difference in salary: men vs. females

The focus of this part is on the delta between males and females salaries,
comparing classical and robust approaches (with respect to noise/outliers).

Function for interactive plotting used in the next chunks
```{r function interactive regression diagnostic plots}

# This function is used to obtain interactive regression diagnostic plots
RegressionPlots <- function(fit, textlabels, robust=F, out = NULL){
  
  # number of units
  n = length(fit$fitted.values)
  
  # original response
  y = fit$y
  
  # Extract fitted values from lm() object
  Fitted.Values <-  fit$fitted.values
  
  # Extract residuals from lm() object
  Residuals <-  fit$residuals
  
  if (robust == F){
    # Extract standardized residuals from lm() object
    Standardized.Residuals <- MASS::stdres(fit)  
    
    # Calculate Leverage
    Leverage <- lm.influence(fit)$hat
    
    # text for the labels of the plots
    tt = "OLS"
  } else{
    # standardized residuals based on a robust scale estimate
    Standardized.Residuals <- fit$residuals/fit$scale
    
    # get the robust distances based on mahalanobis distance from the robust output 
    # (it has to be specified in the call to lmrob using: control = lmrob.control(compute.rd = T))
    Leverage <- fit$MD
    
    # text for the labels of the plots
    tt = "Robust"
  }
  
  # Extract fitted values for lm() object
  Theoretical.Quantiles <- qqnorm(Residuals, plot.it = F)$x
  
  # Create data frame 
  # Will be used as input to plotly
  regMat <- data.frame(Fitted.Values, 
                       Residuals, 
                       Standardized.Residuals, 
                       Theoretical.Quantiles,
                       Leverage, 
                       textlabels)
  
  # Plot using Plotly
  
  # text info
  t <- list(
    family = "sans serif",
    size = 14,
    color = toRGB("grey50"))
  
  
  # Plot 1: Fitted vs Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Residuals)
  
  plt1 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 

    layout(title = paste(tt, " Residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Residuals"))
  
  # Plot 2: Fitted vs Standardized Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Standardized.Residuals)
  
  plt2 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
    
    layout(title = paste(tt, " Standardized residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Standardized residuals"))  

  # Plot 3: Residuals index
  plt3 <- regMat %>% 
    plot_ly(x = 1:n, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', 1:n,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_segments(x = -5, xend = n+5, y = 0, yend = 0, mode = "line", inherit = F, 
                 name = "Null line", line = list(width = 2)) %>%
    
    layout(title = paste(tt, " Standardized Residuals' index"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Index"),
             yaxis = list(title = "Standardized.Residuals")) 
    

  # Plot 4: QQ Pot
  plt4 <- regMat %>% 
    plot_ly(x = Theoretical.Quantiles, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Theoretical.Quantiles,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = Theoretical.Quantiles, y = Theoretical.Quantiles, type = "scatter", 
              mode = "lines", name = "Theoretical", line = list(width = 2), 
              inherit = F) %>%
    
    layout(title = paste(tt, " Q-Q Plot"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Normal theoretical quantiles"),
             yaxis = list(title = "Data quantiles"))  

  
  # Plot5: Residuals vs Leverage
  # For scatter plot smoother
  LOESS <- loess.smooth(Leverage, Residuals)
  
  plt5 <- regMat %>% 
    plot_ly(x = Leverage, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5), 
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Leverage,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              line = list(width = 2), inherit = F) %>%
    
    layout(title = paste(tt, " Leverage vs Residuals"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Leverage values"),
             yaxis = list(title = "Residuals"))  


  # Plot 6: Fitted vs response
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, y)
  
  plt6 <- regMat %>% 
    plot_ly(x = Fitted.Values, y =y, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            color = out, name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', y)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
  
    layout(title = paste(tt, " Fitted Values vs response"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Fitted Values"),
             yaxis = list(title = "Response values"))

  # # Plot 7: MISSING: sqrt(abs(Residuals))
  # # For scatter plot smoother
  # LOESS2 <- loess.smooth(Fitted.Values, Root.Residuals)
  # 
  # plt3 <- regMat %>% 
  #   plot_ly(x = Fitted.Values, y = Root.Residuals, 
  #           type = "scatter", mode = "markers", hoverinfo = "text", 
  #           name = "Data", marker = list(size = 10, opacity = 0.5),
  #           text = paste('town: ', as.character(textlabels))) %>%
  #   
  #   add_trace(x = LOESS2$x, y = LOESS2$y, type = "scatter", mode = "lines", name = "Smooth",
  #             line = list(width = 2), inherit = F) %>% 
  #   
  #   layout(title = "Scale Location", plot_bgcolor = "#e6e6e6")
  plt = list(plt1, plt2, plt3, plt4, plt5, plt6)
  return(plt)

  # not working...to add in all plots
  # to avoid error with colorPalette<3
  suppressWarnings(warning("RColorBrewer::brewer.pal"))
  
}

```

Build the considered model
```{r linear model for delta M-F salary, fig.keep='all'}

# response variable
y = newDat$sal_Males - newDat$sal_Females
# original response variable histogram
ggplot(data=as.data.frame(y), aes(y)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) +
  geom_density(col="black") +
  labs(x="Male-Female salary", y="Density") +
  ggtitle("Original response variable") +
  theme(plot.title = element_text(hjust = 0.5))

x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)
# merge variables
mod = cbind.data.frame(y, x)

# scatter plot matrix
# pairs(mod)
# correlations
corrplot(cor(mod), method = "number", title = "Correlation matrix",
         diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black")  # , mar=c(0,0,1.5,0)
plot(newDat$sal_M_26_50 - newDat$sal_F_26_50,y)
```

A classical linear model is first fitted
```{r OLS fit}

# OLS
ols = lm(y ~ ., data = mod, y=T)
summary(ols)

# diagnostic plots
# layout(matrix(1:6, nr = 2), height = c(2, 1))
RegressionPlots(ols, newDat$town, robust = F)

```

Model fitting using MM estimator:
```{r MM fit}

# robust fit using MM estimator
mm = lmrob(y~., data = mod, y=T, control = lmrob.control(compute.rd = T)) # setting = "KS2014",
summary(mm)
# outliers identified
outl = mm$rweights < 2e-05

# diagnostic plots
# layout(matrix(1:6, nr = 2), height = c(2, 1))
# plot(mm, cex = 0.6, id.n = 10, labels.id = newDat$town_name, sub.caption="MM")
RegressionPlots(mm, newDat$town, robust = T)
outl = mm$rweights < 2e-05

```

Specify the model:
```{r model specification}

# copy of the previous options
y = newDat$sal_Males - newDat$sal_Females
x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)

# merge variables
mod = cbind.data.frame(y, x)
# merge variables excluding ouliers
# mod = mod[!outl,]

# add transformations on the predictors
mod = cbind.data.frame(mod, mod[,2:ncol(mod)]^2, mod[,2:ncol(mod)]^3)
namesdataBS = names(mod)
lll = ((dim(mod)[2]-1)/3)
# for (i in 1:lll+1){
#   str = namesdataBS[i]
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1]
# }
namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll), sep="")
namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll), sep="")
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll))
names(mod) = namesdataBS

```


Use 10-folds cross validation on Elastic-net:
```{r CV for Elastic net}

# initialize list to contain the variables selected and the counter
var_selec = list()
i = 1

# Elastic net
x = as.matrix(mod[, 2:ncol(mod)])
y = mod[, 1]
par(mfrow =c(3,2))
par(oma=c(0,0,2,0))
alp = c(0, 0.2, 0.4, 0.6, 0.8, 1)
for (j in alp){
  # set.seed (3)
  cv.out = cv.glmnet(x, y, alpha = j, standardize.response = T ) # standardize.response = T , parallel = T 
  plot(cv.out)
  title(paste("alpha = ", j), line = 0.3)
  tmp_coeffs <- coef(cv.out, s = "lambda.min") # cv.out$lambda.1se 
  var_selec[[i]] = data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
  i = i + 1
}
mtext(expression("Best lambda for delta gender salary using elastic-net and 10-folds CV"), outer=TRUE,  cex=1, line=0.2)

# compare the results
print(var_selec)

# Manual evaluation
# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE',
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 

```

Add a second step OLS estimate on the variables identified by LASSO:
```{r OLS on Lasso solution}

for (i in 1:6){
  mod_red = mod[,colnames(mod) %in% var_selec[[i]]$name ] 
  mod_red = cbind.data.frame(y, mod_red)
  olsRed = lm(y ~ ., data = mod_red, y=T)
  print("--------------------------------------------")
  print(paste("Elastic net based on alpha = ", alp[i]))
  print("--------------------------------------------")
  print(summary(olsRed))
}

# # OLS
# ols = lm(y ~ ., data = mod, y=T)
# summary(ols)
# 
# # diagnostic plots
# # layout(matrix(1:6, nr = 2), height = c(2, 1))
# RegressionPlots(ols, newDat$town_name, robust = F)

```


Using best subset selection.

Function to plot the results
```{r plot.regsubsets2}

plot.regsubsets2 <-  
  function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
     "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
  { 
    obj <- x 
    lsum <- summary(obj) 
    par(mar = c(7, 5, 6, 3) + 0.1) 
    nmodels <- length(lsum$rsq) 
    np <- obj$np 
    propscale <- FALSE 
    sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
                     nomatch = 0) 
    if (sscale == 0)  
      stop(paste("Unrecognised scale=", scale)) 
    if (propscale)  
      stop(paste("Proportional scaling only for probabilities")) 
    yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
    up <- switch(sscale, -1, -1, 1, 1) 
    index <- order(yscale * up) 
    colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
                                                           1e-04)), -log(pmax(yscale, 1e-04))) 
    image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
                       NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
          x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
          col = col) 
    laspar <- par("las") 
    on.exit(par(las = laspar)) 
    par(las = 2) 
    axis(1, at = 1:np, labels = labels, ...) # I modified this line 
    axis(2, at = 1:nmodels, labels = signif(yscale[index], 2), ...) 
    # axis(2,cex.axis=0.01)
    if (!is.null(main))  
      title(main = main) 
    box() 
    invisible(NULL) 
    
  } 
```

BSS results:
```{r BSS results}

# best subset selection
best.sub = regsubsets(y ~ ., data = mod, nvmax = ncol(mod))
best.sub.summary = summary(best.sub)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
mtext("Best subset selection for delta gender salary", outer=TRUE,  cex=1.2, line=-1.5)

# built-in plots
par(mfrow=c(1,1))
plot.regsubsets2(best.sub, scale = "r2", cex.axis = 0.6, main = "Best subset selection using R squared")
plot.regsubsets2(best.sub, scale = "adjr2", cex.axis = 0.6, main = "Best subset selection using adjusted R squared")
plot.regsubsets2(best.sub, scale = "Cp", cex.axis = 0.6, main = "Best subset selection using Mallows' Cp")
plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.6, main = "Best subset selection using BIC")

# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5)

# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))
nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
nnn <- gsub(x = nnn,
            pattern = "`",
            replacement = "")
# apply OLS
mod_bic = mod[,colnames(mod) %in% nnn ] 
mod_bic = cbind.data.frame(y, mod_bic)
ols_bic = lm(y ~ ., data = mod_bic, y=T)
summary(ols_bic)

```

Using sparse-LTS, which is a robustification of Lasso
```{r sparseLTS, warning=FALSE}

# fit robust Lasso
sol_sparseLTS = sparseLTS(x, y, mode = "lambda", alpha = 0.95, ncores = NA, model=T)
# check the solution
sol_sparseLTS
# construct mahalanobis distance (used for diagnostic plots)
sol_sparseLTS$MD = covMcd(x)$mah
# diagnostic plots
RegressionPlots(sol_sparseLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_sparseLTS$wt))[[2]]
RegressionPlots(sol_sparseLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_sparseLTS$wt))[[3]]
RegressionPlots(sol_sparseLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_sparseLTS$wt))[[4]]

```


Robust elastic net
```{r enetLTS, echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}


sol_enetLTS = enetLTS(x, y, family="gaussian", alphas=seq(0, 1, 0.2), hsize=0.9, nfold=10, seed=1)
# check the solution
print(sol_enetLTS)


# save original response (used for diagnostic plots)
sol_enetLTS$y = y
# construct mahalanobis distance (used for diagnostic plots)
sol_enetLTS$MD = covMcd(x)$mah
# construct robust scale (used for diagnostic plots)
sol_enetLTS$scale = sd(sol_enetLTS$residuals[!sol_enetLTS$wt])

# interactive plots
RegressionPlots(sol_enetLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_enetLTS$wt))[[2]]
RegressionPlots(sol_sparseLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_sparseLTS$wt))[[3]]
RegressionPlots(sol_sparseLTS, textlabels=newDat$town, robust = TRUE, out = as.factor(sol_sparseLTS$wt))[[4]]
# built-in plots
plotCoef.enetLTS(sol_enetLTS, cex = 2)
plotDiagnostic.enetLTS(sol_enetLTS, cex = 2)

# perform CV for user-values
# # Calculate lambda path (first get lambda_max):
# lambda_max <- 1 # max(abs(colSums(sx*sy)))/n
# epsilon <- .0001
# K <- 10
# lambdapath <- round(exp(seq(log(lambda_max), log(lambda_max*epsilon), 
#                             length.out = K)), digits = 10)
# sol_cv = cv.enetLTS(index=NULL,x,y,family="gaussian",round(length(y)*0.90),
#            seq(0, 1, 0.2),lambdas = lambdapath ,ncores=1,repl=5,nfold=10,plot=TRUE)

```


# Interactive map (trial)

France map:
```{r}
# require dev version
# devtools::install_github('ropensci/plotly')
# library(plotly)

# temporary here...
newDat$salary_ratio_FvsM = salary$salary_ratio_FvsM

newDat$firms_over_pop = newDat$total/newDat$total_population
m <- list(colorbar = list(title = "Total Inches"))

# geo styling
g <- list(
  scope = 'france',
  showland = TRUE,
  landcolor = plotly::toRGB("grey86"),
  subunitcolor = plotly::toRGB("white"),
  countrycolor = plotly::toRGB("black"),
  showlakes = TRUE,
  lakecolor = plotly::toRGB("white"),
  showsubunits = TRUE,
  showcountries = TRUE,
  resolution = 50,
  projection = list(
    type = 'conic conformal',
    rotation = list(lon = 1)
  ),
  lonaxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(-5, 10),
    dtick = 5
  ),
  lataxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(40, 55),
    dtick = 5
  )
)


p <- plotly::plot_geo(newDat, lat = ~latitude, lon = ~longitude) %>%
  plotly::add_markers(
    text = ~paste(town, paste("Firms tot:", total), 
                  paste("Population:", total_population),
                  paste("Mean salary:", sal_general), 
                  paste("Salary ratio FvsM:", round(salary_ratio_FvsM, 4)),
                  sep = "<br />"),
    color = ~firms_over_pop,  size = I(2), hoverinfo = "text", opacity = 0.8 #, colors = 'Purples'
  ) %>%
  plotly::colorbar(title = "Total population<br />in 2014") %>%
  plotly::layout(title = 'Geographic information about France', geo = g)
p

```



# FIRMS: old code

## PCA 

PCA on firms data:

```{r PCA}

# pairs(firms[, 3:8])
# 
# salary_NEW = salary[, 3:ncol(salary)]
# # colnames(salary_NEW) = 1:32
# corrplot(cor(salary_NEW), method = "circle", title = "Correlation matrix for salary", 
#          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1,0)) 
# 
# 
# # firms_clean <- firms[firms$micro < 20000 & firms$large < 200,]
# firms_clean <- firms[firms$micro > 1 & firms$micro < 2500 & 
#                      firms$small > 1 & firms$small < 800 & 
#                      firms$medium > 1 & firms$medium < 500 & 
#                      firms$large > 1 & firms$large < 200 &
#                      firms$null > 1 & firms$null < 2000 ,]
# myPr <- prcomp(firms_clean[, 4:8], scale = TRUE)
# #plot(scale(firms_clean$micro), scale(firms_clean$large))
# #mean(firms_clean$micro)
# #mean(firms_clean$large)
# myPr
# summary(myPr)
# plot(myPr, type = "l")
# # biplot(myPr, scale = 0)
# #extract PC scores...
# str(myPr)
# #myPr$x #checking principal component scores
# firms2 <- cbind(firms_clean, myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])
# 
# # using ggbiplot
# ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
#   ggtitle("Biplot for the firms's size") +
#   theme(plot.title = element_text(hjust = 0.5))
```

Modified PCA [Luca]:
```{r PCA 2}

# get Paris in order to exclude it in the following
Paris = which.max(firms$total)

# Original scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], gap=0, main = "Scatter matrix for firms")

# Correlation matrix
corrplot(cor(firms[-Paris, 3:8]), method = "number", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0))

# Log scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], log = "xy", gap=0, main = "Scatter matrix for firms in log scale")

# Correlation matrix
firmsLog = log(firms[, 3:8]) 
firmsLog[firmsLog == -Inf] = 0
corrplot(cor(firmsLog), method = "number", title = "Correlation matrix for firms  in log scale", 
          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
          tl.col = "black", mar=c(0,0,1.5,0))

# PCA
myPr <- prcomp(firms[-Paris, 3:8], scale = TRUE)
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0)
#extract PC scores...
str(myPr)
#myPr$x #checking principal component scores
# firms2 <- cbind(firms[-Paris, 3:8], myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])

# using ggbiplot
ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
  ggtitle("Biplot for the firms's size") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Cluster Analysis 

```{r clustering}
# firms_sampled <- firms[1:1000, ] # subsampling
# head(firms_sampled)
# firms_scaled <- scale(firms_sampled[, 3:8]) #scaling the data
# 
# head(firms_scaled)
# 
# firms_truncated <- firms_sampled[, 4:8]
# head(firms_truncated)
# plot(firms_sampled)
# # K-means clustering...
# 
# 
# fitK_scaled <- kmeans(firms_scaled, 4) 
# head(fitK_scaled)
# 
# fitK <- kmeans(firms_truncated, 5)
# head(fitK)
# str(fitK)
# plot(firms_sampled, col = fitK$cluster) #plotting data colored according to cluster membership
# 
# #choosing K---
# k <- list()
# for(i in 1:10){
#   k[[i]] <- kmeans(firms_truncated, i)
# }
# head(k)
# 
# betweenss_totalss <- list()
# for(i in 1:10){
#   betweenss_totalss[[i]] <- k[[i]]$betweenss/k[[i]]$totss
# }
# plot(1:10, betweenss_totalss, type ="b",
#      ylab = "Between SS / Total SS", xlab = "Clusters (k)") #calculating and plotting between SS to total SS ratio against number of clusters
# 
# for(i in 1:5) {
#   plot(firms, col = k[[i]]$cluster) #plotting data based on membership to clusters for k = 1 to 5 clusters
# }
# head(fitM)
# fitM <- Mclust(firms_truncated)
# plot(fitM)
# #Model-based clustering using mclust
# 
# head(clusters)
# plot(firms_sampled, col = clusters) # as we can see, it performs quite similar to the K-means 
# rect.hclust(fitH, k = 5, border = "red") # visualising dendrogam cut at k =5
# clusters <- cutree(fitH, 5)  # vector with cluster membership for each observation
# plot(fitH)
# d <- dist(firms_truncated)
# fitH <- hclust(d, "ward.D2")
# 
# #Hierarchical clustering---
# 

```

# SALARY: old code
## Linear models

First we perform some easy task
fitting a regression model to predict the salaries of people in age 26-50 using as regressor 51+ years:

```{r exLinMod, fig.keep='all'}
# # fit and show OLS estimate
# plot(salary$sal_26_50 ~ salary$sal_51plus)
# fit_LM_26_50 = lm(salary$sal_26_50 ~ salary$sal_51plus, data = salary) 
# abline(fit_LM_26_50, lwd=3, col="red")
# # diagnostics
# summary(fit_LM_26_50)
# plot(fit_LM_26_50)
# 
# # Same as before but adding polynomials which are evauated using 10-folds cross validation
# set.seed(1)
# # k-Fold Cross-Validation
# cv.err.K = rep(0, 5)
# cv.err.K = rbind(cv.err.K, cv.err.K)
# for (i in 1:5){
#   fit_LM_26_50.K = glm(sal_26_50 ~ poly(sal_51plus, i), data = salary)
#   cv.err.K[,i] = cv.glm(salary, fit_LM_26_50.K, K = 10)$delta[1]
# }
# # plotting results
# plot(cv.err.K[1,], type = 'l', col = 'red', xlab = "Polynomials' order", 
#      ylab = "10-folds CV", main = "CV and adjusted CV for different polynomials")
# lines(cv.err.K[2,], col = 'green')
# points(which.min(cv.err.K), cv.err.K[1, which.min(cv.err.K)], col = "red", cex=2, pch=20)
# legend('topright', legend = c('CV', 'Adj. CV'), col = c('red', 'green'), pch = 10)

```


Predicting salary for young people using Elastic NEt with CV and 10-folds CV. The predictors are most of the original variables and their 2nd, 3rd order polynomials and log tranformation:

```{r}
# # funtion to plot BSS
# plot.regsubsets2 <-  
#   function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
#                                                            "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
#   { 
#     obj <- x 
#     lsum <- summary(obj) 
#     par(mar = c(7, 5, 6, 3) + 0.1) 
#     nmodels <- length(lsum$rsq) 
#     np <- obj$np 
#     propscale <- FALSE 
#     sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
#                      nomatch = 0) 
#     if (sscale == 0)  
#       stop(paste("Unrecognised scale=", scale)) 
#     if (propscale)  
#       stop(paste("Proportional scaling only for probabilities")) 
#     yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
#     up <- switch(sscale, -1, -1, 1, 1) 
#     index <- order(yscale * up) 
#     colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
#                                                            1e-04)), -log(pmax(yscale, 1e-04))) 
#     image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
#                        NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
#           x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
#           col = col) 
#     laspar <- par("las") 
#     on.exit(par(las = laspar)) 
#     par(las = 2) 
#     axis(1, at = 1:np, labels = labels, ...) # I modified this line 
#     axis(2, at = 1:nmodels, labels = signif(yscale[index], 2)) 
#     if (!is.null(main))  
#       title(main = main) 
#     box() 
#     invisible(NULL) 
#   } 
# 
# set.seed(2018)
# 
# # load the data
# ind = salary$sal_18_25 < 11.5
# y = salary$sal_18_25[ind]
# x = cbind.data.frame(salary$sal_26_50, salary$sal_51plus, 
#                      salary$sal_general,  salary$sal_executive, salary$sal_midManager, 
#                      salary$sal_employee, salary$sal_worker, 
#                      salary$sal_Males, salary$sal_Females)
# x = x[ind, ]
# yx = cbind.data.frame(y, x)
# 
# # original response variable histogram
# ggplot(data=salary, aes(salary$sal_18_25)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # response variable histogram (excluding outliers)
# ggplot(data=salary[ind,], aes(y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # after sampling to avoid serial correlation
# indd = sample(1:length(y), round(length(y)*0.4))
# yx = yx[indd,]
# ggplot(data=salary[ind,][indd,], aes(y[indd])) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 30) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Sampled response variable and excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # correlations
# # corrplot(cor(yx), method = "number", title = "Correlation matrix for salary",
# #          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
# #          tl.col = "black")  # , mar=c(0,0,1.5,0)
# 
# # scatter matrix
# # pairs(yx, gap=0, main = "Scatter matrix for some variables in salary")
# # manually remove outliers in mid mangager
# summary(yx)
# inddd = yx$`salary$sal_midManager` < 20
# yx = yx[inddd,]
# # final dataset
# pairs(yx, gap=0, main = "Scatter matrix for the variables in the model")
# 
# # create final dataframe
# require(leaps) 
# # assign names for predictors transformation
# namesdataBS = names(yx)
# yx = cbind.data.frame(yx, yx[,2:ncol(yx)]^2, yx[,2:ncol(yx)]^3, log(yx[,2:ncol(yx)]))
# dim(yx)
# lll = ((dim(yx)[2]-1)/4) 
# for (i in 1:lll+1){ 
#   str = namesdataBS[i] 
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1] 
# } 
# namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll)) 
# namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll)) 
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll)) 
# names(yx) = namesdataBS 
# 
# # Elastic net
# x = as.matrix(yx[, 2:ncol(yx)])
# y = yx[, 1]
# par(mfrow =c(3,2))
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   cv.out = cv.glmnet(x, y, alpha = j)
#   plot(cv.out)
#   title(paste("alpha = ", j), line = 2.3)
# }
# mtext(expression("Best lambda for salary 18-25 using elastic and 10-folds CV"), outer=TRUE,  cex=1, line=-1.4) 
# 
# 
# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 
# 
# # best subset selection
# best.sub = regsubsets(y ~ ., data = yx, nvmax = ncol(yx)) 
# best.sub.summary = summary(best.sub) 
# # manual plotting 
# par(mfrow =c(2,2))
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
# # built-in plots 
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "adjr2") 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.9) 
# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5) 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# names(coefficients(best.sub, 10))
# # nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
# 


```



ANOVA model for salary:
```{r}
# 
# # create response variable
# sal_y = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus,
#           salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
#           salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus,
#           salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker)
# 
# n_sal_y = length(sal_y)             # length response variable
# n_cat = length(salary$sal_M_18_25)  # length of each category (i.e., original vectors)
# 
# # create sex dummy variable, 1 for males and 0 for females
# sal_sex = rep(0, n_sal_y)   # full regressors
# sal_sex[1:n_sal_y/2] = 1    # assign males
# 
# # create age dummy variables, 18-25 years old is the base case
# sal_age = cbind(rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # 26-50 y.o.
# sal_age[(n_cat+1):(n_cat*2), 1] = 1     # males
# sal_age[(n_cat*8+1):(n_cat*9), 1] = 1   # females
# # 51+ y.o.
# sal_age[(n_cat*2+1):(n_cat*3), 2] = 1   # males
# sal_age[(n_cat*9+1):(n_cat*10), 2] = 1  # females
# 
# # create job type dummy variables, worker is the base case
# sal_job = cbind(rep(0, n_sal_y), rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # executives
# sal_job[(n_cat*3+1):(n_cat*4), 1] = 1     # males
# sal_job[(n_cat*10+1):(n_cat*11), 1] = 1   # females
# # middle managers
# sal_job[(n_cat*4+1):(n_cat*5), 2] = 1     # males
# sal_job[(n_cat*11+1):(n_cat*12), 2] = 1   # females
# # employee
# sal_job[(n_cat*5+1):(n_cat*6), 3] =   1   # males
# sal_job[(n_cat*12+1):(n_cat*13), 3] = 1   # females
# 
# # final data set 
# data_ANOVA = cbind.data.frame(response = sal_y, sex = sal_sex, age = sal_age, job = sal_job)
# names(data_ANOVA)
# # show regressors' shape
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# # sub sample to avoid correlation
# set.seed(20)
# subs = sample(1:n_sal_y, size = round(n_sal_y*0.2))
# data_ANOVA = data_ANOVA[subs,]
# # show randomized data
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# 
# # plot response variable
# hist(data_ANOVA[,1], 30)
# hist(log(data_ANOVA[,1]), 30)
# hist(sqrt(data_ANOVA[,1]), 30)
# hist(data_ANOVA[,1]^-1, 30)
# sal_y = data_ANOVA[,1]^-1  # also suggested by Box-Cox transformation
# ggplot(data=data.frame(data_ANOVA[,1]), aes(sal_y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 40) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Transformation of salary found using Box-Cox transformation") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# sex = sal_sex[subs]
# age = sal_age[subs,]
# job = sal_job[subs,]
# # ANOVA model
# sal_ANOVA = lm(sal_y ~ sex + age + job + sex:age + sex:job)
# # names(sal_ANOVA$coefficients) = c("Males", "26-50", )
# summary(sal_ANOVA)
# anova((sal_ANOVA))
# 
# # box-cox transformation suggested to use y^-1
# boxcox(sal_ANOVA)
# title("Box-Cox transformation of the response")
# 
# # BSS
# best.sub = regsubsets((sal_y ~ sex + age + job + sex:age + sex:job), 
#                       data = data_ANOVA, nvmax = 12) 
# best.sub.summary = summary(best.sub) 
# # plots: manual plotting 
# par(mfrow =c(2,2)) 
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-2.5) 
# # existing plots
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.7) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5) 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "adjr2") 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# 
# 
# # group lasso
# # install.packages("gglasso")
# library(gglasso)
# # install.packages("RColorBrewer")
# library(RColorBrewer)
# # install.packages("zoo")
# library(zoo)
# grp = c(1,2,2,3,3,3,4,4,5,5,5)
# form <- model.matrix(sal_y ~ (sex + age + job)^2)
# head(form)
# form = form[,2:12]
# fit = gglasso(x=form,y=sal_y,group=grp,loss='ls')
# coef.mat=fit$beta
# 
# #Group1 enters the equation
# g1=max(which(coef.mat[1,]==0))
# #Group2 enters the equation
# g2=max(which(coef.mat[2,]==0))
# #Group3 enters the equation
# g3=max(which(coef.mat[4,]==0))
# #Group4 enters the equation
# g4=max(which(coef.mat[7,]==0))
# #Group5 enters the equation
# g5=max(which(coef.mat[9,]==0))
# #Coefficient Plot
# cols=brewer.pal(5,name="Set1")
# plot(fit$b0,main="Coefficient vs Step",
#      ylab="Intercept",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      xlim=c(-1,100),
#      ylim=c(0.076,max(fit$b0)+0.001),
#      type="l",lwd=4)
# grid()
# par(new=T)
# xx=c(g1,g2,g3,g4,g5)
# yy=c(fit$b0[g1],fit$b0[g2],fit$b0[g3],fit$b0[g4],fit$b0[g5])
# plot(x=xx,y=yy,pch=13,lwd=2,cex=2,col=cols[-1],
#      xlim=c(-1,100),ylim=c(0.076,max(fit$b0)+0.001),
#      xaxt='n',yaxt='n',xlab="",ylab="")
# lmda=round(fit$lambda[c(g1,g2,g3,g4,g5)],4)
# text(x=xx-0.005,y=yy+0.0001,labels=c("Group1","Group2","Group3","Group4","Group5","Group6"),pos=3,cex=0.7)
# text(x=xx-0.005,y=yy-0.0001,labels=paste("Lambda\n=",lmda),pos=1,cex=0.6)
# 
# # coefficient plot 2 (my version)
# cols=brewer.pal(5,name="Set1")
# plot(fit$beta[1,], main="Coefficients vs Lambda",
#      ylab="Coefficients",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      # xlim=c(-1,100),
#      ylim=c(min(fit$beta), max(fit$beta)),
#      type="l",lwd=4)
# for (j in 2:11){
#   lines(fit$beta[j,],
#         type="l",lwd=4, col=cols[j])
# }
# # plot legend once
# grid()
# par(new=T)
# legend('bottomleft', legend = paste("group ", 1:5), lty=1, col=cols[1:5], cex = 0.7,lwd=2)
# 
# #Cross Validation
# fit.cv=cv.gglasso(x=form,y=sal_y,group=grp,nfolds=10)
# plot(fit.cv, main="10-folds CV for groupwise Lasso in ANOVA")
# #Pick the best Lambda
# lmbda=fit.cv$lambda.1se
# (coefs=coef.gglasso(object=fit,s=lmbda))
# #At best lambda get coefficients and fitted values
# plt=sal_y-predict.gglasso(object=fit,newx=form,s=lmbda,type='link')
# plot(plt, ylab="residuals", xlab="index", main="Plot of residuals")
# abline(0, 0, col= "red")
# # matplot(plt,main="Predicted vs Actual",type='l',lwd=2,col=cols[c(1,2)]),
# #         ylab="Unemplyoment %",
# #         xlab="Time")
# grid()
# 


```


PCA for salary:
```{r}

myPr <- prcomp(salary[, 3:26], scale = TRUE)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0, cex = 0.5)
str(myPr)
#myPr$x #checking principal component scores
salary2 <- cbind(salary, myPr$x[, 1:2])
head(salary2)
#plot with ggplot...
#require(ggplot2)
ggplot(salary2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(salary[, 3:26], salary2[,27:28])

ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 1, varname.adjust = 1) + 
  ggtitle("Biplot for the firms's size") + 
  theme(plot.title = element_text(hjust = 0.5)) 
  
```



# POP: old code


## Unsupervised learning 

General:
```{r Correlation plot with new merged data}
# corrplot(cor(newDat[, 3:20]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", type = "lower", mar=c(0,0,1.5,0))
# corrplot(cor(newDat[, 21:41]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, type = "lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1.5,0))
```

Firms and geo:
```{r}

```

new trial
```{r}
 
# geo styling
#g <- list(
#   scope = 'france',
#   showland = TRUE,
#   landcolor = plotly::toRGB("grey86"),
#   subunitcolor = plotly::toRGB("white"),
#   countrycolor = plotly::toRGB("black"),
#   showlakes = TRUE,
#   lakecolor = plotly::toRGB("white"),
#   showsubunits = TRUE,
#   showcountries = TRUE,
#   resolution = 50,
#   projection = list(
#     type = 'conic conformal',
#     rotation = list(lon = 1)
#   ),
#   lonaxis = list(
#     showgrid = TRUE,
#     gridwidth = 0.5,
#     range = c(-5, 10),
#     dtick = 5
#   ),
#   lataxis = list(
#     showgrid = TRUE,
#     gridwidth = 0.5,
#     range = c(40, 55),
#     dtick = 5
#   )
# )
#  
#  
# p <- plotly::plot_geo(geoNEW, lat = ~latitude, lon = ~longitude) %>%
#   plotly::add_markers(
#     # text = ~paste(town_name, paste("Firms tot:", total), 
#     #               paste("Population:", total_population),
#     #               paste("Mean salary:", sal_general), 
#     #               paste("Salary ratio FvsM:", round(salary_ratio_FvsM, 4)),
#     #               sep = "<br />"),
#     color = ~cluster,  size = I(2), hoverinfo = "text", opacity = 0.8 #, colors = 'Purples'
#   ) %>%
#   plotly::colorbar(title = "Total population<br />in 2014") %>%
#   plotly::layout(title = 'Geographic information about France', geo = g)
# p
 
 
```

## Regression
```{r salary for executives}
# average salary for executives
# average_sal_F_exec <- mean(newDat$sal_F_executive)
# average_sal_M_exec <- mean(newDat$sal_M_executive)
# # -> average salary gap between females and males (executives) are 20.20613 - 25.19371= -4.98758.
# # note that the max variable for sal_M_executive is 58 for female it is 35.5, nearly double. 
# # Strangely, for middle manager male, the max is 93.4 which is in Ile-de-France en Yvelines, Chambourcy. 
# 
# # delta salary female and male
# newDat$sal_MF_exec <- newDat$sal_M_executive - newDat$sal_F_executive
# 
# # linear regression
# lm=lm(sal_MF_exec~ univ + nodip + ,  data=newDat, subset=set.seed(2018))
# summary(lm)



```

Practise what has been done thoughout class.
```{r PCA population-Create clean dataset}
# # PCA trial 
# # create a clean dataset with only necessary variables inside.
# pca_pop_town <- ddply(population, .(CODGEO), function(population) {
#   data.frame(population$sex_ratio, population$dependency_ratio, population$aged_dependency_ratio, population$child_dependency_ratio)})
# 
# # rename the variables 
# names(pca_pop_town)[1:ncol(pca_pop_town)] <-
#   c("CODGEO",
#     "sex_ratio",
#     "dependency_ratio",
#     "aged_dep_ratio",
#     "child_dep_ratio")
# 
# summary(pca_pop_town)
```


```{r PCA population }
# PCA population
# set.seed(3)
# subs = sample(1:nrow(pca_pop_town), size = round(nrow(pca_pop_town)*0.2))
# summary(pca_pop_town)
# 
# pcapop <- prcomp(pca_pop_town[subs, 2:5])
# # pcapop$rotation
# 
# plot(pcapop, type = "l")
# biplot(pcapop, scale = 0, cex = 0.5)
# # fviz_pca_ind(pcapop) + labs(title="PCA", x="PC1", y="PC2")
# # scale_color_gradient2(low="blue", mid="white",high="red", midpoint=4)
# # fviz_pca_ind(pcapop, col.ind="contrib") + scale_color_gradient2(low="blue", mid="white", high="red", midpoint=4) + theme_minimal()
# str(pcapop)
# 
# ggbiplot(pcapop, obs.scale = 1, var.scale = 1, varname.size = 4.5, varname.adjust = 1) +
#   ggtitle("Biplot for dependency ratios") + 
#   theme(plot.title = element_text(hjust = 0.5))
# 
# #pcapop$x #checking principal component scores
# pca_pop_town2 <- cbind(pca_pop_town[subs,], pcapop$x[, 1:2])
# # head(pca_pop_town2)
```

```{r Plot PCA}
#plot with ggplot...
# ggplot(pca_pop_town2, aes(PC1, PC2)) +
#     stat_ellipse(geom = "polygon", col = "gray", alpha = 0.5) +
#     geom_point(shape = 21, col = "black")
```

```{r Proportion of variance explained & cumulative PVE}
# Proportion of variance explained & cumulative PVE
# pcapop$sdev
# pr.var=pcapop$sdev^2 
# pve=pr.var/sum(pr.var)
# pve
# 
# plot(pve, xlab="PCA", ylab="PVE", ylim=c(0,1), type='b')
# plot(cumsum(pve), xlab="PCA", ylab="Cumulative PVE", ylim=c(0,1), type='b')
```

```{r Kmean}
# kmean trial
# km.out=kmeans(pca_pop_town[subs,2:5], 5, nstart=50)
# plot(pca_pop_town[subs,2:5], col=(km.out$cluster), pch=20, cex=1)
```

```{r H-cluster}
#hierarchical clustering
# sd.data=scale(pca_pop_town)
# par(mfrow=c(1,3))
# data.dist=dist(sd.data)
# plot(hclust(data.dist), labels=pca_pop_town$CODGEO, main="complete", xlab="", sub="", ylab="")

# hc_pop=hclust(dist(pca_pop_town), method="complete")
# plot(hc_pop, labels=pca_pop_town$CODGEO, main="Complete Linkage", xlab="", ylab="", sub="", cex =.9)
```


### section: old ANOVA


Now a best subset selction approach is used, in order
to identify the best subset of variables.
Such approach however is not satisfactory, because
it doesn't consider groups as a whole.
It is used only for illustrative purposes.

This function is used to plot some results in the next chunk.
```{r}

plot.regsubsets2 <-  
  function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
     "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
  { 
    obj <- x 
    lsum <- summary(obj) 
    par(mar = c(7, 5, 6, 3) + 0.1) 
    nmodels <- length(lsum$rsq) 
    np <- obj$np 
    propscale <- FALSE 
    sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
                     nomatch = 0) 
    if (sscale == 0)  
      stop(paste("Unrecognised scale=", scale)) 
    if (propscale)  
      stop(paste("Proportional scaling only for probabilities")) 
    yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
    up <- switch(sscale, -1, -1, 1, 1) 
    index <- order(yscale * up) 
    colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
                                                           1e-04)), -log(pmax(yscale, 1e-04))) 
    image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
                       NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
          x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
          col = col) 
    laspar <- par("las") 
    on.exit(par(las = laspar)) 
    par(las = 2) 
    axis(1, at = 1:np, labels = labels, ...) # I modified this line 
    axis(2, at = 1:nmodels, labels = signif(yscale[index], 2), ...) 
    # axis(2,cex.axis=0.01)
    if (!is.null(main))  
      title(main = main) 
    box() 
    invisible(NULL)
  } 

```

Best subset selection 
```{r salary ANOVA BSS}

# BSS
best.sub = regsubsets((sal_y ~ sex + age + job + sex:age + sex:job),
                      data = data_ANOVA, nvmax = 12)
best.sub.summary = summary(best.sub)
# plots: manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
# mtext("My 'Title' in a strange place", line=15)
mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-2.5)
# existing plots
par(mfrow=c(1,1))
plot.regsubsets2(best.sub, scale = "r2", cex.axis = 0.8)
mtext("R2 best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5)
plot.regsubsets2(best.sub, scale = "Cp", cex.axis = 0.8)
mtext("Cp best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5)
plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.8)
mtext("BIC best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5)
plot.regsubsets2(best.sub, scale = "adjr2", cex.axis = 0.8)
mtext("Adj-R2 subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5)
# original plots (without title and fixed cex)
# plot(best.sub, scale = "r2")
# plot(best.sub, scale = "bic")
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.8)
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5)
# plot(best.sub, scale = "Cp")
# plot(best.sub, scale = "adjr2")
# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))


```