---
title: "TSL Project"
author: "Luca Insolia, Jisu Kim and Gevorg Yeghikyan"    
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_notebook:
    toc: yes
    toc_depth: '3' 
    highlight: tango
    keep_tex: yes
    number_sections: yes
  html_document:
    theme: united
    highlight: tango    
    toc: yes
    toc_depth: '3' 
#   pdf_document: # ADD PDF!
#     toc: yes
#     toc_depth: '3'
#     keep_tex: true
#     latex_engine: pdflatex
# editor_options:
#   # toc: yes
#   # toc_depth: 3
#   chunk_output_type: inline
---


# General information 

We analyze a dataset published on [Kaggle](https://www.kaggle.com/etiennelq/french-employment-by-town).
It refers to french employment, salaries, population per town. 
The aim is to evaluate equality/inequalities in France, and geographical distribution of business according to their size.

Such data are collected by the INSEE.
Information regarding the number of firms in every french town, categorized by size
can be found [here](https://www.insee.fr/fr/metadonnees/definition/c1135). This dataset contains about 35000 units/per town.

Information about salaries around french town per job categories, age and sex (expressed in average net amount per hour in euro) can be found [here](https://www.insee.fr/fr/statistiques/2522515). This dataset contains about 5000 units/per town. 

Demographic information in France per town, age, sex and living mode
can be found [here](https://www.insee.fr/fr/statistiques/2863607). This dataset contains about 8 million units/per town. Additional info about Population Data can be found [here](https://www.insee.fr/fr/statistiques/2863607#dictionnaire). 

These datasets have been pre-processed and put together. The final dataset contains 58 variables and 5022 observations. 


## Aim of the study 

This project aims to explore structure of French labour market. 
In particular, we are interested in:

* evaluating possible inequalities: per towns/region, sex, age, job categories etc.;
* discover geographical distribution of business according to their size
* predicting the ... using a regression model;
* reduce the dimensionality of ... performing a PCA;
* explore different algorithms to cluster male/females using ...


## Plan for the study 

1. Unsupervised learning: 
* PCA
* Clustering methods (K-means/Hierarchical)
2. Supervised learning:
* Linear/Quadratic Discriminant Analysis
* KNN
* Cross-validation
* Bootstrap
* Subset selection 
* Shrinkage methods
* Dimension reduction methods
3. Description of population demographics in France
4. Structure of the french labour market
5. ...
6. Future works


# General pre-processing phase 

Loading all libraries needed throughout the notebook:
```{r loading libraries}

# data manipulation
if(!require(plyr)){install.packages("dplyr"); library(plyr)}
if(!require(dplyr)){install.packages("dplyr"); library(dplyr)}
# plotting
if(!require(ggplot2)){install.packages("ggplot2"); library(ggplot2)}
if(!require(ggfortify)){install.packages("ggplot2"); library(ggfortify)}
if(!require(ggmap)){install.packages("ggmap"); library(ggmap)}
if(!require(ggbiplot)){
  if(!require(devtools)){
    install.packages("devtools")}
  install_github("vqv/ggbiplot")
  library(ggbiplot)}
if(!require(corrplot)){install.packages("corrplot"); library(corrplot)}
if(!require(zoo)){install.packages("zoo"); library(zoo)}
if(!require(plotly)){install.packages("plotly"); library(plotly)} 
if(!require(lattice)){install.packages("lattice"); library(lattice)}
if(!require(MVA)){install.packages("MVA"); library(MVA)}
if(!require(ape)){install.packages("ape"); library(ape)}
if(!require(KernSmooth)){install.packages("KernSmooth"); library(KernSmooth)}


# interactive plotting
# Mclust clustering
if(!require(mclust)){install.packages("mclust"); library(mclust)}
# plot design matrix
if(!require(rafalib)){install.packages("rafalib"); library(rafalib)}
# cross validation
if(!require(boot)){install.packages("boot"); library(boot)}
# elastic net
if(!require(glmnet)){install.packages("glmnet"); library(glmnet)}
# group Lasso
if(!require(gglasso)){install.packages("gglasso"); library(gglasso)}
# robust fit
if(!require(robustbase)){install.packages("robustbase"); library(robustbase)}
# box-cox tranformation (in ANOVA) and standardized residuals
if(!require(MASS)){install.packages("MASS"); library(MASS)}
# best subset selection
if(!require(leaps)){install.packages("leaps"); library(leaps)}

# plot for PCA
# if(!require(factoextra)){install.packages("factoextra"); library(factoextra)}

```


Import the four main datasets:
```{r load datasets, warning=FALSE}

setwd("./data")
firms       <- read.csv("base_etablissement_par_tranche_effectif.csv", encoding = "UTF-8")
geo         <- read.csv("name_geographic_information.csv", encoding = "UTF-8")
salary      <- read.csv("net_salary_per_town_categories.csv", encoding = "UTF-8")
population  <- read.csv("population.csv", encoding = "UTF-8")

```

Check variables' names:
```{r original names}

names(firms) 
names(population)
names(salary)
names(geo)

```


Assign meaningful names and drop some variables which are not needed:
```{r assign names}

names(firms)[2:ncol(firms)] <-
  c("town", 
    "regNum",
    "deptNum",
    "total",
    "null",
    "firmsEmpl_1_5",
    "firmsEmpl_6_9",
    "firmsEmpl_10_19",
    "firmsEmpl_20_49",
    "firmsEmpl_50_99",
    "firmsEmpl_100_199",
    "firmsEmpl_200_499",
    "firmsEmpl_500plus")

names(salary)[2:ncol(salary)] <-
  c("town",
    "sal_general",    
    "sal_executive",
    "sal_midManager",
    "sal_employee",
    "sal_worker",
    "sal_Females",
    "sal_F_executive",
    "sal_F_midManager",
    "sal_F_employee",
    "sal_F_worker",
    "sal_Males",
    "sal_M_executive",
    "sal_M_midManager",
    "sal_M_employee",
    "sal_M_worker",
    "sal_18_25",
    "sal_26_50",
    "sal_51plus",
    "sal_F_18_25",
    "sal_F_26_50",
    "sal_F_51plus",
    "sal_M_18_25",
    "sal_M_26_50",
    "sal_M_51plus")

names(population)[5:7] <-
  c("ageCateg5",
    "sex",
    "peopleCategNum")

names(geo)[2:11] =
  c("code_region",
    "region", 
    "region_capital",
    "number_depart",
    "department", 
    "prefecture",
    "circons",
    "town_name", 
    "postal_code", 
    "CODGEO")

names(geo)[14] =
  c("eloignement")

# Drop unnecessary columns (code/num and name represents same thing) 
geo <- subset(geo, select = -c(EU_circo, code_region, number_depart, prefecture, circons, eloignement))

```

Check new variables' names:
```{r new names}

names(firms) 
names(population)
names(salary)
names(geo)

```


## Additional notes 

* Keep Department number?

* Eloignement is the distance from where?



# Analyze firms data 

## Pre-processing:

Categorize firms' size according to [EU standard](http://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Enterprise_size), but in a slightly different form for medium and large firms (i.e., medium firms have <200 instead of <250 employees):

```{r pre processing firms}

# preliminary checks
dim(firms)
names(firms)
head(firms)
str(firms)
summary(firms)

# No duplicated data
sum(duplicated.data.frame(firms))

# merge variables
firms$micro   <- firms$firmsEmpl_1_5 + firms$firmsEmpl_6_9
firms$small   <- firms$firmsEmpl_10_19 + firms$firmsEmpl_20_49
firms$medium  <- firms$firmsEmpl_50_99 + firms$firmsEmpl_100_199
firms$large   <- firms$firmsEmpl_200_499 + firms$firmsEmpl_500plus

# Drop unnecessary (at the moment) columns 
firms <- subset(firms, select = c(CODGEO, town, total, micro, small, medium, large, null))

# check
head(firms)
summary(firms)
str(firms)

```

## Descriptive statistics 

Check the distribution of the null firms (i.e., unknown sizes) and analyze
firms' distribution per town:
  
```{r firms distribution}

# there is an obs with more than 316K null data: we check if it is plausible
# get the highest 20 null values
str_firms <- sort(firms$null, decreasing = T)[1:20]
# get their indexes
str_firms_ind <- match(str_firms, firms$null)
# get the corresponding city
firms$town[str_firms_ind]
# it is about the largest cities, hence it seems reasonable..

# check the ratio of null firms for each town
summary(firms$null/firms$total)
# a lot of information is missing
# should we remove these data?
hist(firms$null/firms$total)
ggplot(data=firms, aes(firms$null/firms$total)) +
  geom_histogram(aes(y =..count..), col="black", fill="blue", alpha = .3, bins = 50) +
  labs(x="Null firms/total firms", y="Count") +
  ggtitle("Ratio between the number of null firms and total firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# evaluate the distribution of all the sizes 
ggplot(data=firms, aes(log(firms$total))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms (log scale)", y="Density") +
  ggtitle("Total number of firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

ggplot(data=firms, aes(log(firms$null))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of unknown size (log scale)", y="Density") +
  ggtitle("Number of firms of unknown size per town") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data=firms, aes(log(firms$micro))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of micro size (log scale)", y="Density") +
  ggtitle("Number of firms of micro size per town") +
  theme(plot.title = element_text(hjust = 0.5))



ggplot(data=firms, aes(log(firms$small))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of small size (log scale)", y="Density") +
  ggtitle("Number of firms of small size per town") +
  theme(plot.title = element_text(hjust = 0.5))


ggplot(data=firms, aes(log(firms$medium))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of medium size (log scale)", y="Density") +
  ggtitle("Number of firms of medium size per town") +
  theme(plot.title = element_text(hjust = 0.5))


ggplot(data=firms, aes(log(firms$large))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of large size (log scale)", y="Density") +
  ggtitle("Number of firms of large size per town") +
  theme(plot.title = element_text(hjust = 0.5))

```

## PCA 

PCA on firms data:

```{r PCA}

# pairs(firms[, 3:8])
# 
# salary_NEW = salary[, 3:ncol(salary)]
# # colnames(salary_NEW) = 1:32
# corrplot(cor(salary_NEW), method = "circle", title = "Correlation matrix for salary", 
#          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1,0)) 
# 
# 
# # firms_clean <- firms[firms$micro < 20000 & firms$large < 200,]
# firms_clean <- firms[firms$micro > 1 & firms$micro < 2500 & 
#                      firms$small > 1 & firms$small < 800 & 
#                      firms$medium > 1 & firms$medium < 500 & 
#                      firms$large > 1 & firms$large < 200 &
#                      firms$null > 1 & firms$null < 2000 ,]
# myPr <- prcomp(firms_clean[, 4:8], scale = TRUE)
# #plot(scale(firms_clean$micro), scale(firms_clean$large))
# #mean(firms_clean$micro)
# #mean(firms_clean$large)
# myPr
# summary(myPr)
# plot(myPr, type = "l")
# # biplot(myPr, scale = 0)
# #extract PC scores...
# str(myPr)
# #myPr$x #checking principal component scores
# firms2 <- cbind(firms_clean, myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])
# 
# # using ggbiplot
# ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
#   ggtitle("Biplot for the firms's size") +
#   theme(plot.title = element_text(hjust = 0.5))
```

Modified PCA [Luca]:
```{r PCA 2}

# get Paris in order to exclude it in the following
Paris = which.max(firms$total)

# Original scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], gap=0, main = "Scatter matrix for firms")

# Correlation matrix
corrplot(cor(firms[-Paris, 3:8]), method = "number", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0))

# Log scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], log = "xy", gap=0, main = "Scatter matrix for firms in log scale")

# Correlation matrix
firmsLog = log(firms[, 3:8]) 
firmsLog[firmsLog == -Inf] = 0
corrplot(cor(firmsLog), method = "number", title = "Correlation matrix for firms  in log scale", 
          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
          tl.col = "black", mar=c(0,0,1.5,0))

# PCA
myPr <- prcomp(firms[-Paris, 3:8], scale = TRUE)
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0)
#extract PC scores...
str(myPr)
#myPr$x #checking principal component scores
# firms2 <- cbind(firms[-Paris, 3:8], myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])

# using ggbiplot
ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
  ggtitle("Biplot for the firms's size") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Cluster Analysis 

```{r clustering}
# firms_sampled <- firms[1:1000, ] # subsampling
# head(firms_sampled)
# firms_scaled <- scale(firms_sampled[, 3:8]) #scaling the data
# 
# head(firms_scaled)
# 
# firms_truncated <- firms_sampled[, 4:8]
# head(firms_truncated)
# plot(firms_sampled)
# # K-means clustering...
# 
# 
# fitK_scaled <- kmeans(firms_scaled, 4) 
# head(fitK_scaled)
# 
# fitK <- kmeans(firms_truncated, 5)
# head(fitK)
# str(fitK)
# plot(firms_sampled, col = fitK$cluster) #plotting data colored according to cluster membership
# 
# #choosing K---
# k <- list()
# for(i in 1:10){
#   k[[i]] <- kmeans(firms_truncated, i)
# }
# head(k)
# 
# betweenss_totalss <- list()
# for(i in 1:10){
#   betweenss_totalss[[i]] <- k[[i]]$betweenss/k[[i]]$totss
# }
# plot(1:10, betweenss_totalss, type ="b",
#      ylab = "Between SS / Total SS", xlab = "Clusters (k)") #calculating and plotting between SS to total SS ratio against number of clusters
# 
# for(i in 1:5) {
#   plot(firms, col = k[[i]]$cluster) #plotting data based on membership to clusters for k = 1 to 5 clusters
# }
# head(fitM)
# fitM <- Mclust(firms_truncated)
# plot(fitM)
# #Model-based clustering using mclust
# 
# head(clusters)
# plot(firms_sampled, col = clusters) # as we can see, it performs quite similar to the K-means 
# rect.hclust(fitH, k = 5, border = "red") # visualising dendrogam cut at k =5
# clusters <- cutree(fitH, 5)  # vector with cluster membership for each observation
# plot(fitH)
# d <- dist(firms_truncated)
# fitH <- hclust(d, "ward.D2")
# 
# #Hierarchical clustering---
# 

```


## Issues

* Some towns have 100% of null firms


## How to use these data 

We plan to use these for the following tasks:

* predict the salaries using such information as proxy for the competition in the job market;
* predict the total number of firms, using salary data;
* geo-spatial plot for firms' size 
* ... missing: histogram for firms' size


# Analyze geographical data 

## Pre-processing 

Correct typos for longitude data and keep just the unique CODGEO to avoid towns with multiple postal codes:
```{r pre processing geo}

# preliminary checks
dim(geo)
names(geo)
head(geo)
str(geo)
summary(geo)

# spot "," instead of "." in longitude
newLong       <- as.character(geo$longitude)    # copy the vector
sum(grep(",", newLong))                         # total commas
ind_long_err  <- grep(",", newLong)             # indexing them
newLong       <- gsub(",", ".", newLong)        # substituting them with dots
indNA_Long    <- is.na(as.numeric((newLong)))   # spot NA
geo$longitude[indNA_Long]                       # verify that they were actually missing
geo$longitude <- as.numeric(newLong)            # overwrite the longitude variable with the new one

# Check for duplicated data (e.g., cities with different postal codes, that we dropped):
  # ex. to verify it,  try on the initial dataset:
  # sum(geo$nom_commune == "Paris")
  # ind_duplic <- geo$nom_commune == "Paris"
  # geo[ind_duplic,]
sum(duplicated.data.frame(geo)) 
# retaing unique postal cities
geo <- unique(geo, by = "CODGEO")


# check again
head(geo)
summary(geo)

```


Assign latitude and longitude values for missing data (almost 3000):
```{r assign NA, warning=FALSE}

# index of NAs and their total
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)

# code used to retrieve the NA using Google API, which have been saved in a csv file
# 
# # initialize variables
# city_search = 0
# res = as.data.frame(matrix(c(0, 0, 0), 1, 3))
# names(res) = c("lon", "lat", "address")
# 
# # retrieve lat and long (Google API = 2500 request per day)
# # my_iter = floor(sum(indNA_coord)/3)
# for (i in 1:sum(indNA_coord)){
# 
#   # city searched
#   city_search[i] = paste(c(as.character(NA_coord$town_name[i]), as.character(NA_coord$postal_code[i]), as.character(NA_coord$department[i]), "France"), sep=" ", collapse = ", ")
#   
#   # solution
#   res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
# 
#   # retrieve still missing data, because of existing problems with API (up to 15 trials)
#   j = 0
#   while (any(is.na(res[i,])) & j < 25){
#     res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
#     j = j + 1
#   }
# }

# # check the solution
# sol = cbind(searched = city_search, res)

# # save it as a csv file to save time
# write.csv(retrieved_geo_NA[,2:3], "geo_NA_Final.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
    

# read the created csv
setwd("./data")
retrieved_geo_NA = read.csv("geo_NA_Final.csv", header = T, encoding = "UTF-8")
# get only long and lat and assign to original NA 
geo$latitude[indNA_coord] = retrieved_geo_NA[,2]
geo$longitude[indNA_coord] = retrieved_geo_NA[,1]

# there are 37 still missing units, which are towns located in old colonies far from Europe
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)
# exclude those towns
geo = geo[!indNA_coord,]

summary(geo)

```

## Descriptive statistics 

Plot available towns in a map:

```{r map}

# delete non-European countries
ind_nonEur = geo$latitude < 30 | geo$latitude > 70 |geo$longitude < -20 | geo$longitude > 20
sum(ind_nonEur)
geo = geo[!ind_nonEur,]

# center of France, obtained using:
# fra_center = as.numeric(geocode("France"))
fra_center = c(2.213749, 46.227638)
# plot all European towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.2, size=0.01) 

```



## What we have learned 

Solved: 

* Latitude is missing and not longitude
* There are some duplications
* Exclude non-European


## How to use these data 

* Compare European towns vs. old colonies?
* Useful for all datasets/analyses


# Analyze salary data 

## Pre-processing 

Verification of the dataset:
```{r pre processing salary}

# preliminary checks
dim(salary)
names(salary)
head(salary)
str(salary)
summary(salary)

# Check for duplicated data
sum(duplicated.data.frame(salary))

```



## Descriptive statistics 

Univariate analysis comparing salaries for both genders among various job categories:

```{r boxplots works}

#  number of units
n_sex <- length(salary$sal_Females)
# vector representing males and females
Label <- c(rep("M", n_sex*5), rep("F", n_sex*5))
# vector representing the variable considered
Variable <- c(rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex),
             rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex))
# merge these data
sal_sex = cbind.data.frame(Label = Label, 
             value = c(salary$sal_Males, salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
                       salary$sal_Females, salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker),
             Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  # not color points replacing colour = group instead of colour=Label
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different job positions") +
  theme(plot.title = element_text(hjust = 0.5)) +      stat_boxplot(geom = "errorbar", width = 0.5)
  # + guides(fill=guide_legend(title="Legend"))


# the same but excluding outliers
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  scale_y_continuous(limits = quantile(sal_sex$value, c(0, 0.9))) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + 
  ggtitle("Gender comparison for different job positions excluding the last decile") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


Univariate analysis comparing salaries for both genders among various ages:

```{r boxplots ages}

# vector representing males and females
Label <- c(rep("M", n_sex*3), rep("F", n_sex*3))
# vector representing the variable considered
Variable <- c(rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex),
              rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex))
# merge these data
sal_sex <- cbind.data.frame(Label = Label, 
                           value = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus, 
                                     salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus),
                           Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different ages") +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(c(5, 100)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


The income inequality between genders, age groups and working positions is clear.
In the following analyses the focus is on the salary ratio between women and men among different
job positions:
```{r ratio F vs M accross jobs}

# Gender salary ratio and general level of income

# Overall mean salary: The higher the net mean income, the more skewed the ratio of salary between female and male is. Only 2 towns have a ratio>1
# create overall F vs M ratio
salary$salary_ratio_FvsM <- salary$sal_Females / salary$sal_Males
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Overall salary ratio (females/males)", y="Density") + 
  labs(title = "Overall salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs overall mean salary
ggplot(salary, aes(x= sal_general, y=salary_ratio_FvsM)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Overall salary ratio(females/males)") + 
  labs(title = "Overall salary ratio between females and males vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm") 


# Executives mean salary: a bit better the situation for females in this case and less skewed
# create Executives F vs M ratio
salary$salary_ratio_FvsM_Exec <- salary$sal_F_executive / salary$sal_M_executive
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Executives salary ratio (females/males)", y="Density") + 
  labs(title = "Executives salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_Exec)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Executives salary ratio (females/males)") + 
  labs(title = "Executives salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Middle managers mean salary: ....
# create Middle managers F vs M ratio
salary$salary_ratio_FvsM_midManag <- salary$sal_F_midManager / salary$sal_M_midManager
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Middle managers salary ratio (females/males)", y="Density") + 
  labs(title = "Middle managers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_midManag)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Middle managers salary ratio (females/males)") + 
  labs(title = "Middle managers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Workers mean salary: ...
# create workers F vs M ratio
salary$salary_ratio_FvsM_worker <- salary$sal_F_worker / salary$sal_M_worker
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_worker)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Workers salary ratio (females/males)", y="Density") + 
  labs(title = "Workers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs workers mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_worker)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Workers salary ratio (females/males)") + 
  labs(title = "Workers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Employee mean salary: ...
# create Employee F vs M ratio
salary$salary_ratio_FvsM_employee <- salary$sal_F_employee / salary$sal_M_employee
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_employee)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Employee salary ratio (females/males)", y="Density") + 
  labs(title = "Employee salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs Employee mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_employee)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Employee salary ratio (females/males)") + 
  labs(title = "Employee salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Now the focus is on the salary ratio between women and men among different age groups:
```{r ratio F vs M accross ages}

# 18-25 mean salary: are quite equal apart from some outliers and a quadratic trend
# create 18-25 F vs M ratio
salary$salary_ratio_FvsM_18_25 <- salary$sal_F_18_25 / salary$sal_M_18_25
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_18_25)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="18-25 salary ratio (females/males)", y="Density") + 
  labs(title = "18-25 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 18-25 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")
# scatter plot vs 18-25 mean salary for them
ggplot(salary, aes(x= sal_18_25, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="18-25 salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall 18-25 salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "loess")


# 26-50 mean salary: ...
# create 26-50 F vs M ratio
salary$salary_ratio_FvsM_26_50 <- salary$sal_F_26_50 / salary$sal_M_26_50
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_26_50)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="26-50 salary ratio (females/males)", y="Density") + 
  labs(title = "26-50 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_26_50)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="26-50 salary ratio (females/males)") + 
  labs(title = "26-50 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# 51+ mean salary: ...
# create 51+ F vs M ratio
salary$salary_ratio_FvsM_51plus <- salary$sal_F_51plus / salary$sal_M_51plus
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_51plus)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="51+ salary ratio (females/males)", y="Density") + 
  labs(title = "51+ salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_51plus)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="51+ salary ratio (females/males)") + 
  labs(title = "51+ salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Highlight bivariate relations:
```{r bivariate plots}

# correlation matrix
corrplot(cor(salary[, 3:ncol(salary)]), method = "circle", title = "Correlation matrix for salary data", 
         diag = T, tl.cex=0.5, type="lower", 
         tl.col = "black", mar=c(0,0,1.5,0)) 

# most general pairs
pairs(salary[c(3:8, 13, 18:20)], gap=0, main = "Scatter matrix of the main variables in salary data", cex = 0.6)
# pairs highlighting genders' differences
pairs(salary[c(9:12, 14:17)], gap=0, main = "Scatter matrix of job categories for both genders", cex = 0.6)

```



## Linear models

First we perform some easy task
fitting a regression model to predict the salaries of people in age 26-50 using as regressor 51+ years:

```{r exLinMod, fig.keep='all'}
# # fit and show OLS estimate
# plot(salary$sal_26_50 ~ salary$sal_51plus)
# fit_LM_26_50 = lm(salary$sal_26_50 ~ salary$sal_51plus, data = salary) 
# abline(fit_LM_26_50, lwd=3, col="red")
# # diagnostics
# summary(fit_LM_26_50)
# plot(fit_LM_26_50)
# 
# # Same as before but adding polynomials which are evauated using 10-folds cross validation
# set.seed(1)
# # k-Fold Cross-Validation
# cv.err.K = rep(0, 5)
# cv.err.K = rbind(cv.err.K, cv.err.K)
# for (i in 1:5){
#   fit_LM_26_50.K = glm(sal_26_50 ~ poly(sal_51plus, i), data = salary)
#   cv.err.K[,i] = cv.glm(salary, fit_LM_26_50.K, K = 10)$delta[1]
# }
# # plotting results
# plot(cv.err.K[1,], type = 'l', col = 'red', xlab = "Polynomials' order", 
#      ylab = "10-folds CV", main = "CV and adjusted CV for different polynomials")
# lines(cv.err.K[2,], col = 'green')
# points(which.min(cv.err.K), cv.err.K[1, which.min(cv.err.K)], col = "red", cex=2, pch=20)
# legend('topright', legend = c('CV', 'Adj. CV'), col = c('red', 'green'), pch = 10)

```


Predicting salary for young people using Elastic NEt with CV and 10-folds CV. The predictors are most of the original variables and their 2nd, 3rd order polynomials and log tranformation:

```{r}
# # funtion to plot BSS
# plot.regsubsets2 <-  
#   function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
#                                                            "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
#   { 
#     obj <- x 
#     lsum <- summary(obj) 
#     par(mar = c(7, 5, 6, 3) + 0.1) 
#     nmodels <- length(lsum$rsq) 
#     np <- obj$np 
#     propscale <- FALSE 
#     sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
#                      nomatch = 0) 
#     if (sscale == 0)  
#       stop(paste("Unrecognised scale=", scale)) 
#     if (propscale)  
#       stop(paste("Proportional scaling only for probabilities")) 
#     yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
#     up <- switch(sscale, -1, -1, 1, 1) 
#     index <- order(yscale * up) 
#     colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
#                                                            1e-04)), -log(pmax(yscale, 1e-04))) 
#     image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
#                        NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
#           x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
#           col = col) 
#     laspar <- par("las") 
#     on.exit(par(las = laspar)) 
#     par(las = 2) 
#     axis(1, at = 1:np, labels = labels, ...) # I modified this line 
#     axis(2, at = 1:nmodels, labels = signif(yscale[index], 2)) 
#     if (!is.null(main))  
#       title(main = main) 
#     box() 
#     invisible(NULL) 
#   } 
# 
# set.seed(2018)
# 
# # load the data
# ind = salary$sal_18_25 < 11.5
# y = salary$sal_18_25[ind]
# x = cbind.data.frame(salary$sal_26_50, salary$sal_51plus, 
#                      salary$sal_general,  salary$sal_executive, salary$sal_midManager, 
#                      salary$sal_employee, salary$sal_worker, 
#                      salary$sal_Males, salary$sal_Females)
# x = x[ind, ]
# yx = cbind.data.frame(y, x)
# 
# # original response variable histogram
# ggplot(data=salary, aes(salary$sal_18_25)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # response variable histogram (excluding outliers)
# ggplot(data=salary[ind,], aes(y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # after sampling to avoid serial correlation
# indd = sample(1:length(y), round(length(y)*0.4))
# yx = yx[indd,]
# ggplot(data=salary[ind,][indd,], aes(y[indd])) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 30) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Sampled response variable and excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # correlations
# # corrplot(cor(yx), method = "number", title = "Correlation matrix for salary",
# #          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
# #          tl.col = "black")  # , mar=c(0,0,1.5,0)
# 
# # scatter matrix
# # pairs(yx, gap=0, main = "Scatter matrix for some variables in salary")
# # manually remove outliers in mid mangager
# summary(yx)
# inddd = yx$`salary$sal_midManager` < 20
# yx = yx[inddd,]
# # final dataset
# pairs(yx, gap=0, main = "Scatter matrix for the variables in the model")
# 
# # create final dataframe
# require(leaps) 
# # assign names for predictors transformation
# namesdataBS = names(yx)
# yx = cbind.data.frame(yx, yx[,2:ncol(yx)]^2, yx[,2:ncol(yx)]^3, log(yx[,2:ncol(yx)]))
# dim(yx)
# lll = ((dim(yx)[2]-1)/4) 
# for (i in 1:lll+1){ 
#   str = namesdataBS[i] 
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1] 
# } 
# namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll)) 
# namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll)) 
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll)) 
# names(yx) = namesdataBS 
# 
# # Elastic net
# x = as.matrix(yx[, 2:ncol(yx)])
# y = yx[, 1]
# par(mfrow =c(3,2))
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   cv.out = cv.glmnet(x, y, alpha = j)
#   plot(cv.out)
#   title(paste("alpha = ", j), line = 2.3)
# }
# mtext(expression("Best lambda for salary 18-25 using elastic and 10-folds CV"), outer=TRUE,  cex=1, line=-1.4) 
# 
# 
# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 
# 
# # best subset selection
# best.sub = regsubsets(y ~ ., data = yx, nvmax = ncol(yx)) 
# best.sub.summary = summary(best.sub) 
# # manual plotting 
# par(mfrow =c(2,2))
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
# # built-in plots 
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "adjr2") 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.9) 
# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5) 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# names(coefficients(best.sub, 10))
# # nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
# 


```



ANOVA model for salary:
```{r}
# 
# # create response variable
# sal_y = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus,
#           salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
#           salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus,
#           salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker)
# 
# n_sal_y = length(sal_y)             # length response variable
# n_cat = length(salary$sal_M_18_25)  # length of each category (i.e., original vectors)
# 
# # create sex dummy variable, 1 for males and 0 for females
# sal_sex = rep(0, n_sal_y)   # full regressors
# sal_sex[1:n_sal_y/2] = 1    # assign males
# 
# # create age dummy variables, 18-25 years old is the base case
# sal_age = cbind(rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # 26-50 y.o.
# sal_age[(n_cat+1):(n_cat*2), 1] = 1     # males
# sal_age[(n_cat*8+1):(n_cat*9), 1] = 1   # females
# # 51+ y.o.
# sal_age[(n_cat*2+1):(n_cat*3), 2] = 1   # males
# sal_age[(n_cat*9+1):(n_cat*10), 2] = 1  # females
# 
# # create job type dummy variables, worker is the base case
# sal_job = cbind(rep(0, n_sal_y), rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # executives
# sal_job[(n_cat*3+1):(n_cat*4), 1] = 1     # males
# sal_job[(n_cat*10+1):(n_cat*11), 1] = 1   # females
# # middle managers
# sal_job[(n_cat*4+1):(n_cat*5), 2] = 1     # males
# sal_job[(n_cat*11+1):(n_cat*12), 2] = 1   # females
# # employee
# sal_job[(n_cat*5+1):(n_cat*6), 3] =   1   # males
# sal_job[(n_cat*12+1):(n_cat*13), 3] = 1   # females
# 
# # final data set 
# data_ANOVA = cbind.data.frame(response = sal_y, sex = sal_sex, age = sal_age, job = sal_job)
# names(data_ANOVA)
# # show regressors' shape
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# # sub sample to avoid correlation
# set.seed(20)
# subs = sample(1:n_sal_y, size = round(n_sal_y*0.2))
# data_ANOVA = data_ANOVA[subs,]
# # show randomized data
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# 
# # plot response variable
# hist(data_ANOVA[,1], 30)
# hist(log(data_ANOVA[,1]), 30)
# hist(sqrt(data_ANOVA[,1]), 30)
# hist(data_ANOVA[,1]^-1, 30)
# sal_y = data_ANOVA[,1]^-1  # also suggested by Box-Cox transformation
# ggplot(data=data.frame(data_ANOVA[,1]), aes(sal_y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 40) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Transformation of salary found using Box-Cox transformation") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# sex = sal_sex[subs]
# age = sal_age[subs,]
# job = sal_job[subs,]
# # ANOVA model
# sal_ANOVA = lm(sal_y ~ sex + age + job + sex:age + sex:job)
# # names(sal_ANOVA$coefficients) = c("Males", "26-50", )
# summary(sal_ANOVA)
# anova((sal_ANOVA))
# 
# # box-cox transformation suggested to use y^-1
# boxcox(sal_ANOVA)
# title("Box-Cox transformation of the response")
# 
# # BSS
# best.sub = regsubsets((sal_y ~ sex + age + job + sex:age + sex:job), 
#                       data = data_ANOVA, nvmax = 12) 
# best.sub.summary = summary(best.sub) 
# # plots: manual plotting 
# par(mfrow =c(2,2)) 
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-2.5) 
# # existing plots
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.7) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5) 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "adjr2") 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# 
# 
# # group lasso
# # install.packages("gglasso")
# library(gglasso)
# # install.packages("RColorBrewer")
# library(RColorBrewer)
# # install.packages("zoo")
# library(zoo)
# grp = c(1,2,2,3,3,3,4,4,5,5,5)
# form <- model.matrix(sal_y ~ (sex + age + job)^2)
# head(form)
# form = form[,2:12]
# fit = gglasso(x=form,y=sal_y,group=grp,loss='ls')
# coef.mat=fit$beta
# 
# #Group1 enters the equation
# g1=max(which(coef.mat[1,]==0))
# #Group2 enters the equation
# g2=max(which(coef.mat[2,]==0))
# #Group3 enters the equation
# g3=max(which(coef.mat[4,]==0))
# #Group4 enters the equation
# g4=max(which(coef.mat[7,]==0))
# #Group5 enters the equation
# g5=max(which(coef.mat[9,]==0))
# #Coefficient Plot
# cols=brewer.pal(5,name="Set1")
# plot(fit$b0,main="Coefficient vs Step",
#      ylab="Intercept",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      xlim=c(-1,100),
#      ylim=c(0.076,max(fit$b0)+0.001),
#      type="l",lwd=4)
# grid()
# par(new=T)
# xx=c(g1,g2,g3,g4,g5)
# yy=c(fit$b0[g1],fit$b0[g2],fit$b0[g3],fit$b0[g4],fit$b0[g5])
# plot(x=xx,y=yy,pch=13,lwd=2,cex=2,col=cols[-1],
#      xlim=c(-1,100),ylim=c(0.076,max(fit$b0)+0.001),
#      xaxt='n',yaxt='n',xlab="",ylab="")
# lmda=round(fit$lambda[c(g1,g2,g3,g4,g5)],4)
# text(x=xx-0.005,y=yy+0.0001,labels=c("Group1","Group2","Group3","Group4","Group5","Group6"),pos=3,cex=0.7)
# text(x=xx-0.005,y=yy-0.0001,labels=paste("Lambda\n=",lmda),pos=1,cex=0.6)
# 
# # coefficient plot 2 (my version)
# cols=brewer.pal(5,name="Set1")
# plot(fit$beta[1,], main="Coefficients vs Lambda",
#      ylab="Coefficients",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      # xlim=c(-1,100),
#      ylim=c(min(fit$beta), max(fit$beta)),
#      type="l",lwd=4)
# for (j in 2:11){
#   lines(fit$beta[j,],
#         type="l",lwd=4, col=cols[j])
# }
# # plot legend once
# grid()
# par(new=T)
# legend('bottomleft', legend = paste("group ", 1:5), lty=1, col=cols[1:5], cex = 0.7,lwd=2)
# 
# #Cross Validation
# fit.cv=cv.gglasso(x=form,y=sal_y,group=grp,nfolds=10)
# plot(fit.cv, main="10-folds CV for groupwise Lasso in ANOVA")
# #Pick the best Lambda
# lmbda=fit.cv$lambda.1se
# (coefs=coef.gglasso(object=fit,s=lmbda))
# #At best lambda get coefficients and fitted values
# plt=sal_y-predict.gglasso(object=fit,newx=form,s=lmbda,type='link')
# plot(plt, ylab="residuals", xlab="index", main="Plot of residuals")
# abline(0, 0, col= "red")
# # matplot(plt,main="Predicted vs Actual",type='l',lwd=2,col=cols[c(1,2)]),
# #         ylab="Unemplyoment %",
# #         xlab="Time")
# grid()
# 


```


PCA for salary:
```{r}

myPr <- prcomp(salary[, 3:26], scale = TRUE)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0, cex = 0.5)
str(myPr)
#myPr$x #checking principal component scores
salary2 <- cbind(salary, myPr$x[, 1:2])
head(salary2)
#plot with ggplot...
#require(ggplot2)
ggplot(salary2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(salary[, 3:26], salary2[,27:28])

ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 1, varname.adjust = 1) + 
  ggtitle("Biplot for the firms's size") + 
  theme(plot.title = element_text(hjust = 0.5)) 
  
```



## What we have learned 

* Check the outlier in sal_18_25: which city is it, etc.
* evaluate possible multicollinearity
* Try a regression with all predictors ans lasso


## How to use these data 

* ...

# Analyze population data 

## Pre-processing 

Rename the variables for population and exclude the unnecessary ones:
```{r Pre processing population}

# preliminary checks
names(population)
summary(population)

# drop unnecessary columns (NIVGEO is the same for all)
population <- subset(population, select = -c(NIVGEO, LIBGEO))

# Refactor sex and MOCO
population$MOCO <- factor(population$MOCO, levels = c(11,12,21,22,23,31,32),
                          labels = c("children_living_with_two_parents", 
                                     "children_living_with_one_parent",
                                     "adults_living_in_couple_without_child",
                                     "adults_living_in_couple_with_children",
                                     "adults_living_alone_with_children",
                                     "persons_not_from_family_living_in_the_home",
                                     "persons_living_alone"))
population$sex <- factor(population$sex, levels = c(1,2), labels = c("Male", "Female"))

# check again
head(population)
summary(population)

# total number of population= 65mil
sum(population$total_population)

```

Understand the distribution of population according to different age categories using population pyramide:
```{r Population pyramide}

# need the initial shape of data to construct the pyramide 
# Population pyramide
population_data2 <- ddply(population, .(sex, ageCateg5), function(population) {
  data.frame(total_population = sum(population$peopleCategNum))
  })
pop_pyramid <- ggplot(data = population_data2,
       mapping = aes(x = ageCateg5, fill = sex,
                     y = ifelse(test = sex == "Male",
                                yes = -total_population, no = total_population))) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(population_data2$total_population) * c(-1,1)) + 
  ggtitle("Pyramid of Population") + theme(plot.title = element_text(hjust = 0.5)) + 
  labs(x= "Age")+ labs(y = "Population") + coord_flip() # + scale_fill_brewer(palette = "Set1")
pop_pyramid

```

Re-organize population dataset and set CODGEO as units and create new variables using the available variables.
```{r Re-organize population dataset by CODGEO}

# Re-organize the data by creating new variables: "total population", "male", "female", "child", "elderly" and "workforce".
population <- ddply(population, .(CODGEO), function(population) {
  data.frame(total_population = sum(population$peopleCategNum),
             male = sum(population[population$sex == "Male",]$peopleCategNum),
             female = sum(population[population$sex == "Female",]$peopleCategNum),
             child = sum(population[population$ageCateg5 %in% seq(0, 10, by=5),]$peopleCategNum),
             elderly = sum(population[population$ageCateg5 %in% seq(65, 80, by=5),]$peopleCategNum),
             workforce = sum(population[population$ageCateg5 %in% seq(15, 60, by=5),]$peopleCategNum) 
  )})

# Calculate ratios using the existing variables
population$dependent <- population$child + population$elderly
population$sex_ratio <- ifelse(population$female==0, 0, population$male / population$female)
population$dependency_ratio <- ifelse(population$workforce==0, 0, population$dependent / population$workforce)
population$aged_dependency_ratio <- ifelse(population$workforce==0, 0, population$elderly / population$workforce)
population$child_dependency_ratio <- ifelse(population$workforce==0, 0, population$child / population$workforce)

```

## Descriptive statistics 

Plot population data across different towns.
```{r plotting some population data in log10}

# Histogram of total population per town in log
ggplot(data=population, aes(ifelse(total_population!=0, log10(total_population), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 total population", y="Density") +
  ggtitle("Histogram of total population per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of dependency ratio per town
ggplot(data=population, aes(ifelse(dependency_ratio!=0, log10(dependency_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 dependency ratio", y="Density") +
  ggtitle("Histogram of dependency ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of sex ratio per town
ggplot(data=population, aes(ifelse(sex_ratio!=0, log10(sex_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 sex ratio", y="Density") +
  ggtitle("Histogram of sex ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

```

Understand which towns have the highest concentration of population and ratios.
```{r geo plot for each town}

# Merge geography and population data
geo_population <- merge(geo, population, by="CODGEO")

# Try to cll google API until there is no error
# 
# FraMap = NA
# while (any(is.na(FraMap))){
#   FraMap = tryCatch({
#       ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal")
#   }, error = function(e) {
#     message("cannot open URL")  
#     FraMap = NA
#   })
# }

# France map
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")

# Plot "Distribution of total population for each town" 
sc <- scale_colour_gradientn(colours =palette(rainbow(3)), limits=c(min(geo_population$total_population), max(geo_population$total_population)))
population_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude, colour=geo_population$total_population),
             data=geo_population, alpha=0.2, size=0.01) +
  sc + labs(color='') + ggtitle("Distribution of Population for each town")
population_distribution

# Plot "Distribution of aged dependency ratio for each town"
sc <- scale_colour_gradientn(colours =palette(rainbow(3)), limits=c(min(geo_population$aged_dependency_ratio), max(geo_population$aged_dependency_ratio)))
aged_ratio_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude, colour=geo_population$aged_dependency_ratio),
             data=geo_population, alpha=0.2, size=0.01) +
  sc + labs(color='') + ggtitle("Aged dependency ratio per town")
aged_ratio_distribution


```


## Unsupervised learning 

Practise what has been done thoughout class.
```{r PCA population-Create clean dataset}
# # PCA trial 
# # create a clean dataset with only necessary variables inside.
# pca_pop_town <- ddply(population, .(CODGEO), function(population) {
#   data.frame(population$sex_ratio, population$dependency_ratio, population$aged_dependency_ratio, population$child_dependency_ratio)})
# 
# # rename the variables 
# names(pca_pop_town)[1:ncol(pca_pop_town)] <-
#   c("CODGEO",
#     "sex_ratio",
#     "dependency_ratio",
#     "aged_dep_ratio",
#     "child_dep_ratio")
# 
# summary(pca_pop_town)
```


```{r PCA population }
# PCA population
# set.seed(3)
# subs = sample(1:nrow(pca_pop_town), size = round(nrow(pca_pop_town)*0.2))
# summary(pca_pop_town)
# 
# pcapop <- prcomp(pca_pop_town[subs, 2:5])
# # pcapop$rotation
# 
# plot(pcapop, type = "l")
# biplot(pcapop, scale = 0, cex = 0.5)
# # fviz_pca_ind(pcapop) + labs(title="PCA", x="PC1", y="PC2")
# # scale_color_gradient2(low="blue", mid="white",high="red", midpoint=4)
# # fviz_pca_ind(pcapop, col.ind="contrib") + scale_color_gradient2(low="blue", mid="white", high="red", midpoint=4) + theme_minimal()
# str(pcapop)
# 
# ggbiplot(pcapop, obs.scale = 1, var.scale = 1, varname.size = 4.5, varname.adjust = 1) +
#   ggtitle("Biplot for dependency ratios") + 
#   theme(plot.title = element_text(hjust = 0.5))
# 
# #pcapop$x #checking principal component scores
# pca_pop_town2 <- cbind(pca_pop_town[subs,], pcapop$x[, 1:2])
# # head(pca_pop_town2)
```

```{r Plot PCA}
#plot with ggplot...
# ggplot(pca_pop_town2, aes(PC1, PC2)) +
#     stat_ellipse(geom = "polygon", col = "gray", alpha = 0.5) +
#     geom_point(shape = 21, col = "black")
```

```{r Proportion of variance explained & cumulative PVE}
# Proportion of variance explained & cumulative PVE
# pcapop$sdev
# pr.var=pcapop$sdev^2 
# pve=pr.var/sum(pr.var)
# pve
# 
# plot(pve, xlab="PCA", ylab="PVE", ylim=c(0,1), type='b')
# plot(cumsum(pve), xlab="PCA", ylab="Cumulative PVE", ylim=c(0,1), type='b')
```

```{r Kmean}
# kmean trial
# km.out=kmeans(pca_pop_town[subs,2:5], 5, nstart=50)
# plot(pca_pop_town[subs,2:5], col=(km.out$cluster), pch=20, cex=1)
```

```{r H-cluster}
#hierarchical clustering
# sd.data=scale(pca_pop_town)
# par(mfrow=c(1,3))
# data.dist=dist(sd.data)
# plot(hclust(data.dist), labels=pca_pop_town$CODGEO, main="complete", xlab="", sub="", ylab="")

# hc_pop=hclust(dist(pca_pop_town), method="complete")
# plot(hc_pop, labels=pca_pop_town$CODGEO, main="Complete Linkage", xlab="", ylab="", sub="", cex =.9)
```

## What we have learned 
...

## How to use these data 
...

# Produce consistent datasets

The CODGEO variables (code_insee in the original geo data) have to be merged.
However, for different reasons already identified by other kaggle users they need some pre-processing.
To do so, some already known mistakes are corrected:

```{r fix and match CODGEO}

firms$CODGEO      <- sub("A", "0", firms$CODGEO)
firms$CODGEO      <- sub("B", "0", firms$CODGEO)
salary$CODGEO     <- sub("A", "0", salary$CODGEO)
salary$CODGEO     <- sub("B", "0", salary$CODGEO)
population$CODGEO <- sub("A", "0", population$CODGEO)
population$CODGEO <- sub("B", "0", population$CODGEO)
geo$CODGEO        <- sub("A", "0", geo$CODGEO)
geo$CODGEO        <- sub("B", "0", geo$CODGEO)


# Then all CODGEO are trasformed to integers and four new datasets are created retaining only the common CODGEO:
# use only integer values
geo$CODGEO = as.integer(geo$CODGEO)  
population$CODGEO = as.integer(population$CODGEO)
firms$CODGEO = as.integer(firms$CODGEO)
salary$CODGEO = as.integer(salary$CODGEO)

# store datasets' names to loop on them
dataset = c("population", "salary", "firms", "geo")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and create a new variable concateneting "NEW" at the end
  nam <- paste(i, "NEW", sep = "")
  # initialize counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # for each dataset different from the i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(i))
  del_obs = paste("For", i, del_rows, "units have been deleted.",
                  "They were the", round(del_prop*100, digits=2), "% of the total.", sep = " ")
  print(paste(del_obs))
}

print(paste("The new dataset has", nrow(salaryNEW), "units and", 
      ncol(salaryNEW)+ncol(populationNEW)+ncol(firmsNEW)+ncol(geoNEW), "features."))
```


## merge the data:
```{r Merge the data and create csv file}

# exclude same CODGEO and different postal code in geo data
# skip cities with multiple postal code
sum(duplicated.data.frame(as.data.frame(geoNEW$CODGEO)))
# retaing unique postal cities
geoNEW <- geoNEW[!duplicated.data.frame(as.data.frame(geoNEW$CODGEO)),]

# merging
newDat = merge(firmsNEW, populationNEW, by="CODGEO")
newDat = merge(newDat, salaryNEW, by="CODGEO")
newDat = merge(newDat, geoNEW, by="CODGEO")
# newDat = subset(newDat, select = -town.y)

# check
names(newDat)
head(newDat)

# NewDat csv file created
# write.csv(newDat, "newDat.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
```

Spatial plot of the data to spot possible patterns:
....
```{r}

```



# Analysis 

General:
```{r Correlation plot with new merged data}
# corrplot(cor(newDat[, 3:20]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", type = "lower", mar=c(0,0,1.5,0))
# corrplot(cor(newDat[, 21:41]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, type = "lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1.5,0))
```


Firms and geo:
```{r}

```
## PCA 
```{r}

```
## Regression
```{r}




```
## Clustering 
```{r}

```

## Supervised Learning data

# Import additional dataset

Import additional dataset needed:
```{r Load additional datasets, warning=FALSE}

# loading data
setwd("./data/Add_data")
educ        <- read.csv("level_education.csv", sep =";")
categ_socio <- read.csv("Categorie_socioprofessionnelle.csv", sep =";")
status_work <- read.csv("Emplois_lieu_travail.csv", sep =";")
ineq        <- read.csv("Comparateur_territoires.csv", sep =";")

```

# Pre-processing
Check variables' names:
```{r Check names}

# names(educ)
# names(categ_socio)
# names(ineq)
# names(status_work)

```

To better understand the data, assign meaningful names and drop unnecessary variables (at the moment):
```{r Educ-rename and drop variables}

# drop unnecessary variable 
educ <- subset(educ, select= -c(LIBGEO))

# rename variables
names(educ)[2:ncol(educ)] <-
  c("Age15_NoDip_M","Age15_NoDip_F", "Age15_Sec_M", "Age15_Sec_F", "Age15_Hi_M","Age15_Hi_F", "Age15_Univ_M", "Age15_Univ_F", "Age20_NoDip_M", "Age20_NoDip_F", "Age20_Sec_M","Age20_Sec_F", "Age20_Hi_M", "Age20_Hi_F", "Age20_Univ_M", "Age20_Univ_F","Age25_NoDip_M", "Age25_NoDip_F","Age25_Sec_M","Age25_Sec_F", "Age25_Hi_M", "Age25_Hi_F","Age25_Univ_M","Age25_Univ_F", "Age30_NoDip_M", "Age30_NoDip_F", "Age30_Sec_M", "Age30_Sec_F", "Age30_Hi_M", "Age30_Hi_F","Age30_Univ_M","Age30_Univ_F", "Age35_NoDip_M", "Age35_NoDip_F", "Age35_Sec_M", "Age35_Sec_F", "Age35_Hi_M", "Age35_Hi_F", "Age35_Univ_M","Age35_Univ_F", "Age40_NoDip_M", "Age40_NoDip_F", "Age40_Sec_M","Age40_Sec_F", "Age40_Hi_M", "Age40_Hi_F", "Age40_Univ_M","Age40_Univ_F", "Age45_NoDip_M", "Age45_NoDip_F","Age45_Sec_M","Age45_Sec_F", "Age45_Hi_M", "Age45_Hi_F", "Age45_Univ_M","Age45_Univ_F", "Age50_NoDip_M", "Age50_NoDip_F","Age50_Sec_M","Age50_Sec_F", "Age50_Hi_M", "Age50_Hi_F", "Age50_Univ_M", "Age50_Univ_F", "Age55_NoDip_M", "Age55_NoDip_F","Age55_Sec_M", "Age55_Sec_F", "Age55_Hi_M", "Age55_Hi_F",
"Age55_Univ_M", "Age55_Univ_F", "Age60_NoDip_M", "Age60_NoDip_F","Age60_Sec_M","Age60_Sec_F", "Age60_Hi_M", "Age60_Hi_F", "Age60_Univ_M", "Age60_Univ_F", "Age65_NoDip_M", "Age65_NoDip_F","Age65_Sec_M","Age65_Sec_F", "Age65_Hi_M", "Age65_Hi_F", "Age65_Univ_M", "Age65_Univ_F")

```

```{r Ineq-Drop unnecessary columns and rename them}

# Drop unnecessary variables
ineq <- subset(ineq, select = -c(P09_POP, NAISD16, DECESD16, P14_RP_PROP, P09_EMPLT,ETTOT15, ETAZ15, ETBE15, ETFZ15, ETGU15, ETGZ15, ETOQ15, ETTEF115, ETTEFP1015))

# rename variables 
names(ineq)[5:ncol(ineq)] <-
  c("pop_2014",
    "Superficie",
    "birth09_14",
    "death09_14",
    "households14",
    "housing14",
    "princ_resid14",
    "sec_resid14",
    "vac_resid14",
    "tax_house14",
    "shared_tax_house14",
    "median_living14",
    "lev_ineq14",
    "empl",
    "emp_sal",
    "pop15_64",
    "unemp15_64",
    "act15_64")
```

```{r Categ_socio-rename variables }

# rename variables
names(categ_socio)[3:ncol(categ_socio)] <-
  c("M_immi_agri",
    "F_immi_agri",
    "M_NoImmi_agri",
    "F_NoImmi_agri",
    "M_immi_comm",
    "F_immi_comm",
    "M_NoImmi_comm",
    "F_NoImmi_comm",
    "M_immi_exec",
    "F_immi_exec",
    "M_NoImmi_exec",
    "F_NoImmi_exec",
    "M_immi_midman",
    "F_immi_midman",
    "M_NoImmi_midman",
    "F_NoImmi_midman",
    "M_immi_emp",
    "F_immi_emp",
    "M_NoImmi_emp",
    "F_NoImmi_emp",
    "M_immi_worker",
    "F_immi_worker",
    "M_NoImmi_worker",
    "F_NoImmi_worker",
    "M_immi_retired",
    "F_immi_retired",
    "M_NoImmi_retired",
    "F_NoImmi_retired",
    "M_immi_noAct",
    "F_immi_noAct",
    "M_NoImmi_noAct",
    "F_NoImmi_noAct")

# Drop unnecessary columns (code/num and name represents same thing) 
categ_socio <- subset(categ_socio, select = -c(LIBGEO))

# Create new variables: total number immigrants per town generated by summing up each categories
categ_socio$total_immig <- rowSums(cbind(categ_socio$M_immi_agri, categ_socio$F_immi_agri, categ_socio$M_immi_comm, categ_socio$F_immi_comm, categ_socio$M_immi_exec, categ_socio$F_immi_exec, categ_socio$M_immi_midman, categ_socio$F_immi_midman, categ_socio$M_immi_noAct, categ_socio$F_immi_noAct, categ_socio$M_immi_retired, categ_socio$F_immi_retired, categ_socio$M_immi_emp, categ_socio$F_immi_emp, categ_socio$M_immi_worker, categ_socio$F_immi_worker))
 
# total number of male immigrants per town 
categ_socio$male_immig<-rowSums(cbind(categ_socio$male_immig <- sum(categ_socio$M_immi_agri, categ_socio$M_immi_comm,categ_socio$M_immi_emp, categ_socio$M_immi_emp, categ_socio$M_immi_exec, categ_socio$M_immi_midman, categ_socio$M_immi_noAct, categ_socio$M_immi_retired, categ_socio$M_immi_worker)))
 
# total number of female immigrants per town
categ_socio$female_immig <- rowSums(cbind(categ_socio$F_immi_agri,categ_socio$F_immi_comm, categ_socio$F_immi_emp, categ_socio$F_immi_emp, categ_socio$F_immi_exec, categ_socio$F_immi_midman, categ_socio$F_immi_noAct, categ_socio$F_immi_retired, categ_socio$F_immi_worker))

```

```{r Status drop and rename variables}

# Drop unnecessary variables
status_work <- subset(status_work, select = -c(LIBGEO))

# rename variables
names(status_work)[2:ncol(status_work)] <-
  c("Less20_M_Empl_Full", "Less20_M_Empl_Half", "Less20_M_Indp1_Full", "Less20_M_Indp1_Half", "Less20_M_Indp2_Ful", "Less20_M_Indp2_Half", "Less20_M_Indp3_Full", "Less20_M_Indp3_Half", "Less20_F_Empl_Full", "Less20_F_Empl_Half", "Less20_F_Indp1_Full", "Less20_F_Indp1_Half", "Less20_F_Indp2_Full", "Less20_F_Indp2_Half", "Less20_F_Indp3_Full", "Less20_F_Indp3_Half", "24_M_Empl_Full", "24_M_Empl_Half", "24_M_Indp1_Full", "24_M_Indp1_Half", "24_M_Indp2_Full", "24_M_Indp2_Half", "24_M_Indp3_Full", "24_M_Indp3_Half", "24_F_Empl_Full", "24_F_Empl_Half", "24_F_Indp1_Full", "24_F_Indp1_Half", "24_F_Indp2_Full", "24_F_Indp2_Half", "24_F_Indp3_Full", "24_F_Indp3_Half", "29_M_Empl_Full", "29_M_Empl_Half", "29_M_Indp1_Full", "29_M_Indp1_Half", "29_M_Indp2_Full", "29_M_Indp2_Half", "29_M_Indp3_Full", "29_M_Indp3_Half", "29_F_Empl_Full", "29_F_Empl_Half", "29_F_Indp1_Full", "29_F_Indp1_Half", "29_F_Indp2_Full", "29_F_Indp2_Half", "29_F_Indp3_Full", "29_F_Indp3_Half", "34_M_Empl_Full", "34_M_Empl_Half", "34_M_Indp1_Full", "34_M_Indp1_Half", "34_M_Indp2_Full", "34_M_Indp2_Half", "34_M_Indp3_Full", "34_M_Indp3_Half", "34_F_Empl_Full", "34_F_Empl_Half", "34_F_Indp1_Full", "34_F_Indp1_Half", "34_F_Indp2_Full", "34_F_Indp2_Half", "34_F_Indp3_Full", "34_F_Indp3_Half", "39_M_Empl_Full", "39_M_Empl_Half", "39_M_Indp1_Full", "39_M_Indp1_Half", "39_M_Indp2_Full", "39_M_Indp2_Half", "39_M_Indp3_Full", "39_M_Indp3_Half", "39_F_Empl_Full", "39_F_Empl_Half", "39_F_Indp1_Full", "39_F_Indp1_Half", "39_F_Indp2_Full", "39_F_Indp2_Half", "39_F_Indp3_Full", "39_F_Indp3_Half", "44_M_Empl_Full", "44_M_Empl_Half", "44_M_Indp1_Full", "44_M_Indp1_Half", "44_M_Indp2_Full", "44_M_Indp2_Half", "44_M_Indp3_Full", "44_M_Indp3_Half", "44_F_Empl_Full", "44_F_Empl_Half", "44_F_Indp1_Full", "44_F_Indp1_Half", "44_F_Indp2_Full", "44_F_Indp2_Half", "44_F_Indp3_Full", "44_F_Indp3_Half", "49_M_Empl_Full", "49_M_Empl_Half", "49_M_Indp1_Full", "49_M_Indp1_Half", "49_M_Indp2_Full", "49_M_Indp2_Half", "49_M_Indp3_Full", "49_M_Indp3_Half", "49_F_Empl_Full", "49_F_Empl_Half", "49_F_Indp1_Full", "49_F_Indp1_Half", "49_F_Indp2_Full", "49_F_Indp2_Half", "49_F_Indp3_Full", "49_F_Indp3_Half", "54_M_Empl_Full", "54_M_Empl_Half", "54_M_Indp1_Full", "54_M_Indp1_Half", "54_M_Indp2_Full", "54_M_Indp2_Half", "54_M_Indp3_Full", "54_M_Indp3_Half", "54_F_Empl_Full", "54_F_Empl_Half", "54_F_Indp1_Full", "54_F_Indp1_Half", "54_F_Indp2_Full", "54_F_Indp2_Half", "54_F_Indp3_Full", "54_F_Indp3_Half", "59_M_Empl_Full", "59_M_Empl_Half", "59_M_Indp1_Full", "59_M_Indp1_Half", "59_M_Indp2_Full", "59_M_Indp2_Half", "59_M_Indp3_Full", "59_M_Indp3_Half", "59_F_Empl_Full", "59_F_Empl_Half", "59_F_Indp1_Full", "59_F_Indp1_Half", "59_F_Indp2_Full", "59_F_Indp2_Half", "59_F_Indp3_Full", "59_F_Indp3_Half", "64_M_Empl_Full", "64_M_Empl_Half", "64_M_Indp1_Full", "64_M_Indp1_Half", "64_M_Indp2_Full", "64_M_Indp2_Half", "64_M_Indp3_Full", "64_M_Indp3_Half", "64_F_Empl_Full", "64_F_Empl_Half", "64_F_Indp1_Full", "64_F_Indp1_Half", "64_F_Indp2_Full", "64_F_Indp2_Half", "64_F_Indp3_Full", "64_F_Indp3_Half", "69_M_Empl_Full", "69_M_Empl_Half", "69_M_Indp1_Full", "69_M_Indp1_Half", "69_M_Indp2_Full", "69_M_Indp2_Half", "69_M_Indp3_Full", "69_M_Indp3_Half", "69_F_Empl_Full", "69_F_Empl_Half", "69_F_Indp1_Full", "69_F_Indp1_Half", "69_F_Indp2_Full", "69_F_Indp2_Half", "69_F_Indp3_Full", "69_F_Indp3_Half")
```


```{r match CODGEO}

educ$CODGEO            <- sub("A", "0", educ$CODGEO)
educ$CODGEO            <- sub("B", "0", educ$CODGEO)
categ_socio$CODGEO     <- sub("A", "0", categ_socio$CODGEO)
categ_socio$CODGEO     <- sub("B", "0", categ_socio$CODGEO)
ineq$CODGEO            <- sub("A", "0", ineq$CODGEO)
ineq$CODGEO            <- sub("B", "0", ineq$CODGEO)
status_work$CODGEO     <- sub("A", "0", status_work$CODGEO)
status_work$CODGEO     <- sub("B", "0", status_work$CODGEO)


# Then all CODGEO are trasformed to integers and four new datasets are created retaining only the common CODGEO:
# use only integer values
status_work$CODGEO = as.integer(status_work$CODGEO)  
ineq$CODGEO = as.integer(ineq$CODGEO)
educ$CODGEO = as.integer(educ$CODGEO)
categ_socio$CODGEO = as.integer(categ_socio$CODGEO)

# store datasets' names to loop on them
dataset = c("educ", "categ_socio", "ineq", "status_work")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and create a new variable concateneting "NEW" at the end
  nam <- paste(i, "NEW", sep = "")
  # initialize counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # for each dataset different from the i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(i))
  del_obs = paste("For", i, del_rows, "units have been deleted.",
                  "They were the", round(del_prop*100, digits=2), "% of the total.", sep = " ")
  print(paste(del_obs))
}

```

# Chiaromonte's part

## Unsupervised

Preliminary multivariate salary data visualisation to better conduct the unsupervised learning  
```{r}

# add region variable to the salary dataset
salary_with_dep <- cbind(geoNEW[1], salaryNEW)
#check
#ncol(salary_with_dep)
#head(salary_with_dep)
# extract continuous variables
salary_variables <- salary_with_dep[, 4:35]

#scatterplot to visually see the correlations between variables
pairs(salary_variables[1:8], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#scatterplot between variables of major interest
pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#bivariate densities among general, males and females salaries
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, 
       breaks[-1], y, col="grey", ...)
}


pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")],
      diag.panel = panel.hist,
      panel = function (x,y) {
        data <- data.frame(cbind(x,y))
        par(new = TRUE)
        den <- bkde2D(data, 
                      bandwidth=sapply(data,dpik))
        contour(x = den$x1, y = den$x2, 
                z = den$fhat, axes = FALSE)
      })

#scale the data
salary_variables_scaled <- as.data.frame(scale(salary_variables))
#divide sal_general into 4 parts
salary_levels <- with(salary_variables_scaled, 
                  equal.count(sal_general,4))
#make a 3D plot of salary_ratio_FvsM against sal_26_50 and sal_employee given salary_levels
plot(cloud(salary_ratio_FvsM ~ sal_26_50 * sal_employee | salary_levels, 
           panel.aspect = 0.5,
           data = salary_variables_scaled))

#plot sal_worker against sal_midManager given 3 partitions of sal_general
plot(xyplot(sal_worker ~ sal_midManager| cut(sal_general, 3), 
            data = salary_variables, 
            layout = c(3, 1), 
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))

#multidimesnional plot of midManager salary against worker salary, given 4 partitions of F/M salary ratio, and grayscale intensities of general salary
M_F_salary_ratio  <- with(salary_variables, equal.count(salary_ratio_FvsM, 4))
general_salary.ord <- with(salary_variables, rev(order(sal_general)))
salary_variables.ordered <- salary_variables[general_salary.ord,]
sal_general.breaks <- with(salary_variables.ordered, 
                     do.breaks(range(sal_general),50))
salary_variables.ordered$color<-level.colors(salary_variables.ordered$sal_general,
                                   at=sal_general.breaks,
                                   col.regions=grey.colors)
plot(xyplot(sal_worker ~ sal_midManager | M_F_salary_ratio, 
            data = salary_variables.ordered,
            aspect = "iso", 
            groups = color, 
            cex = 1, col = "black",
       panel = function(x, y, groups, ..., subscripts) {
           fill <- groups[subscripts]
           panel.grid(h = -1, v = -1)
           panel.xyplot(x, y, pch = 21, 
                        fill = fill, ...)
       },
       legend =
       list(right =
            list(fun = draw.colorkey,
                 args=list(key=list(col=gray.colors,
                                    at = sal_general.breaks),
                           draw = FALSE))),
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))


#plot(cloud(sal_general ~ sal_worker * salary_ratio_FvsM | sal_26_50, 
#           data = salary_variables,
#           zlim = rev(range(salary_variables$sal_general)),
#           screen = list(z = 5, x = -3), 
#           panel.aspect = 0.9,
#           xlab = "Worker salary", 
#           ylab = "M_F_salary_ratio", 
#           zlab = "General salary"))



```

Multi-dimensional scaling (MDS) on salary data
```{r}
# produces a list with covariance matrix
# for each region as component:
salary_var <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
                     function(i) var(salary_with_dep[i,4:35]))
str(salary_var)

#remove regions with null values
salary_var <- salary_var[-c(11, 12, 15, 19, 20, 28)]
str(salary_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(salary_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(salary_var[[1]])
# creates common covariance matrix S
s <- 0
for (v in c(1:22)) S <- S + regions_count_sub$Freq[v] * as.matrix(salary_var[[v]]) 
S <- S / 5021

# finds center of each variable (sal_general, etc)
# for each region
salary_cen <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
    function(i) apply(salary_with_dep[i,-c(1:3)], 2, mean))
salary_cen
salary_cen <- salary_cen[-c(11, 12, 15, 19, 20, 28)]
str(salary_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
salary_cen <- matrix(unlist(salary_cen), 
    nrow = length(salary_cen), byrow = TRUE)
salary_cen

# compute the mahalanobis distances:
salary_mah <- apply(salary_cen, 1, 
    function(cen) mahalanobis(salary_cen, cen, S))
salary_mah



# run it on two dimensions
salary_mds <- cmdscale(salary_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(salary_mds) * 1.2
plot(salary_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(salary_mds, labels = levels(salary_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
salary_mds <- cmdscale(salary_mah, k = nrow(salary_mah) - 1, 
         eig = TRUE)

#only 9 of the first 21 eigenvalues are > 0
x <- salary_mds$points[,1]
y <- salary_mds$points[,2]
st <- mst(salary_mah)
plot(x, y, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = range(x)*1.2, type = "n")
for (i in 1:nrow(salary_mah)) {
  w1 <- which(st[i, ] == 1)
  segments(x[i], y[i], x[w1], y[w1])
}
text(x, y, labels = colnames(salary_mah), 
     cex = 1.5)

```


PCA on salary data and delta M-F
```{r}

pca_salary <- prcomp(salary_variables)
autoplot(pca_salary, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary, type = "l")
#pca_salary
#summary(pca_salary)

#create dataset with M-F differences
salary_var_delta <- data.frame("?? M-F" = salary_variables$sal_Males -                                                           salary_variables$sal_Females,
                     "?? Executive M-F" = salary_variables$sal_M_executive -
                                         salary_variables$sal_F_executive,
                    "?? midManager M-F" = salary_variables$sal_M_midManager -
                                         salary_variables$sal_F_midManager,
                      "?? employee M-F" = salary_variables$sal_M_employee -
                                         salary_variables$sal_F_employee,
                        "?? worker M-F" = salary_variables$sal_M_worker -
                                         salary_variables$sal_F_worker,
                         "?? 18-25 M-F" = salary_variables$sal_M_18_25 -
                                         salary_variables$sal_F_18_25,
                         "?? 26-50 M-F" = salary_variables$sal_M_26_50 -
                                         salary_variables$sal_F_26_50,
                       "?? 51 plus M-F" = salary_variables$sal_M_51plus -
                                         salary_variables$sal_F_51plus)
pca_salary_delta <- prcomp(salary_var_delta)
autoplot(pca_salary_delta, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary_delta, type = "l")
#pca_salary_delta
#summary(pca_salary_delta)



```

Cluster Analysis on salary and delta M-F
```{r}


#create dataframe with first two principal components scores as variables
salary_pca_scores<- as.data.frame(pca_salary$x[,1:2])
salary_pca_scores <- as.data.frame(scale(salary_pca_scores))
sapply(salary_pca_scores, var)
#scatterplot between PC1 and PC2 to see if there are outliers
pairs(salary_pca_scores, 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#after visually identifying thresholds, get rid of outliers
outliers <- subset(salary_pca_scores, PC1 > 8 | PC2 > 13)
outliers_names <- unlist(as.numeric(rownames(outliers)))
salary_pca_scores <- salary_pca_scores[-outliers_names,]
#renumber rows
row.names(salary_pca_scores) <- 1:nrow(salary_pca_scores)

#sample data
set.seed(123)
salary_scores_sampled <- sample_n(salary_pca_scores, 1000)

#renumber rows
row.names(salary_scores_sampled) <- 1:nrow(salary_scores_sampled)

#create distance matrix for hclust
dm <- dist(salary_scores_sampled)
#round to two decimals
dm <- round(dm, 2)

# Below we show the cluster solutions for
# salary data. The top row gives the cluster
# dendograms along with the cutoff used to
# derive the classes presented (in the space
# of the two principal components) in the
# bottom row:


salary_pc <- princomp(dm, cor = TRUE)
layout(matrix(1:6, nr = 2), height = c(2, 1))
plot(cs <- hclust(dm, method = "single"), 
     main = "Single")
abline(h = 3.8, col = "lightgrey")
xlim <- range(salary_pc$scores[,1])
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cs, h = 3.8)
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)
plot(cc <- hclust(dm, method = "complete"), 
     main = "Complete")
abline(h = 7, col = "lightgrey")
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cc, h = 7)  
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)     
plot(ca <- hclust(dm, method = "average"), 
     main = "Average")
abline(h = 7.8, col = "lightgrey")
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(ca, h = 7.8)                             
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)  

# K-means clustering
n <- nrow(salary_pca_scores)
wss <- rep(0, 22)
wss[1] <- (n - 1) * sum(sapply(salary_pca_scores, var))
for (i in 2:22)
    wss[i] <- sum(kmeans(salary_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:22, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 5 groups 
plot(salary_pca_scores, 
     pch = kmeans(salary_pca_scores, 
                  centers = 5)$cluster)

#now we have to see whether these groups correspond to geographical regions...

```


## Supervised

Function for interactive plotting:
```{r}
# This function is used to obtain interactive regression diagnostic plots
RegressionPlots <- function(fit, textlabels, robust=F){
  
  # number of units
  n = length(fit$fitted.values)
  
  # original response
  y = fit$y
  
  # Extract fitted values from lm() object
  Fitted.Values <-  fit$fitted.values
  
  # Extract residuals from lm() object
  Residuals <-  fit$residuals
  
  if (robust == F){
    # Extract standardized residuals from lm() object
    Standardized.Residuals <- MASS::stdres(fit)  
    
    # Calculate Leverage
    Leverage <- lm.influence(fit)$hat
    
    # text for the labels of the plots
    tt = "OLS"
  } else{
    # standardized residuals based on a robust scale estimate
    Standardized.Residuals <- fit$residuals/fit$scale
    
    # get the robust distances based on mahalanobis distance from the robust output 
    # (it has to be specified in the call to lmrob using: control = lmrob.control(compute.rd = T))
    Leverage <- fit$MD
    
    # text for the labels of the plots
    tt = "Robust"
  }
  
  # Extract fitted values for lm() object
  Theoretical.Quantiles <- qqnorm(Residuals, plot.it = F)$x
  
  # Create data frame 
  # Will be used as input to plot_ly
  regMat <- data.frame(Fitted.Values, 
                       Residuals, 
                       Standardized.Residuals, 
                       Theoretical.Quantiles,
                       Leverage, 
                       textlabels)
  
  # Plot using Plotly
  
  # text info
  t <- list(
    family = "sans serif",
    size = 14,
    color = toRGB("grey50"))
  
  
  # Plot 1: Fitted vs Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Residuals)
  
  plt1 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 

    layout(title = paste(tt, " Residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Residuals"))
  
  # Plot 2: Fitted vs Standardized Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Standardized.Residuals)
  
  plt2 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
    
    layout(title = paste(tt, " Standardized residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Standardized residuals"))  

  # Plot 3: Residuals index
  plt3 <- regMat %>% 
    plot_ly(x = 1:n, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', 1:n,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_segments(x = -5, xend = n+5, y = 0, yend = 0, mode = "line", inherit = F, 
                 name = "Null line", line = list(width = 2)) %>%
    
    layout(title = paste(tt, " Standardized Residuals' index"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Index"),
             yaxis = list(title = "Standardized.Residuals")) 

  
  # Plot 4: QQ Pot
  plt4 <- regMat %>% 
    plot_ly(x = Theoretical.Quantiles, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Theoretical.Quantiles,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = Theoretical.Quantiles, y = Theoretical.Quantiles, type = "scatter", 
              mode = "lines", name = "Theoretical", line = list(width = 2), 
              inherit = F) %>%
    
    layout(title = paste(tt, " Q-Q Plot"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Normal theoretical quantiles"),
             yaxis = list(title = "Data quantiles"))  

  
  # Plot5: Residuals vs Leverage
  # For scatter plot smoother
  LOESS <- loess.smooth(Leverage, Residuals)
  
  plt5 <- regMat %>% 
    plot_ly(x = Leverage, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            name = "Data", marker = list(size = 10, opacity = 0.5), 
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Leverage,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              line = list(width = 2), inherit = F) %>%
    
    layout(title = paste(tt, " Leverage vs Residuals"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Leverage values"),
             yaxis = list(title = "Residuals"))  


  # Plot 6: Fitted vs response
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, y)
  
  plt6 <- regMat %>% 
    plot_ly(x = Fitted.Values, y =y, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', y)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
  
    layout(title = paste(tt, " Fitted Values vs response"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Fitted Values"),
             yaxis = list(title = "Response values"))

  # # Plot 7: MISSING: sqrt(abs(Residuals))
  # # For scatter plot smoother
  # LOESS2 <- loess.smooth(Fitted.Values, Root.Residuals)
  # 
  # plt3 <- regMat %>% 
  #   plot_ly(x = Fitted.Values, y = Root.Residuals, 
  #           type = "scatter", mode = "markers", hoverinfo = "text", 
  #           name = "Data", marker = list(size = 10, opacity = 0.5),
  #           text = paste('town: ', as.character(textlabels))) %>%
  #   
  #   add_trace(x = LOESS2$x, y = LOESS2$y, type = "scatter", mode = "lines", name = "Smooth",
  #             line = list(width = 2), inherit = F) %>% 
  #   
  #   layout(title = "Scale Location", plot_bgcolor = "#e6e6e6")
  
  plt = list(plt1, plt2, plt3, plt4, plt5, plt6)
  return(plt)
}

```


The focus of this part is on the delta between males and females salaries.
```{r linear model for delta M-F salary, fig.keep='all'}

# response variable
y = newDat$sal_Males - newDat$sal_Females
# original response variable histogram
ggplot(data=as.data.frame(y), aes(y)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) +
  geom_density(col="black") +
  labs(x="Male-Female salary", y="Density") +
  ggtitle("Original response variable") +
  theme(plot.title = element_text(hjust = 0.5))

x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)
# merge variables
mod = cbind.data.frame(y, x)

# scatter plot matrix
# pairs(mod)
# correlations
corrplot(cor(mod), method = "number", title = "Correlation matrix",
         diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black")  # , mar=c(0,0,1.5,0)
plot(newDat$sal_M_26_50 - newDat$sal_F_26_50,y)

# linear model
ols = lm(y ~ ., data = mod, y=T)
summary(ols)
# plot(ols)
RegressionPlots(ols, newDat$town_name, robust = F)

# robust fit
# MM estimator
# 
mm = lmrob(y~., data = mod, y=T, setting = "KS2014", control = lmrob.control(compute.rd = T))
summary(mm)
names(mm)
# plot(mm, cex = 0.6, id.n = 10, labels.id = newDat$town_name, sub.caption="MM")
RegressionPlots(mm, newDat$town_name, robust = T)


```


Model selection:
```{r}
# outliers identified
indd = c(165,170,188,193,206,208,209,365,367,371,377,444,448,735,1184,1193,1226,1247,1251,1385,1477,1495,1710,1711,1729,1759,1786,1796,2450,2757,2942,2999,3091,3277,3463,3554,3557,3565,3566,3580,3588,3602,3621,3781,3891,3896,3904,4027,4091,4118,4123,4124,4128,4129,4131,4132,4133,4137,4142,4143,4148,4158,4161,4164,4167,4168,4169,4172,4173,4177,4179,4181,4182,4184,4185,4191,4193,4198,4199,4209,4210,4211,4214,4215,4725,4757,4792,4803,4805,4806,4825,4834,4843,4846,4847,4848,4849,4852,4853,4918,4919,4931,4966,4985,5011)

# copy of the prrevious options
y = newDat$sal_Males - newDat$sal_Females
x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)

# merge variables
mod = cbind.data.frame(y, x)
# mod = mod[indd,]

# mod = cbind.data.frame(y, x)
mod = cbind.data.frame(mod, mod[,2:ncol(mod)]^2, mod[,2:ncol(mod)]^3)
namesdataBS = names(mod)
lll = ((dim(mod)[2]-1)/3)

# for (i in 1:lll+1){
#   str = namesdataBS[i]
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1]
# }
namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll), sep="")
namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll), sep="")
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll))
names(mod) = namesdataBS

# Elastic net
x = as.matrix(mod[, 2:ncol(mod)])
y = mod[, 1]
par(mfrow =c(3,2))
for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
  # set.seed (3)
  cv.out = cv.glmnet(x, y, alpha = j) # , parallel = T
  plot(cv.out)
  title(paste("alpha = ", j), line = 2.3)
}
mtext(expression("Best lambda for delta gender salary using elastic-net and 10-folds CV"), outer=TRUE,  cex=1, line=-1.4)


# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE',
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 

# best subset selection
best.sub = regsubsets(y ~ ., data = mod, nvmax = ncol(mod))
best.sub.summary = summary(best.sub)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
# mtext("My 'Title' in a strange place", line=15)
mtext("Best subset selection for delta gender salary", outer=TRUE,  cex=1.2, line=-2.5)
# built-in plots
par(mfrow=c(1,1))
plot(best.sub, scale = "r2")
plot(best.sub, scale = "adjr2")
plot(best.sub, scale = "Cp")
plot(best.sub, scale = "bic")
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.9)
# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5)

# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))
# nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
# names(mod) %in% nnn 

```

