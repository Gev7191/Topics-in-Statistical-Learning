---
title: "TSL Project"
author: "Luca Insolia, Jisu Kim and Gevorg Yeghikyan"    
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_notebook:
    toc: yes
    toc_depth: '3' 
    highlight: tango
    keep_tex: yes
    number_sections: yes
  html_document:
    theme: united
    highlight: tango    
    toc: yes
    toc_depth: '3' 
#   pdf_document: # ADD PDF!
#     toc: yes
#     toc_depth: '3'
#     keep_tex: true
#     latex_engine: pdflatex
# editor_options:
#   # toc: yes
#   # toc_depth: 3
#   chunk_output_type: inline
---


# General information 

We analyze a dataset published on [Kaggle](https://www.kaggle.com/etiennelq/french-employment-by-town).
It refers to french employment, salaries, population per town. 
The aim is to evaluate equality/inequalities in France, and geographical distribution of business according to their size.

Such data are collected by the INSEE.
Information regarding the number of firms in every french town, categorized by size
can be found [here](https://www.insee.fr/fr/metadonnees/definition/c1135). This dataset contains about 35000 units/per town.

Information about salaries around french town per job categories, age and sex (expressed in average net amount per hour in euro) can be found [here](https://www.insee.fr/fr/statistiques/2522515). This dataset contains about 5000 units/per town. 

Demographic information in France per town, age, sex and living mode
can be found [here](https://www.insee.fr/fr/statistiques/2863607). This dataset contains about 8 million units/per town. Additional info about Population Data can be found [here](https://www.insee.fr/fr/statistiques/2863607#dictionnaire). 

These datasets have been pre-processed and put together. The final dataset contains 58 variables and 5022 observations. 


## Aim of the study 

This project aims to explore structure of French labour market. 
In particular, we are interested in:

* evaluating possible inequalities: per towns/region, sex, age, job categories etc.;
* discover geographical distribution of business according to their size
* predicting the ... using a regression model;
* reduce the dimensionality of ... performing a PCA;
* explore different algorithms to cluster male/females using ...


## Plan for the study 

1. Unsupervised learning: 
* PCA
* Clustering methods (K-means/Hierarchical)
2. Supervised learning:
* GLM
* Linear/Quadratic Discriminant Analysis
* KNN
* Cross-validation
* Bootstrap
* Subset selection 
* Shrinkage methods
* Dimension reduction methods
3. Description of population demographics in France
4. Structure of the french labour market
5. ...
6. Future works


## Loading tools

Loading all libraries needed throughout the notebook:
```{r loading libraries}

# data manipulation
if(!require(plyr)){install.packages("dplyr"); library(plyr)}
if(!require(dplyr)){install.packages("dplyr"); library(dplyr)}
# plotting
if(!require(ggplot2)){install.packages("ggplot2"); library(ggplot2)}
if(!require(ggfortify)){install.packages("ggfortify"); library(ggfortify)}
if(!require(ggmap)){install.packages("ggmap"); library(ggmap)}
if(!require(ggbiplot)){
  if(!require(devtools)){
    install.packages("devtools")}
  install_github("vqv/ggbiplot")
  library(ggbiplot)}
if(!require(corrplot)){install.packages("corrplot"); library(corrplot)}
if(!require(zoo)){install.packages("zoo"); library(zoo)}
if(!require(plotly)){install.packages("plotly"); library(plotly)} # interactive plotting
if(!require(lattice)){install.packages("lattice"); library(lattice)}
if(!require(MVA)){install.packages("MVA"); library(MVA)}
if(!require(ape)){install.packages("ape"); library(ape)}
if(!require(KernSmooth)){install.packages("KernSmooth"); library(KernSmooth)}
# Mclust clustering
if(!require(mclust)){install.packages("mclust"); library(mclust)}
# plot design matrix
if(!require(rafalib)){install.packages("rafalib"); library(rafalib)}
# cross validation
if(!require(boot)){install.packages("boot"); library(boot)}
# elastic net
if(!require(glmnet)){install.packages("glmnet"); library(glmnet)}
# group Lasso
if(!require(gglasso)){install.packages("gglasso"); library(gglasso)}
# robust fit
if(!require(robustbase)){install.packages("robustbase"); library(robustbase)}
# box-cox tranformation (in ANOVA) and standardized residuals
if(!require(MASS)){install.packages("MASS"); library(MASS)}
# best subset selection
if(!require(leaps)){install.packages("leaps"); library(leaps)}
# plot for PCA
# if(!require(factoextra)){install.packages("factoextra"); library(factoextra)}

```

Import the four main datasets:
```{r loading datasets, warning=FALSE}

setwd("./data")
firms       <- read.csv("base_etablissement_par_tranche_effectif.csv", encoding = "UTF-8")
geo         <- read.csv("name_geographic_information.csv", encoding = "UTF-8")
salary      <- read.csv("net_salary_per_town_categories.csv", encoding = "UTF-8")
population  <- read.csv("population.csv", encoding = "UTF-8")
educ        <- read.csv("level_education.csv", sep =";", encoding = "UTF-8")
categ_socio <- read.csv("Categorie_socioprofessionnelle.csv", sep =";", encoding = "UTF-8")
status_work <- read.csv("Emplois_lieu_travail.csv", sep =";", encoding = "UTF-8")
ineq        <- read.csv("Comparateur_territoires.csv", sep =";", encoding = "UTF-8")
commune     <- read.csv("insee_commune.csv", encoding = "UTF-8")

```

# Analyze firms data 

## Pre-processing

Assign meaningful names and check the modified data:
```{r pre processing firms}

names(firms) 
names(firms)[2:ncol(firms)] <-
  c("town", 
    "regNum",
    "deptNum",
    "total",
    "null",
    "firmsEmpl_1_5",
    "firmsEmpl_6_9",
    "firmsEmpl_10_19",
    "firmsEmpl_20_49",
    "firmsEmpl_50_99",
    "firmsEmpl_100_199",
    "firmsEmpl_200_499",
    "firmsEmpl_500plus")

# preliminary checks
names(firms)
head(firms)
str(firms)
summary(firms)

# Check for duplicated data: there is no
sum(duplicated.data.frame(firms))

```

Categorize firms' size according to [EU standard](http://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Enterprise_size), but in a slightly different form for medium and large firms (i.e., medium firms have <200 instead of <250 employees):
```{r modify firms}

# merge variables
firms$micro   <- firms$firmsEmpl_1_5 + firms$firmsEmpl_6_9
firms$small   <- firms$firmsEmpl_10_19 + firms$firmsEmpl_20_49
firms$medium  <- firms$firmsEmpl_50_99 + firms$firmsEmpl_100_199
firms$large   <- firms$firmsEmpl_200_499 + firms$firmsEmpl_500plus

# Drop unnecessary (at the moment) columns 
firms <- subset(firms, select = c(CODGEO, town, total, micro, small, medium, large, null))

# check
summary(firms)

# there is an obs with more than 316K null data: we check if it is plausible
# get the highest 20 null values
str_firms <- sort(firms$null, decreasing = T)[1:20]
# get their indexes
str_firms_ind <- match(str_firms, firms$null)
# get the corresponding city
firms$town[str_firms_ind]
# they are the largest cities, hence it seems reasonable..

```

## Descriptive statistics 

Check the distribution of the null firms (i.e., unknown sizes) and analyze
firms' distribution per town:
```{r firms distribution}

# check the ratio of null firms for each town
summary(firms$null/firms$total)
# a lot of information is missing, should we remove these data?
ggplot(data=firms, aes(firms$null/firms$total)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Null firms/total firms", y="Count") +
  ggtitle("Ratio between the number of null firms and total firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# distribution for the firms with unknown size 
ggplot(data=firms, aes(log(firms$null))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of unknown size (log scale)", y="Density") +
  ggtitle("Number of firms of unknown size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the total number of firms 
ggplot(data=firms, aes(log(firms$total))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms (log scale)", y="Density") +
  ggtitle("Total number of firms per town") +
  theme(plot.title = element_text(hjust = 0.5)) 

# distribution for the number of micro size firms 
ggplot(data=firms, aes(log(firms$micro))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of micro size (log scale)", y="Density") +
  ggtitle("Number of firms of micro size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of small size firms 
ggplot(data=firms, aes(log(firms$small))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of small size (log scale)", y="Density") +
  ggtitle("Number of firms of small size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of medium size firms 
ggplot(data=firms, aes(log(firms$medium))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of medium size (log scale)", y="Density") +
  ggtitle("Number of firms of medium size per town") +
  theme(plot.title = element_text(hjust = 0.5))

# distribution for the number of large size firms 
ggplot(data=firms, aes(log(firms$large))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Number of firms of large size (log scale)", y="Density") +
  ggtitle("Number of firms of large size per town") +
  theme(plot.title = element_text(hjust = 0.5))

```

Check correlation among the variables:
```{r firms correlation}
corrplot(cor(firms[,3:8]))
```


## What have learned

Solved problems: 

* Use the EU firms' categorization.

Some problematic aspects:

* Some towns have 100% of null firms, should we remove these data?
* These variables are strongly correlated, should we use just the total amount in the following parts?

We could use these data for the following tasks:

* predict the salaries using such information as proxy for the competition in the job market;
* predict the total number of firms, using salary data;
* geo-spatial plot for firms' size 


# Analyze geographical data 

## Pre-processing 

Assign names and remove some variables:
```{r pre processing geo}

names(geo)
names(geo)[c(2:11, 14)] =
  c("code_region",
    "region", 
    "region_capital",
    "number_depart",
    "department", 
    "prefecture",
    "circons",
    "town_name", 
    "postal_code", 
    "CODGEO",
    "eloignement")

# drop unnecessary columns (code/num and name represents same thing) 
geo <- subset(geo, select = -c(EU_circo, code_region, number_depart, prefecture, circons, eloignement, town_name))

# preliminary checks
names(geo)
head(geo)
str(geo)
summary(geo)

```

Correct typos for longitude data and keep just the unique CODGEO to avoid towns with multiple postal codes:
```{r fix geo}

# spot "," instead of "." in longitude
newLong       <- as.character(geo$longitude)    # copy the vector
sum(grep(",", newLong))                         # total commas
ind_long_err  <- grep(",", newLong)             # indexing them
newLong       <- gsub(",", ".", newLong)        # substituting them with dots
indNA_Long    <- is.na(as.numeric((newLong)))   # spot NA
# geo$longitude[indNA_Long]                       # verify that they were actually missing
geo$longitude <- as.numeric(newLong)            # overwrite the longitude variable with the new one

# Check for duplicated data (e.g., cities with different postal codes, that we dropped):
  # e.g., to verify it,  try on the initial dataset:
  # sum(geo$nom_commune == "Paris")
  # ind_duplic <- geo$nom_commune == "Paris"
  # geo[ind_duplic,]
sum(duplicated.data.frame(geo)) 
# retaing unique postal cities
geo <- geo[!duplicated(geo$CODGEO),]

# check again
summary(geo)

```

Assign latitude and longitude values for missing data (almost 3000):
```{r assign NA in geo, warning=FALSE}

# index of NAs and their total
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)

# code used to retrieve the NA using Google API, which have been saved in a csv file
# 
# # initialize variables
# city_search = 0
# res = as.data.frame(matrix(c(0, 0, 0), 1, 3))
# names(res) = c("lon", "lat", "address")
# 
# # retrieve lat and long (Google API = 2500 request per day)
# # my_iter = floor(sum(indNA_coord)/3)
# for (i in 1:sum(indNA_coord)){
# 
#   # city searched
#   city_search[i] = paste(c(as.character(NA_coord$town_name[i]), as.character(NA_coord$postal_code[i]), as.character(NA_coord$department[i]), "France"), sep=" ", collapse = ", ")
#   
#   # solution
#   res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
# 
#   # retrieve still missing data, because of existing problems with API (up to 15 trials)
#   j = 0
#   while (any(is.na(res[i,])) & j < 25){
#     res[i,] = geocode(city_search[i], output = "latlona", source = c("google", "dsk"), messaging = FALSE)
#     j = j + 1
#   }
# }

# # check the solution
# sol = cbind(searched = city_search, res)

# # save it as a csv file to save time
# write.csv(retrieved_geo_NA[,2:3], "geo_NA_Final.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
    

# read the created csv
setwd("./data")
retrieved_geo_NA = read.csv("geo_NA_Final.csv", header = T, encoding = "UTF-8")
# get only long and lat and assign to original NA 
geo$latitude[indNA_coord] = retrieved_geo_NA[,2]
geo$longitude[indNA_coord] = retrieved_geo_NA[,1]

# there are 37 still missing units, which are towns located in old colonies far from Europe
indNA_coord = is.na(geo$latitude) | is.na(geo$longitude)
sum(indNA_coord)
# exclude those towns
geo = geo[!indNA_coord,]

summary(geo)

```

## Descriptive statistics 

Plot available towns on a map:
```{r geo map}

# delete non-European countries
ind_nonEur = geo$latitude < 30 | geo$latitude > 70 |geo$longitude < -20 | geo$longitude > 20
sum(ind_nonEur)
geo = geo[!ind_nonEur,]

# center of France, obtained using:
# fra_center = as.numeric(geocode("France"))
fra_center = c(2.213749, 46.227638)
# plot all European towns available
geo_pos = as.data.frame(cbind(lon = geo$longitude, lat = geo$latitude))
geo_pos = geo_pos[complete.cases(geo_pos),]
ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") +
  geom_point(aes(x=lon, y=lat), data=geo_pos, col="orange", alpha=0.2, size=0.01) 

# Try to cll google API until there is no error
# 
# FraMap = NA
# while (any(is.na(FraMap))){
#   FraMap = tryCatch({
#       ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal")
#   }, error = function(e) {
#     message("cannot open URL")  
#     FraMap = NA
#   })
# }

```

## What we have learned 

Solved problems: 

* Missing latitude and longitude;
* Duplications due to multiple postal codes in the same town;
* Exclude non-European towns;

These information are useful to plot any future dataset/analysis.
In addition, they could be useful to compare European towns vs. old colonies.


# Analyze salary data 

## Pre-processing 

Assign meaningful names and check the data:
```{r pre processing salary}

names(salary)
names(salary)[2:ncol(salary)] <-
  c("town",
    "sal_general",    
    "sal_executive",
    "sal_midManager",
    "sal_employee",
    "sal_worker",
    "sal_Females",
    "sal_F_executive",
    "sal_F_midManager",
    "sal_F_employee",
    "sal_F_worker",
    "sal_Males",
    "sal_M_executive",
    "sal_M_midManager",
    "sal_M_employee",
    "sal_M_worker",
    "sal_18_25",
    "sal_26_50",
    "sal_51plus",
    "sal_F_18_25",
    "sal_F_26_50",
    "sal_F_51plus",
    "sal_M_18_25",
    "sal_M_26_50",
    "sal_M_51plus")

# preliminary checks
names(salary)
head(salary)
str(salary)
summary(salary)

# Check for duplicated data: there are no
sum(duplicated.data.frame(salary))

# drop unnecessary variable
salary <-subset(salary, select = -c(town))

```

## Descriptive statistics 

Univariate analysis comparing salaries for both genders among various job categories:

```{r boxplots works}

#  number of units
n_sex <- length(salary$sal_Females)
# vector representing males and females
Label <- c(rep("M", n_sex*5), rep("F", n_sex*5))
# vector representing the variable considered
Variable <- c(rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex),
             rep("General", n_sex), 
             rep("Executive", n_sex),
             rep("MidManager", n_sex),
             rep("Employee", n_sex),
             rep("Worker",n_sex))
# merge these data
sal_sex = cbind.data.frame(Label = Label, 
             value = c(salary$sal_Males, salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
                       salary$sal_Females, salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker),
             Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  # not color points replacing colour = group instead of colour=Label
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different job positions") +
  theme(plot.title = element_text(hjust = 0.5)) +      stat_boxplot(geom = "errorbar", width = 0.5)
  # + guides(fill=guide_legend(title="Legend"))

# the same but excluding outliers
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  scale_y_continuous(limits = quantile(sal_sex$value, c(0, 0.9))) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + 
  ggtitle("Gender comparison for different job positions excluding the last decile") +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


Univariate analysis comparing salaries for both genders among various ages:

```{r boxplots ages}

# vector representing males and females
Label <- c(rep("M", n_sex*3), rep("F", n_sex*3))
# vector representing the variable considered
Variable <- c(rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex),
              rep("18-25", n_sex), 
              rep("26-50", n_sex),
              rep("51+", n_sex))
# merge these data
sal_sex <- cbind.data.frame(Label = Label, 
                           value = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus, 
                                     salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus),
                           Variable = Variable)
# plotting phase
ggplot(data = sal_sex, aes(x=Label, y=value)) +
  geom_boxplot(aes(fill = Label)) +
  geom_point(aes(y=value, colour=Label), position = position_dodge(width=0.75)) +
  facet_wrap( ~ Variable, scales="free") +
  xlab("Sex") + ylab("Mean net salary per hour") + ggtitle("Gender comparison for different ages") +
  theme(plot.title = element_text(hjust = 0.5)) + ylim(c(5, 100)) +
  stat_boxplot(geom = "errorbar", width = 0.5)

```


The income inequality between genders, age groups and working positions is clear.
In the following analyses the focus is on the salary ratio between women and men among different
job positions:
```{r ratio F vs M accross jobs}

# Gender salary ratio and general level of income

# Overall mean salary: The higher the net mean income, the more skewed the ratio of salary between female and male is. Only 2 towns have a ratio>1
# create overall F vs M ratio
salary$salary_ratio_FvsM <- salary$sal_Females / salary$sal_Males
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Overall salary ratio (females/males)", y="Density") + 
  labs(title = "Overall salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs overall mean salary
ggplot(salary, aes(x= sal_general, y=salary_ratio_FvsM)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Overall salary ratio(females/males)") + 
  labs(title = "Overall salary ratio between females and males vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm") 


# Executives mean salary: a bit better the situation for females in this case and less skewed
# create Executives F vs M ratio
salary$salary_ratio_FvsM_Exec <- salary$sal_F_executive / salary$sal_M_executive
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Executives salary ratio (females/males)", y="Density") + 
  labs(title = "Executives salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_Exec)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Executives salary ratio (females/males)") + 
  labs(title = "Executives salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Middle managers mean salary: ....
# create Middle managers F vs M ratio
salary$salary_ratio_FvsM_midManag <- salary$sal_F_midManager / salary$sal_M_midManager
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_Exec)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Middle managers salary ratio (females/males)", y="Density") + 
  labs(title = "Middle managers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs executives mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_midManag)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Middle managers salary ratio (females/males)") + 
  labs(title = "Middle managers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Workers mean salary: ...
# create workers F vs M ratio
salary$salary_ratio_FvsM_worker <- salary$sal_F_worker / salary$sal_M_worker
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_worker)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Workers salary ratio (females/males)", y="Density") + 
  labs(title = "Workers salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs workers mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_worker)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Workers salary ratio (females/males)") + 
  labs(title = "Workers salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# Employee mean salary: ...
# create Employee F vs M ratio
salary$salary_ratio_FvsM_employee <- salary$sal_F_employee / salary$sal_M_employee
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_employee)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="Employee salary ratio (females/males)", y="Density") + 
  labs(title = "Employee salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs Employee mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_employee)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="Employee salary ratio (females/males)") + 
  labs(title = "Employee salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Now the focus is on the salary ratio between women and men among different age groups:
```{r ratio F vs M accross ages}

# 18-25 mean salary: are quite equal apart from some outliers and a quadratic trend
# create 18-25 F vs M ratio
salary$salary_ratio_FvsM_18_25 <- salary$sal_F_18_25 / salary$sal_M_18_25
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_18_25)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="18-25 salary ratio (females/males)", y="Density") + 
  labs(title = "18-25 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 18-25 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")
# scatter plot vs 18-25 mean salary for them
ggplot(salary, aes(x= sal_18_25, y= salary_ratio_FvsM_18_25)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="18-25 salary", y="18-25 salary ratio (females/males)") + 
  labs(title = "18-25 salary ratio between females and males \n vs. overall 18-25 salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "loess")


# 26-50 mean salary: ...
# create 26-50 F vs M ratio
salary$salary_ratio_FvsM_26_50 <- salary$sal_F_26_50 / salary$sal_M_26_50
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_26_50)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="26-50 salary ratio (females/males)", y="Density") + 
  labs(title = "26-50 salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_26_50)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="26-50 salary ratio (females/males)") + 
  labs(title = "26-50 salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")


# 51+ mean salary: ...
# create 51+ F vs M ratio
salary$salary_ratio_FvsM_51plus <- salary$sal_F_51plus / salary$sal_M_51plus
# histogram
ggplot(data=salary, aes(salary$salary_ratio_FvsM_51plus)) + 
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) + 
  geom_density(col="black") + 
  labs(x="51+ salary ratio (females/males)", y="Density") + 
  labs(title = "51+ salary ratio between females and males") + 
  theme(plot.title = element_text(hjust = 0.5)) 
# scatter plot vs 26-50 mean salary
ggplot(salary, aes(x= sal_general, y= salary_ratio_FvsM_51plus)) +  
  geom_point(size = 0.5, colour = "#0091ff")+ 
  labs(x="Overall salary", y="51+ salary ratio (females/males)") + 
  labs(title = "51+ salary ratio between females and males \n vs. overall salary") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm")

```


Highlight bivariate relations:
```{r bivariate plots}

# correlation matrix
corrplot(cor(salary[, 3:ncol(salary)]), method = "circle", title = "Correlation matrix for salary data", 
         diag = T, tl.cex=0.5, type="lower", 
         tl.col = "black", mar=c(0,0,1.5,0)) 

# most general pairs
pairs(salary[c(3:8, 13, 18:20)], gap=0, main = "Scatter matrix of the main variables in salary data", cex = 0.6)
# pairs highlighting genders' differences
pairs(salary[c(9:12, 14:17)], gap=0, main = "Scatter matrix of job categories for both genders", cex = 0.6)

```

## What we have learned

....



# Analyze population data 

## Pre-processing 

Rename the variables for population and exclude the unnecessary ones:
```{r pre processing population}

#names(population)
names(population)[5:7] <-
  c("ageCateg5",
    "sex",
    "peopleCategNum")

# drop unnecessary columns (NIVGEO is the same for all)
population <- subset(population, select = -c(NIVGEO, LIBGEO))

# Refactor sex and MOCO
population$MOCO <- factor(population$MOCO, levels = c(11,12,21,22,23,31,32),
                          labels = c("children_living_with_two_parents", 
                                     "children_living_with_one_parent",
                                     "adults_living_in_couple_without_child",
                                     "adults_living_in_couple_with_children",
                                     "adults_living_alone_with_children",
                                     "persons_not_from_family_living_in_the_home",
                                     "persons_living_alone"))
population$sex <- factor(population$sex, levels = c(1,2), labels = c("Male", "Female"))

# check again
summary(population)

```

Understand the distribution of population according to different age categories using population pyramide
(performed now because later the dataset will be modified):
```{r Population pyramide}

# need the initial shape of data to construct the pyramide 
# Population pyramide
population_data2 <- ddply(population, .(sex, ageCateg5), function(population) {
  data.frame(total_population = sum(population$peopleCategNum))
  })
pop_pyramid <- ggplot(data = population_data2,
       mapping = aes(x = ageCateg5, fill = sex,
                     y = ifelse(test = sex == "Male",
                                yes = -total_population, no = total_population))) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(population_data2$total_population) * c(-1,1)) + 
  ggtitle("Pyramid of Population") + theme(plot.title = element_text(hjust = 0.5)) + 
  labs(x= "Age")+ labs(y = "Population") + coord_flip() # + scale_fill_brewer(palette = "Set1")
pop_pyramid

```

Re-organize population dataset and set CODGEO as units and create new variables using the available variables.
```{r Re-organize population dataset by CODGEO}

# Re-organize the data by creating new variables: "total population", "male", "female", "child", "elderly" and "workforce".
population <- ddply(population, .(CODGEO), function(population) {
  data.frame(total_population = sum(population$peopleCategNum),
             male = sum(population[population$sex == "Male",]$peopleCategNum),
             female = sum(population[population$sex == "Female",]$peopleCategNum),
             child = sum(population[population$ageCateg5 %in% seq(0, 10, by=5),]$peopleCategNum),
             elderly = sum(population[population$ageCateg5 %in% seq(65, 80, by=5),]$peopleCategNum),
             workforce = sum(population[population$ageCateg5 %in% seq(15, 60, by=5),]$peopleCategNum) 
  )})

# Calculate ratios using the existing variables
population$dependent <- population$child + population$elderly
population$sex_ratio <- ifelse(population$female==0, 0, population$male / population$female)
population$dependency_ratio <- ifelse(population$workforce==0, 0, population$dependent / population$workforce)
population$aged_dependency_ratio <- ifelse(population$workforce==0, 0, population$elderly / population$workforce)
population$child_dependency_ratio <- ifelse(population$workforce==0, 0, population$child / population$workforce)

# total population=65mil, which is reasonable
sum(population$total_population)

```

## Descriptive statistics 

Plot population data across different towns:
```{r plotting some population data in log10}

# Histogram of total population per town in log
ggplot(data=population, aes(ifelse(total_population!=0, log10(total_population), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 total population", y="Density") +
  ggtitle("Histogram of total population per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of dependency ratio per town
ggplot(data=population, aes(ifelse(dependency_ratio!=0, log10(dependency_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 dependency ratio", y="Density") +
  ggtitle("Histogram of dependency ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

# Histogram of sex ratio per town
ggplot(data=population, aes(ifelse(sex_ratio!=0, log10(sex_ratio), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="Log10 sex ratio", y="Density") +
  ggtitle("Histogram of sex ratio per town in log10 scale") +
  theme(plot.title = element_text(hjust = 0.5))

```

Understand which towns have the highest concentration of population and ratios.
```{r geo plot for each town}

# Merge geography and population data
# geo_population <- merge(geo, population, by="CODGEO")
geo_population <- cbind.data.frame(geo[unique(geo$CODGEO) %in% unique(population$CODGEO),],
                                   population[unique(population$CODGEO) %in% unique(geo$CODGEO),])

# France map
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")

# Plot "Distribution of total population for each town" 
sc <- scale_colour_gradientn(colours =palette(rainbow(3)), limits=c(min(geo_population$total_population),
                                                                    max(geo_population$total_population)))
population_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude,
                 colour=geo_population$total_population),
             data=geo_population, alpha=0.2, size=0.01) +
             sc + labs(color='') + ggtitle("Distribution of Population for each town")
population_distribution

# Plot "Distribution of aged dependency ratio for each town"
sc <- scale_colour_gradientn(colours =palette(rainbow(3)), limits=c(min(geo_population$aged_dependency_ratio),
                                                                  max(geo_population$aged_dependency_ratio)))
aged_ratio_distribution <-
  FraMap +
  geom_point(aes(x=geo_population$longitude, y=geo_population$latitude,
                 colour=geo_population$aged_dependency_ratio),
             data=geo_population, alpha=0.2, size=0.01) +
             sc + labs(color='') + ggtitle("Aged dependency ratio per town")
aged_ratio_distribution

```


## What we have learned 

...

# Analyze education data

## Pre-processing

Assign meaningful names and build some useful indicators:
```{r}

names(educ)[3:ncol(educ)] <-
  c("Age15_NoDip_M","Age15_NoDip_F", "Age15_Sec_M", "Age15_Sec_F", "Age15_Hi_M","Age15_Hi_F", "Age15_Univ_M",
    "Age15_Univ_F", "Age20_NoDip_M", "Age20_NoDip_F", "Age20_Sec_M","Age20_Sec_F", "Age20_Hi_M", "Age20_Hi_F",
    "Age20_Univ_M", "Age20_Univ_F","Age25_NoDip_M", "Age25_NoDip_F","Age25_Sec_M","Age25_Sec_F", "Age25_Hi_M",
    "Age25_Hi_F","Age25_Univ_M","Age25_Univ_F", "Age30_NoDip_M", "Age30_NoDip_F", "Age30_Sec_M", "Age30_Sec_F",
    "Age30_Hi_M", "Age30_Hi_F","Age30_Univ_M","Age30_Univ_F", "Age35_NoDip_M", "Age35_NoDip_F", "Age35_Sec_M",
    "Age35_Sec_F", "Age35_Hi_M", "Age35_Hi_F", "Age35_Univ_M","Age35_Univ_F", "Age40_NoDip_M", "Age40_NoDip_F",
    "Age40_Sec_M","Age40_Sec_F", "Age40_Hi_M", "Age40_Hi_F", "Age40_Univ_M","Age40_Univ_F", "Age45_NoDip_M",
    "Age45_NoDip_F","Age45_Sec_M","Age45_Sec_F", "Age45_Hi_M", "Age45_Hi_F", "Age45_Univ_M","Age45_Univ_F",
    "Age50_NoDip_M", "Age50_NoDip_F","Age50_Sec_M","Age50_Sec_F", "Age50_Hi_M", "Age50_Hi_F", "Age50_Univ_M",
    "Age50_Univ_F", "Age55_NoDip_M", "Age55_NoDip_F","Age55_Sec_M", "Age55_Sec_F", "Age55_Hi_M", "Age55_Hi_F",
    "Age55_Univ_M", "Age55_Univ_F", "Age60_NoDip_M", "Age60_NoDip_F","Age60_Sec_M","Age60_Sec_F", "Age60_Hi_M",
    "Age60_Hi_F", "Age60_Univ_M", "Age60_Univ_F", "Age65_NoDip_M", "Age65_NoDip_F","Age65_Sec_M","Age65_Sec_F",
    "Age65_Hi_M", "Age65_Hi_F", "Age65_Univ_M", "Age65_Univ_F")

# Aggregate variables by grouping all the education level categories by gender
# total number of females with no diploma
educ$female_NoDip <- rowSums(cbind(educ$Age15_NoDip_F, educ$Age20_NoDip_F, educ$Age25_NoDip_F, educ$Age30_NoDip_F,
                                   educ$Age35_NoDip_F,educ$Age40_NoDip_F,educ$Age45_NoDip_F, educ$Age50_NoDip_F,
                                   educ$Age55_NoDip_F, educ$Age60_NoDip_F, educ$Age65_NoDip_F))

# total number of males with no diploma
educ$male_NoDip <- rowSums(cbind(educ$Age15_NoDip_M, educ$Age20_NoDip_M, educ$Age25_NoDip_M, educ$Age30_NoDip_M,
                                 educ$Age35_NoDip_M,educ$Age40_NoDip_M,educ$Age45_NoDip_M, educ$Age50_NoDip_M,
                                 educ$Age55_NoDip_M, educ$Age60_NoDip_M, educ$Age65_NoDip_M))

# total number of females with secondary education level
educ$female_Sec <- rowSums(cbind(educ$Age15_Sec_F, educ$Age20_Sec_F, educ$Age25_Sec_F, educ$Age30_Sec_F,
                                 educ$Age35_Sec_F,educ$Age40_Sec_F,educ$Age45_Sec_F, educ$Age50_Sec_F, 
                                 educ$Age55_Sec_F, educ$Age60_Sec_F, educ$Age65_Sec_F))

# total number of males with secondary education level
educ$male_Sec <- rowSums(cbind(educ$Age15_Sec_M, educ$Age20_Sec_M, educ$Age25_Sec_M, educ$Age30_Sec_M,
                               educ$Age35_Sec_M,educ$Age40_Sec_M,educ$Age45_Sec_M, educ$Age50_Sec_M, 
                               educ$Age55_Sec_M, educ$Age60_Sec_M, educ$Age65_Sec_M))

# total number of females with high-school education level
educ$female_Hi <- rowSums(cbind(educ$Age15_Hi_F, educ$Age20_Hi_F, educ$Age25_Hi_F, educ$Age30_Hi_F,
                                educ$Age35_Hi_F,educ$Age40_Hi_F,educ$Age45_Hi_F, educ$Age50_Hi_F, 
                                educ$Age55_Hi_F, educ$Age60_Hi_F, educ$Age65_Hi_F))

# total number of males with high-school education level
educ$male_Hi <- rowSums(cbind(educ$Age15_Hi_M, educ$Age20_Hi_M, educ$Age25_Hi_M, educ$Age30_Hi_M,
                              educ$Age35_Hi_M,educ$Age40_Hi_M,educ$Age45_Hi_M, educ$Age50_Hi_M, 
                              educ$Age55_Hi_M, educ$Age60_Hi_M, educ$Age65_Hi_M))

# total number of females with university degree
educ$female_Univ <- rowSums(cbind(educ$Age15_Univ_F, educ$Age20_Univ_F, educ$Age25_Univ_F, educ$Age30_Univ_F,
                                  educ$Age35_Univ_F,educ$Age40_Univ_F,educ$Age45_Univ_F, educ$Age50_Univ_F,
                                  educ$Age55_Univ_F, educ$Age60_Univ_F, educ$Age65_Univ_F))

# total number of males with university degree
educ$male_Univ <- rowSums(cbind(educ$Age15_Univ_M, educ$Age20_Univ_M, educ$Age25_Univ_M, educ$Age30_Univ_M,
                                educ$Age35_Univ_M,educ$Age40_Univ_M,educ$Age45_Univ_M, educ$Age50_Univ_M,
                                educ$Age55_Univ_M, educ$Age60_Univ_M, educ$Age65_Univ_M))

# drop all the unnecessary variables 
educ <- subset(educ, select= c(CODGEO, LIBGEO,
                               female_NoDip, male_NoDip, 
                               female_Sec, male_Sec, 
                               female_Hi, male_Hi, 
                               female_Univ, male_Univ))

# total number of population without a diploma
educ$nodip <-rowSums(cbind(educ$female_NoDip, educ$male_NoDip))

# total number of population with secondary level of education
educ$sec <-rowSums(cbind(educ$female_Sec, educ$male_NoDip))

# total number of population with highschool diploma
educ$hi <-rowSums(cbind(educ$female_Hi, educ$male_Hi))

# total number of poopulation with university diploma
educ$univ <- rowSums(cbind(educ$female_Univ, educ$male_Univ))

# drop LIBGEO
educ <-subset(educ, select = -c(LIBGEO))
```

## Descriptive statistics


# Analyze demographic/social profiles data

## Pre-processing

```{r pre-processing demographic/social profiles data}

# rename variables
names(categ_socio)[3:ncol(categ_socio)] <-
  c("M_immi_agri",
    "F_immi_agri",
    "M_NoImmi_agri",
    "F_NoImmi_agri",
    "M_immi_comm",
    "F_immi_comm",
    "M_NoImmi_comm",
    "F_NoImmi_comm",
    "M_immi_exec",
    "F_immi_exec",
    "M_NoImmi_exec",
    "F_NoImmi_exec",
    "M_immi_midman",
    "F_immi_midman",
    "M_NoImmi_midman",
    "F_NoImmi_midman",
    "M_immi_emp",
    "F_immi_emp",
    "M_NoImmi_emp",
    "F_NoImmi_emp",
    "M_immi_worker",
    "F_immi_worker",
    "M_NoImmi_worker",
    "F_NoImmi_worker",
    "M_immi_retired",
    "F_immi_retired",
    "M_NoImmi_retired",
    "F_NoImmi_retired",
    "M_immi_noAct",
    "F_immi_noAct",
    "M_NoImmi_noAct",
    "F_NoImmi_noAct")

```



```{r Categ_socio-rename variables }

# Create new variables: total number immigrants per town generated by summing up each categories
categ_socio$total_immig <- rowSums(cbind(categ_socio$M_immi_agri, categ_socio$F_immi_agri, categ_socio$M_immi_comm, categ_socio$F_immi_comm, categ_socio$M_immi_exec, categ_socio$F_immi_exec, categ_socio$M_immi_midman, categ_socio$F_immi_midman, categ_socio$M_immi_noAct, categ_socio$F_immi_noAct, categ_socio$M_immi_retired, categ_socio$F_immi_retired, categ_socio$M_immi_emp, categ_socio$F_immi_emp, categ_socio$M_immi_worker, categ_socio$F_immi_worker))
 
# total number of male immigrants per town 
categ_socio$male_immig<-rowSums(cbind(categ_socio$male_immig <- sum(categ_socio$M_immi_agri, categ_socio$M_immi_comm,categ_socio$M_immi_emp, categ_socio$M_immi_emp, categ_socio$M_immi_exec, categ_socio$M_immi_midman, categ_socio$M_immi_noAct, categ_socio$M_immi_retired, categ_socio$M_immi_worker)))
 
# total number of female immigrants per town
categ_socio$female_immig <- rowSums(cbind(categ_socio$F_immi_agri,categ_socio$F_immi_comm, categ_socio$F_immi_emp, categ_socio$F_immi_emp, categ_socio$F_immi_exec, categ_socio$F_immi_midman, categ_socio$F_immi_noAct, categ_socio$F_immi_retired, categ_socio$F_immi_worker))

# total number of natives per town
categ_socio$male_native <- rowSums(cbind(categ_socio$M_NoImmi_agri, categ_socio$M_NoImmi_comm, categ_socio$M_NoImmi_exec, categ_socio$M_NoImmi_emp, categ_socio$M_NoImmi_midman, categ_socio$M_NoImmi_noAct, categ_socio$M_NoImmi_retired, categ_socio$M_NoImmi_worker))

categ_socio$female_native <- rowSums(cbind(categ_socio$F_NoImmi_agri, categ_socio$F_NoImmi_comm, categ_socio$F_NoImmi_exec, categ_socio$F_NoImmi_emp, categ_socio$F_NoImmi_midman, categ_socio$F_NoImmi_noAct, categ_socio$F_NoImmi_retired, categ_socio$F_NoImmi_worker))

# drop LIBGEO
categ_socio <- subset(categ_socio, select=-c(LIBGEO))

```


# Analyze work status data

```{r pre-processing work status data}

# rename variables
names(status_work)[3:ncol(status_work)] <-
  c("Less20_M_wagearner_Full", "Less20_M_wagearner_Half", "Less20_M_Indp1_Full", "Less20_M_Indp1_Half",
    "Less20_M_empl_Full", "Less20_M_empl_Half", "Less20_M_trans_Full", "Less20_M_trans_Half",
    "Less20_F_wagearner_Full", "Less20_F_wagearner_Half", "Less20_F_Indp1_Full", "Less20_F_Indp1_Half",
    "Less20_F_empl_Full", "Less20_F_empl_Half", "Less20_F_trans_Full", "Less20_F_trans_Half",
    "t4_M_wagearner_Full", "t4_M_wagearner_Half", "t4_M_Indp1_Full", "t4_M_Indp1_Half", "t4_M_empl_Full",
    "t4_M_empl_Half", "t4_M_trans_Full", "t4_M_trans_Half", "t4_F_wagearner_Full", "t4_F_wagearner_Half",
    "t4_F_Indp1_Full", "t4_F_Indp1_Half", "t4_F_empl_Full", "t4_F_empl_Half", "t4_F_trans_Full", "t4_F_trans_Half",
    "t9_M_wagearner_Full", "t9_M_wagearner_Half", "t9_M_Indp1_Full", "t9_M_Indp1_Half", "t9_M_empl_Full",
    "t9_M_empl_Half", "t9_M_trans_Full", "t9_M_trans_Half", "t9_F_wagearner_Full", "t9_F_wagearner_Half",
    "t9_F_Indp1_Full", "t9_F_Indp1_Half", "t9_F_empl_Full", "t9_F_empl_Half", "t9_F_trans_Full", "t9_F_trans_Half",
    "T4_M_wagearner_Full", "T4_M_wagearner_Half", "T4_M_Indp1_Full", "T4_M_Indp1_Half", "T4_M_empl_Full",
    "T4_M_empl_Half", "T4_M_trans_Full", "T4_M_trans_Half", "T4_F_wagearner_Full", "T4_F_wagearner_Half",
    "T4_F_Indp1_Full", "T4_F_Indp1_Half", "T4_F_empl_Full", "T4_F_empl_Half", "T4_F_trans_Full", "T4_F_trans_Half",
    "T9_M_wagearner_Full", "T9_M_wagearner_Half", "T9_M_Indp1_Full", "T9_M_Indp1_Half", "T9_M_empl_Full",
    "T9_M_empl_Half", "T9_M_trans_Full", "T9_M_trans_Half", "T9_F_wagearner_Full", "T9_F_wagearner_Half",
    "T9_F_Indp1_Full", "T9_F_Indp1_Half", "T9_F_empl_Full", "T9_F_empl_Half", "T9_F_trans_Full", "T9_F_trans_Half",
    "FF_M_wagearner_Full", "FF_M_wagearner_Half", "FF_M_Indp1_Full", "FF_M_Indp1_Half", "FF_M_empl_Full",
    "FF_M_empl_Half", "FF_M_trans_Full", "FF_M_trans_Half", "FF_F_wagearner_Full", "FF_F_wagearner_Half",
    "FF_F_Indp1_Full", "FF_F_Indp1_Half", "FF_F_empl_Full", "FF_F_empl_Half", "FF_F_trans_Full", "FF_F_trans_Half",
    "F9_M_wagearner_Full", "F9_M_wagearner_Half", "F9_M_Indp1_Full", "F9_M_Indp1_Half", "F9_M_empl_Full",
    "F9_M_empl_Half", "F9_M_trans_Full", "F9_M_trans_Half", "F9_F_wagearner_Full", "F9_F_wagearner_Half",
    "F9_F_Indp1_Full", "F9_F_Indp1_Half", "F9_F_empl_Full", "F9_F_empl_Half", "F9_F_trans_Full", "F9_F_trans_Half",
    "f4_M_wagearner_Full", "f4_M_wagearner_Half", "f4_M_Indp1_Full", "f4_M_Indp1_Half", "f4_M_empl_Full",
    "f4_M_empl_Half", "f4_M_trans_Full", "f4_M_trans_Half", "f4_F_wagearner_Full", "f4_F_wagearner_Half",
    "f4_F_Indp1_Full", "f4_F_Indp1_Half", "f4_F_empl_Full", "f4_F_empl_Half", "f4_F_trans_Full", "f4_F_trans_Half",
    "f9_M_wagearner_Full", "f9_M_wagearner_Half", "f9_M_Indp1_Full", "f9_M_Indp1_Half", "f9_M_empl_Full",
    "f9_M_empl_Half", "f9_M_trans_Full", "f9_M_trans_Half", "f9_F_wagearner_Full", "f9_F_wagearner_Half",
    "f9_F_Indp1_Full", "f9_F_Indp1_Half", "f9_F_empl_Full", "f9_F_empl_Half", "f9_F_trans_Full", "f9_F_trans_Half",
    "s4_M_wagearner_Full", "s4_M_wagearner_Half", "s4_M_Indp1_Full", "s4_M_Indp1_Half", "s4_M_empl_Full",
    "s4_M_empl_Half", "s4_M_trans_Full", "s4_M_trans_Half", "s4_F_wagearner_Full", "s4_F_wagearner_Half",
    "s4_F_Indp1_Full", "s4_F_Indp1_Half", "s4_F_empl_Full", "s4_F_empl_Half", "s4_F_trans_Full", "s4_F_trans_Half",
    "s9_M_wagearner_Full", "s9_M_wagearner_Half", "s9_M_Indp1_Full", "s9_M_Indp1_Half", "s9_M_empl_Full",
    "s9_M_empl_Half", "s9_M_trans_Full", "s9_M_trans_Half", "s9_F_wagearner_Full", "s9_F_wagearner_Half",
    "s9_F_Indp1_Full", "s9_F_Indp1_Half", "s9_F_empl_Full", "s9_F_empl_Half", "s9_F_trans_Full", "s9_F_trans_Half")

```

```{r Status drop and rename variables}

# Simplify the variables by summing up by categories
# total number of wage earners for males and females.
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('wagearner', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$wagearner_M=rowSums(status_work[,nam.M])
status_work$wagearner_F=rowSums(status_work[,nam.F]) 
                                               
# same but for independent workers by gender
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Indp1', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$independent_M=rowSums(status_work[,nam.M]) 
status_work$independent_F=rowSums(status_work[,nam.F])

# same but for government transfer receivers
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('trans', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$transfer_M=rowSums(status_work[,nam.M])  
status_work$transfer_F=rowSums(status_work[,nam.F])

# same but for employers
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('empl', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$employer_M=rowSums(status_work[,nam.M])
status_work$employer_F=rowSums(status_work[,nam.F])
 
# same but for full-time contracts
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Full', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
# names(status_work[nam.M])
# names(status_work[nam.F])
status_work$full_M=rowSums(status_work[,nam.M])
status_work$full_F=rowSums(status_work[,nam.F])

# same but for half-time contracts
nam.M = rep(0, ncol(status_work))
nam.F = rep(0, ncol(status_work))
for (i in 1:ncol(status_work)){
  sol = grepl('Half', names(status_work)[i])
  if (sol==1){
    if (grepl('M', names(status_work)[i])){
      nam.M[i] = i  
    }else{nam.F[i]=i}
  }else{
    nam.F[i]=0
    nam.M[i]=0}
}
#names(status_work[nam.M])
#names(status_work[nam.F])
status_work$half_M=rowSums(status_work[,nam.M])
status_work$half_F=rowSums(status_work[,nam.F])

# total of each variables
status_work$full <- status_work$full_M + status_work$full_F
status_work$half <- status_work$half_M + status_work$half_F
status_work$wagearner <- status_work$wagearner_M + status_work$wagearner_F
status_work$independent <- status_work$independent_M + status_work$independent_F
status_work$transfer <- sum(status_work$transfer_F, status_work$transfer_M)
status_work$employer <- sum(status_work$employer_F, status_work$employer_M)

# drop unnecessary variables
status_work <- subset(status_work, select = c(CODGEO,
                                              wagearner_M, wagearner_F, 
                                              independent_M, independent_F, 
                                              transfer_M, transfer_F, 
                                              employer_M, employer_F, 
                                              full_M, full_F,
                                              half_M, half_F,
                                              full, half,
                                              wagearner, independent, transfer, employer))
```


## Descriptive statistics

```{r descriptive stats status_work}

summary(status_work)

ggplot(data=status_work, aes(ifelse(full_F!=0, log(full_F), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="", y="Density") +
  ggtitle("") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data=status_work, aes(ifelse(full_M!=0, log(full_M), 0))) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 50) +
  geom_density(col="black") +
  labs(x="", y="Density") +
  ggtitle("") +
  theme(plot.title = element_text(hjust = 0.5))
```


# Analyze inequality data

```{r pre-processing inequality data}
names(ineq)

# drop unnecessary variables
ineq <- subset(ineq, select = -c(LIBGEO, P09_POP, NAISD16, DECESD16, P14_RP_PROP, P09_EMPLT,ETTOT15, ETAZ15, ETBE15, ETFZ15, ETGU15, ETGZ15, ETOQ15, ETTEF115, ETTEFP1015))

# rename variables 
names(ineq)[5:ncol(ineq)] <-
  c("pop_2014",
    "Superficie",
    "birth09_14",
    "death09_14",
    "households14",
    "housing14",
    "princ_resid14",
    "sec_resid14",
    "vac_resid14",
    "tax_house14",
    "shared_tax_house14",
    "median_living14",
    "lev_ineq14",
    "empl",
    "emp_sal",
    "pop15_64",
    "unemp15_64",
    "act15_64")

# drop more unnecessary variables 
ineq <- subset(ineq, select = -c(princ_resid14, sec_resid14, vac_resid14))

```


# Analyze commune data
```{r}
names(commune)

#Drop unnecessary variables
commune <-subset(commune, select=-c(libgeo, scorefiscal, dep, reg, evolutionpop, nbpropritaire, 
                                    nblogementsecondaireetoccasionne, scoreurbanit, scorevargion, scorepib,  
                                    nbrsidencesprincipales, indicefiscalpartiel, nboccupantsrsidenceprincipale, 
                                    depmoyennesalaireshoraires, depmoyennesalairescadrehoraires, 
                                    depmoyennesalairesprofintermdiai, depmoyennesalairesemployhoraires, 
                                    depmoyennesalairesouvrihoraires, valeurajoutergionale, 
                                    nbindustriesdesbiensintermdiaire, nbdecommerce, nbdeservicesauxparticuliers, 
                                    nbinstitutiondeeducationsantacti, scorevargion, scorepib, 
                                    moyennerevenusfiscauxdpartementa, moyennerevenusfiscauxrgionaux, 
                                    regmoyennesalaireshoraires, regmoyennesalairescadrehoraires, 
                                    regmoyennesalairesprofintermdiai, regmoyennesalairesemployhoraires, 
                                    regmoyennesalairesouvrihoraires, tauxtudiants, dynamiquedmographiqueinsee, 
                                    capacitfisc, capacitfiscale, moyennerevnusfiscaux, nblogement, fidlit)) 


# rename variables  
names(commune) <-c("CODGEO", "orient_econ", "demo_index", "population", "nb_household", 
                    "nb_female", "nb_male", "nb_minor", "nb_major", "nb_students",  
                    "nb_firms_service", "nb_firms_commerce", "nb_firms_construction", 
                    "nb_firms_ind", "nb_created_firms", "nb_created_ind", 
                    "nb_created_construction", "nb_created_commerce",
                    "nb_created_services", "urbanRural", "nb_active", "nb_active_employees",
                    "nb_active_non_employees", "dymanic_demo_prof", "profitRate", 
                    "regional_GDP", "demo_environment", "evol_pop_score", 
                    "evol_entrepreneurial_score", "demo_envir2") 
```


# Produce consistent datasets

The CODGEO variables have to be merged.
However, for different reasons already identified by other kaggle users they need some pre-processing.
To do so, some already known mistakes are corrected:

```{r fix and match CODGEO}

firms$CODGEO       <- sub("A", "0", firms$CODGEO)
firms$CODGEO       <- sub("B", "0", firms$CODGEO)
salary$CODGEO      <- sub("A", "0", salary$CODGEO)
salary$CODGEO      <- sub("B", "0", salary$CODGEO)
population$CODGEO  <- sub("A", "0", population$CODGEO)
population$CODGEO  <- sub("B", "0", population$CODGEO)
geo$CODGEO         <- sub("A", "0", geo$CODGEO)
geo$CODGEO         <- sub("B", "0", geo$CODGEO)
educ$CODGEO        <- sub("A", "0", educ$CODGEO)
educ$CODGEO        <- sub("B", "0", educ$CODGEO)
categ_socio$CODGEO <- sub("A", "0", categ_socio$CODGEO)
categ_socio$CODGEO <- sub("B", "0", categ_socio$CODGEO)
ineq$CODGEO        <- sub("A", "0", ineq$CODGEO)
ineq$CODGEO        <- sub("B", "0", ineq$CODGEO)
status_work$CODGEO <- sub("A", "0", status_work$CODGEO)
status_work$CODGEO <- sub("B", "0", status_work$CODGEO)
commune$CODGEO     <- sub("A", "0", commune$CODGEO)
commune$CODGEO     <- sub("B", "0", commune$CODGEO)

# Then all CODGEO are trasformed to integers and four new datasets are created retaining only the common CODGEO:
# use only integer values
geo$CODGEO         <- as.integer(geo$CODGEO)  
population$CODGEO  <- as.integer(population$CODGEO)
firms$CODGEO       <- as.integer(firms$CODGEO)
salary$CODGEO      <- as.integer(salary$CODGEO)
status_work$CODGEO <- as.integer(status_work$CODGEO)  
ineq$CODGEO        <- as.integer(ineq$CODGEO)
educ$CODGEO        <- as.integer(educ$CODGEO)
categ_socio$CODGEO <- as.integer(categ_socio$CODGEO)
commune$CODGEO     <- as.integer(commune$CODGEO)

# store datasets' names to loop on them
dataset = c("population", "salary", "firms", "geo", "ineq", "commune", "educ", "categ_socio", "status_work")

# obtain sommon IDs for all datasets
for (i in dataset){
  # get i-th name and create a new variable concateneting "NEW" at the end
  nam <- paste(i, "NEW", sep = "")
  # initialize counter to identify the number of iteration in j
  iter = 1
  for (j in dataset){
    if (j != i){
      # for each dataset different from the i-th
      if (iter == 1){
        # 1st iteration: use the original dataset (e.g., geo)
        assign(nam, semi_join(get(i), get(j), by = "CODGEO"))
      } else{
        # successive iteration: use the new dataset (e.g., geoNEW)
        assign(nam, semi_join(get(nam), get(j), by = "CODGEO"))
      }
      iter = iter + 1
    }
  }
}

# check how many observation have been deleted
for (i in dataset){
  del_rows = nrow(get(i)) - nrow(get(paste(i, "NEW", sep = "")))
  del_prop = del_rows / nrow(get(i))
  del_obs = paste("For", i, del_rows, "units have been deleted.",
                  "They were the", round(del_prop*100, digits=2), "% of the total.", sep = " ")
  print(paste(del_obs))
}

print(paste("The new dataset has", nrow(salaryNEW), "units and", 
      ncol(salaryNEW)+ncol(populationNEW)+ncol(firmsNEW)+ncol(geoNEW)+
        ncol(status_workNEW)+ncol(ineqNEW)+ncol(educNEW)+ncol(categ_socioNEW)+ncol(communeNEW), "features."))

```


# Merge the data:
```{r Merge the data and create csv file}

# merging
newDat = merge(firmsNEW, populationNEW, by="CODGEO")
newDat = merge(newDat, salaryNEW, by="CODGEO")
newDat = merge(newDat, geoNEW, by="CODGEO")
newDat = merge(newDat, status_workNEW, by="CODGEO")
newDat = merge(newDat, ineqNEW, by="CODGEO")
newDat = merge(newDat, educNEW, by="CODGEO")
newDat = merge(newDat, categ_socioNEW, by="CODGEO")
newDat = merge(newDat, communeNEW, by="CODGEO")
# newDat = subset(newDat, select = -town.y)

# check
names(newDat)
head(newDat)

# NewDat csv file created
# write.csv(newDat, "newDat.csv", quote = FALSE, row.names=FALSE, fileEncoding = "UTF-8")
```


Spatial plot of the data to spot possible patterns:
```{r newDat spatial plot}

ggmap(get_googlemap(center=fra_center, scale=2, zoom=5), extent="normal") +
  geom_point(aes(x=longitude, y=latitude), data=newDat, col="orange", alpha=0.2, size=0.01) 

```



# Analysis 

General:
```{r Correlation plot with new merged data}
# corrplot(cor(newDat[, 3:20]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", type = "lower", mar=c(0,0,1.5,0))
# corrplot(cor(newDat[, 21:41]), method = "circle", title = "Correlation matrix for firms", 
#          diag = F, tl.cex=1, type = "lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1.5,0))
```


Firms and geo:
```{r}

```
## PCA 
```{r}

```
## Regression
```{r salary for executives}
# average salary for executives
average_sal_F_exec <- mean(newDat$sal_F_executive)
average_sal_M_exec <- mean(newDat$sal_M_executive)
# -> average salary gap between females and males (executives) are 20.20613 - 25.19371= -4.98758.
# note that the max variable for sal_M_executive is 58 for female it is 35.5, nearly double. 
# Strangely, for middle manager male, the max is 93.4 which is in Ile-de-France en Yvelines, Chambourcy. 

# delta salary female and male
newDat$sal_MF_exec <- newDat$sal_M_executive - newDat$sal_F_executive

# linear regression
lm=lm(sal_MF_exec~ univ + nodip + ,  data=newDat, subset=set.seed(2018))
summary(lm)



```
## Clustering 
```{r}

```

## Supervised Learning data
```{r}

```




# Chiaromonte's part

## Unsupervised

Preliminary multivariate salary data visualisation to better conduct the unsupervised learning  
```{r}

# add region variable to the salary dataset
salary_with_dep <- cbind(geoNEW[1], salaryNEW)
#check
#ncol(salary_with_dep)
#head(salary_with_dep)
# extract continuous variables
salary_variables <- salary_with_dep[, 4:35]

#scatterplot to visually see the correlations between variables
pairs(salary_variables[1:8], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#scatterplot between variables of major interest
pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#bivariate densities among general, males and females salaries
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, 
       breaks[-1], y, col="grey", ...)
}


pairs(salary_variables[, c("salary_ratio_FvsM", "sal_general", "sal_Females","sal_Males")],
      diag.panel = panel.hist,
      panel = function (x,y) {
        data <- data.frame(cbind(x,y))
        par(new = TRUE)
        den <- bkde2D(data, 
                      bandwidth=sapply(data,dpik))
        contour(x = den$x1, y = den$x2, 
                z = den$fhat, axes = FALSE)
      })

#scale the data
salary_variables_scaled <- as.data.frame(scale(salary_variables))
#divide sal_general into 4 parts
salary_levels <- with(salary_variables_scaled, 
                  equal.count(sal_general,4))
#make a 3D plot of salary_ratio_FvsM against sal_26_50 and sal_employee given salary_levels
plot(cloud(salary_ratio_FvsM ~ sal_26_50 * sal_employee | salary_levels, 
           panel.aspect = 0.5,
           data = salary_variables_scaled))

#plot sal_worker against sal_midManager given 3 partitions of sal_general
plot(xyplot(sal_worker ~ sal_midManager| cut(sal_general, 3), 
            data = salary_variables, 
            layout = c(3, 1), 
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))

#multidimesnional plot of midManager salary against worker salary, given 4 partitions of F/M salary ratio, and grayscale intensities of general salary
M_F_salary_ratio  <- with(salary_variables, equal.count(salary_ratio_FvsM, 4))
general_salary.ord <- with(salary_variables, rev(order(sal_general)))
salary_variables.ordered <- salary_variables[general_salary.ord,]
sal_general.breaks <- with(salary_variables.ordered, 
                     do.breaks(range(sal_general),50))
salary_variables.ordered$color<-level.colors(salary_variables.ordered$sal_general,
                                   at=sal_general.breaks,
                                   col.regions=grey.colors)
plot(xyplot(sal_worker ~ sal_midManager | M_F_salary_ratio, 
            data = salary_variables.ordered,
            aspect = "iso", 
            groups = color, 
            cex = 1, col = "black",
       panel = function(x, y, groups, ..., subscripts) {
           fill <- groups[subscripts]
           panel.grid(h = -1, v = -1)
           panel.xyplot(x, y, pch = 21, 
                        fill = fill, ...)
       },
       legend =
       list(right =
            list(fun = draw.colorkey,
                 args=list(key=list(col=gray.colors,
                                    at = sal_general.breaks),
                           draw = FALSE))),
            xlab = "Worker Salary", 
            ylab = "Midmanager salary"))


#plot(cloud(sal_general ~ sal_worker * salary_ratio_FvsM | sal_26_50, 
#           data = salary_variables,
#           zlim = rev(range(salary_variables$sal_general)),
#           screen = list(z = 5, x = -3), 
#           panel.aspect = 0.9,
#           xlab = "Worker salary", 
#           ylab = "M_F_salary_ratio", 
#           zlab = "General salary"))



```

Preliminary multivariate firms data visualisation to better conduct the unsupervised learning 
```{r}
# add region variable to the salary dataset
firms_with_dep <- cbind(geoNEW[1], firmsNEW)
#check
#ncol(salary_with_dep)
#head(salary_with_dep)
# extract continuous variables
firms_variables <- firms_with_dep[, 4:9]

#scatterplot to visually see the correlations between variables
pairs(firms_variables[, 2:5], 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)


#bivariate densities among general, males and females salaries
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, 
       breaks[-1], y, col="grey", ...)
}


pairs(firms_variables[, 2:5],
      diag.panel = panel.hist,
      panel = function (x,y) {
        data <- data.frame(cbind(x,y))
        par(new = TRUE)
        den <- bkde2D(data, 
                      bandwidth=sapply(data,dpik))
        contour(x = den$x1, y = den$x2, 
                z = den$fhat, axes = FALSE)
      })

#scale the data
firms_variables_scaled <- as.data.frame(scale(firms_variables))
#divide firms total into 4 parts
firms_levels <- with(firms_variables_scaled, 
                  equal.count(total,4))
#make a 3D plot of large against micro and small given firms_levels
plot(cloud(large ~ micro * small | firms_levels, 
           panel.aspect = 0.5,
           data = firms_variables))

#plot sal_worker against sal_midManager given 3 partitions of sal_general
plot(xyplot(medium ~ large| cut(total, 3), 
            data = firms_variables, 
            layout = c(3, 1), 
            xlab = "medium firms", 
            ylab = "large firms"))

#multidimesnional plot of large firms against micro firms, given 4 partitions of medium, and grayscale intensities of total
#medium_firms  <- with(firms_variables, equal.count(medium, 4))
#total.ord <- with(firms_variables, rev(order(total)))
#firms_variables.ordered <- firms_variables[total.ord,]
#total.breaks <- with(firms_variables.ordered, 
                     #do.breaks(range(total),10))
#firms_variables.ordered$color<-level.colors(firms_variables.ordered$total,
                                  # at=total.breaks,
                                   #col.regions=grey.colors)
#plot(xyplot(large ~ micro | medium, 
            #data = firms_variables.ordered,
            #aspect = "iso", 
            #groups = color, 
            #cex = 1, col = "black",
       #panel = function(x, y, groups, ..., subscripts) {
           #fill <- groups[subscripts]
          #panel.grid(h = -1, v = -1)
           #panel.xyplot(x, y, pch = 11, 
                       #fill = fill, ...)
       #},
       #legend =
       #list(right =
            #list(fun = draw.colorkey,
                 #args=list(key=list(col=gray.colors,
                                    #at = total.breaks),
                           #draw = FALSE))),
            #xlab = "micro firms", 
            #ylab = "large firms"))


#plot(cloud(sal_general ~ sal_worker * salary_ratio_FvsM | sal_26_50, 
#           data = salary_variables,
#           zlim = rev(range(salary_variables$sal_general)),
#           screen = list(z = 5, x = -3), 
#           panel.aspect = 0.9,
#           xlab = "Worker salary", 
#           ylab = "M_F_salary_ratio", 
#           zlab = "General salary"))


```


Multi-dimensional scaling (MDS) on salary data
```{r}
# produces a list with covariance matrix
# for each region as component:
salary_var <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
                     function(i) var(salary_with_dep[i,4:35]))
str(salary_var)

#remove regions with null values
salary_var <- salary_var[-c(11, 12, 15, 19, 20, 28)]
str(salary_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(salary_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(salary_var[[1]])
# creates common covariance matrix S
s <- 0
for (v in c(1:22)) S <- S + regions_count_sub$Freq[v] * as.matrix(salary_var[[v]]) 
S <- S / 5021

# finds center of each variable (sal_general, etc)
# for each region
salary_cen <- tapply(1:nrow(salary_with_dep), salary_with_dep$region, 
    function(i) apply(salary_with_dep[i,-c(1:3)], 2, mean))
salary_cen
salary_cen <- salary_cen[-c(11, 12, 15, 19, 20, 28)]
str(salary_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
salary_cen <- matrix(unlist(salary_cen), 
    nrow = length(salary_cen), byrow = TRUE)
salary_cen

# compute the mahalanobis distances:
salary_mah <- apply(salary_cen, 1, 
    function(cen) mahalanobis(salary_cen, cen, S))
salary_mah



# run it on two dimensions
salary_mds <- cmdscale(salary_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(salary_mds) * 1.2
plot(salary_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(salary_mds, labels = levels(salary_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
salary_mds <- cmdscale(salary_mah, k = nrow(salary_mah) - 1, 
         eig = TRUE)

#only 9 of the first 21 eigenvalues are > 0
# x <- salary_mds$points[,1]
# y <- salary_mds$points[,2]
# st <- mst(salary_mah)
# plot(x, y, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = range(x)*1.2, type = "n")
# for (i in 1:nrow(salary_mah)) {
#   w1 <- which(st[i, ] == 1)
#   segments(x[i], y[i], x[w1], y[w1])
# }
# text(x, y, labels = colnames(salary_mah), 
#      cex = 1.5)

```

Multi-dimensional scaling (MDS) on firms data
```{r}
firms_with_dep <- firms_with_dep[, -9]
# produces a list with covariance matrix
# for each region as component:
firms_var <- tapply(1:nrow(firms_with_dep), firms_with_dep$region, 
                     function(i) var(firms_with_dep[i,5:8]))
#str(firms_var)

#remove regions with null values
firms_var <- firms_var[-c(11, 12, 15, 19, 20, 28)]
#check
#str(firms_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(firms_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(firms_var[[1]])

# creates common covariance matrix S
s <- 0
for (v in c(1:5)) S <- S + regions_count_sub$Freq[v] * as.matrix(firms_var[[v]]) 
S <- S / 5021

# finds center of each variable (micro, etc)
# for each region
firms_cen <- tapply(1:nrow(firms_with_dep), firms_with_dep$region, 
    function(i) apply(firms_with_dep[i,-c(1:4)], 2, mean))
firms_cen
firms_cen <- firms_cen[-c(11, 12, 15, 19, 20, 28)]
str(firms_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
firms_cen <- matrix(unlist(firms_cen), 
    nrow = length(firms_cen), byrow = TRUE)
firms_cen

# compute the mahalanobis distances:
firms_mah <- apply(firms_cen, 1, 
    function(cen) mahalanobis(firms_cen, cen, S))
firms_mah



# run it on two dimensions
firms_mds <- cmdscale(firms_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(firms_mds) * 1.2
plot(firms_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(firms_mds, labels = levels(firms_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
firms_mds <- cmdscale(firms_mah, k = nrow(firms_mah) - 1, 
         eig = TRUE)

#only 5 of the first 21 eigenvalues are > 0
# x <- firms_mds$points[,1]
# y <- firms_mds$points[,2]
# st <- mst(firms_mah)
# plot(x, y, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = range(x)*1.2, type = "n")
# for (i in 1:nrow(firms_mah)) {
#   w1 <- which(st[i, ] == 1)
#   segments(x[i], y[i], x[w1], y[w1])
# }
# text(x, y, labels = colnames(firms_mah), 
#      cex = 1.5)
```


PCA on salary data and delta M-F
```{r}

pca_salary <- prcomp(salary_variables)
autoplot(pca_salary, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary, type = "l")
#pca_salary
#summary(pca_salary)

#create dataset with M-F differences
salary_var_delta <- data.frame("?? M-F" = salary_variables$sal_Males -                                                           salary_variables$sal_Females,
                     "?? Executive M-F" = salary_variables$sal_M_executive -
                                         salary_variables$sal_F_executive,
                    "?? midManager M-F" = salary_variables$sal_M_midManager -
                                         salary_variables$sal_F_midManager,
                      "?? employee M-F" = salary_variables$sal_M_employee -
                                         salary_variables$sal_F_employee,
                        "?? worker M-F" = salary_variables$sal_M_worker -
                                         salary_variables$sal_F_worker,
                         "?? 18-25 M-F" = salary_variables$sal_M_18_25 -
                                         salary_variables$sal_F_18_25,
                         "?? 26-50 M-F" = salary_variables$sal_M_26_50 -
                                         salary_variables$sal_F_26_50,
                       "?? 51 plus M-F" = salary_variables$sal_M_51plus -
                                         salary_variables$sal_F_51plus)
pca_salary_delta <- prcomp(salary_var_delta)
autoplot(pca_salary_delta, data = salary_with_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_salary_delta, type = "l")
#pca_salary_delta
#summary(pca_salary_delta)



```


Cluster Analysis on salary and delta M-F
```{r}


#create dataframe with first two principal components scores as variables
salary_pca_scores<- as.data.frame(pca_salary$x[,1:2])
salary_pca_scores <- as.data.frame(scale(salary_pca_scores))
sapply(salary_pca_scores, var)
#scatterplot between PC1 and PC2 to see if there are outliers
pairs(salary_pca_scores, 
      panel = function (x, y, ...) {
          points(x, y, ...)
          abline(lm(y ~ x), col = "red")
      }, pch = ".", cex = 0.5)

#after visually identifying thresholds, get rid of outliers
outliers <- subset(salary_pca_scores, PC1 > 8 | PC2 > 13)
outliers_names <- unlist(as.numeric(rownames(outliers)))
salary_pca_scores <- salary_pca_scores[-outliers_names,]
#renumber rows
row.names(salary_pca_scores) <- 1:nrow(salary_pca_scores)

#sample data
set.seed(123)
salary_scores_sampled <- sample_n(salary_pca_scores, 1000)

#renumber rows
row.names(salary_scores_sampled) <- 1:nrow(salary_scores_sampled)

#create distance matrix for hclust
dm <- dist(salary_scores_sampled)
#round to two decimals
dm <- round(dm, 2)

# Below we show the cluster solutions for
# salary data. The top row gives the cluster
# dendograms along with the cutoff used to
# derive the classes presented (in the space
# of the two principal components) in the
# bottom row:


salary_pc <- princomp(dm, cor = TRUE)
layout(matrix(1:6, nr = 2), height = c(2, 1))
plot(cs <- hclust(dm, method = "single"), 
     main = "Single")
abline(h = 3.8, col = "lightgrey")
xlim <- range(salary_pc$scores[,1])
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cs, h = 3.8)
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)
plot(cc <- hclust(dm, method = "complete"), 
     main = "Complete")
abline(h = 7, col = "lightgrey")
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cc, h = 7)  
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)     
plot(ca <- hclust(dm, method = "average"), 
     main = "Average")
abline(h = 7.8, col = "lightgrey")
plot(salary_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(ca, h = 7.8)                             
text(salary_pc$scores[,1:2], labels = lab, 
     cex=0.6)  

# restore layout 
layout(matrix(1, nr = 1)) 
 
# K-means clustering
n <- nrow(salary_pca_scores)
wss <- rep(0, 22)
wss[1] <- (n - 1) * sum(sapply(salary_pca_scores, var))
for (i in 2:22)
    wss[i] <- sum(kmeans(salary_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 5
plot(1:22, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 5 groups 
plot(salary_pca_scores, 
     pch = kmeans(salary_pca_scores, 
                  centers = 5)$cluster)

#now we have to see whether these groups correspond to geographical regions...

```

#MAPPING THE CLUSTERS
```{r}
 
#now we have to see whether these groups correspond to geographical regions...
clusters <- kmeans(salary_pca_scores, 5)
salary_with_dep$cluster <- clusters$cluster
geoNEW$cluster <- clusters$cluster
 
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
salary_geo <-
  FraMap +
  geom_point(aes(x=geoNEW$longitude, y=geoNEW$latitude, colour= factor(geoNEW$cluster)),
             data=geoNEW, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on salary")
salary_geo
 
 
#salary_with_dep$cluster <- clusters$cluster
#data("world.cities")
#France <- world.cities %>% filter(country.etc == "France")
#EPS <- 0.15
#clusters <- dbscan(France[, 4:5], eps = EPS)
#+France$cluster <- clusters$cluster
 
#groups  <- France %>% filter(cluster != 0)
#noise  <- France %>% filter(cluster == 0)
 
#ggplot(France, aes(x = long, y = lat, alpha = 0.5)) + 
#  geom_point(aes(fill = "grey"), noise) +
#  geom_point(aes(colour = as.factor(cluster)), groups,
 #            size = 3) +
#  coord_map() +
#  theme(legend.position = "none")
 
```

#PCA on merged dataset
```{r}
drops <- c("CODGEO","town.x", "total", "null", "total_population", "town.y", "region", "region_capital", "department", "town_name", "postal_code", "latitude", "longitude", "LIBGEO.x", "LIBGEO.y", "REG", "DEP", "pop_2014", "transfer_M", "transfer_F", "M_immi_agri", "F_immi_agri", "male_immig", "tax_house14", "shared_tax_house14", "median_living14", "lev_ineq14")


newDat_variables <- newDat[ , !(names(newDat) %in% drops)]
newDat_variables <- select_if(newDat_variables, is.numeric)
newDat_variables <- sapply(newDat_variables, as.numeric)
newDat_variables <- as.data.frame(newDat_variables)



names(which(sapply(newDat_variables, anyNA)))

#str(newDat_variables)
newDat_variables_dep <- cbind(geoNEW[1], newDat_variables)

#pca_newDat <- prcomp(na.omit(newDat_variables), scale=TRUE)
pca_newDat <- prcomp(newDat_variables)
str(pca_newDat)


autoplot(pca_newDat, data = newDat_variables_dep, colour = 'region',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
plot(pca_newDat, type = "l")

#pca_newDat



```
#clustering on merged dataset
```{r}
#create dataframe with first three principal components scores as variables
newDat_pca_scores<- as.data.frame(pca_newDat$x[,1:3])
newDat_pca_scores <- as.data.frame(scale(newDat_pca_scores))
sapply(newDat_pca_scores, var)

#sample data
set.seed(123)
newDat_scores_sampled <- sample_n(newDat_variables, 1000)

#renumber rows
row.names(newDat_scores_sampled) <- 1:nrow(newDat_scores_sampled)

#create distance matrix for hclust
dm <- dist(newDat_scores_sampled)
#round to two decimals
dm <- round(dm, 2)

# Below we show the cluster solutions for
# the merged data. The top row gives the cluster
# dendograms along with the cutoff used to
# derive the classes presented (in the space
# of the three principal components) in the
# bottom row:


newDat_pc <- princomp(dm, cor = TRUE)
layout(matrix(1:6, nr = 2), height = c(2, 1))
plot(cs <- hclust(dm, method = "single"), 
     main = "Single")
abline(h = 3.8, col = "lightgrey")
xlim <- range(newDat_pc$scores[,1])
plot(newDat_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cs, h = 3.8)
text(newDat_pc$scores[,1:2], labels = lab, 
     cex=0.6)
plot(cc <- hclust(dm, method = "complete"), 
     main = "Complete")
abline(h = 7, col = "lightgrey")
plot(newDat_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cc, h = 7)  
text(newDat_pc$scores[,1:2], labels = lab, 
     cex=0.6)     
plot(ca <- hclust(dm, method = "average"), 
     main = "Average")
abline(h = 7.8, col = "lightgrey")
plot(newDat_pc$scores[,1:2], type = "n", 
     xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(ca, h = 7.8)                             
text(newDat_pc$scores[,1:2], labels = lab, 
     cex=0.6)  

# restore layout 
layout(matrix(1, nr = 1)) 
 
# K-means clustering
n <- nrow(newDat_pca_scores)
wss <- rep(0, 15)
wss[1] <- (n - 1) * sum(sapply(newDat_pca_scores, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(newDat_pca_scores,
                         centers = i)$withinss)
#we can see from the plot that optimal number of cluster is 11
plot(1:15, wss, type = "b", 
     xlab = "Number of groups",
     ylab = "Within groups sum of squares")

#plotting in PC space shows us that there are indeed distinguishable 6 groups 
plot(newDat_pca_scores, 
     pch = kmeans(newDat_pca_scores, 
                  centers = 6)$cluster)

#now we have to see whether these groups correspond to geographical regions...
```

#mapping the clusters
```{r}
#now we have to see whether these groups correspond to geographical regions...
clusters <- kmeans(newDat_pca_scores, 10)
newDat_variables_dep$cluster <- clusters$cluster
geoNEW$cluster <- clusters$cluster
 
FraMap = ggmap(get_googlemap(center=fra_center, scale=2, zoom=6), extent="normal")
 
# Plot "Distribution of total population for each town" 
newDat_geo <-
  FraMap +
  geom_point(aes(x=geoNEW$longitude, y=geoNEW$latitude, colour= factor(geoNEW$cluster)),
             data=geoNEW, alpha=0.5, size=0.5) + labs(color='') + ggtitle("Clusters of towns based on merged Dataset predictors")
newDat_geo

```
#Multi-Dimensional Scaling for Merged Dataset
```{r}
NewDat_PCA_with_dep <- cbind(geoNEW[1], newDat_pca_scores)
# produces a list with covariance matrix
# for each region as component:
NewDat_PCA_var <- tapply(1:nrow(NewDat_PCA_with_dep), NewDat_PCA_with_dep$region, 
                     function(i) var(NewDat_PCA_with_dep[i,2:4]))


#remove regions with null values
NewDat_PCA_var <- NewDat_PCA_var[-c(11, 12, 15, 19, 20, 28)]
#check
#str(firms_var)



#create dataframe with the counts of towns per region
regions_count <- data.frame(table(firms_with_dep$region))
regions_count_sub<-regions_count[!(regions_count$Freq==0 | regions_count$Freq==1),]
#check total sum
row.names(regions_count_sub) <- 1:nrow(regions_count_sub)
sum(regions_count_sub$Freq)



# initializes common covariance matrix var:
S <- regions_count_sub$Freq[1] * as.matrix(NewDat_PCA_var[[1]])

# creates common covariance matrix S
s <- 0
for (v in c(1:5)) S <- S + regions_count_sub$Freq[v] * as.matrix(NewDat_PCA_var[[v]]) 
S <- S / 5024

# finds center of each variable (micro, etc)
# for each region
NewDat_cen <- tapply(1:nrow(NewDat_PCA_with_dep), NewDat_PCA_with_dep$region, 
    function(i) apply(NewDat_PCA_with_dep[i,-1], 2, mean))
NewDat_cen
NewDat_cen <- NewDat_cen[-c(11, 12, 15, 19, 20, 28)]
str(NewDat_cen)

# create a matrix out of each components, each
# mean measurement for all variables by region
NewDat_cen <- matrix(unlist(NewDat_cen), 
    nrow = length(NewDat_cen), byrow = TRUE)
NewDat_cen

# compute the mahalanobis distances:
NewDat_mah <- apply(NewDat_cen, 1, 
    function(cen) mahalanobis(NewDat_cen, cen, S))
NewDat_mah



# run it on two dimensions
NewDat_mds <- cmdscale(NewDat_mah)

# draw a scatterplot of two-dimensional solution
# from classical MDS applied to Mahalanobis
# distances:
lim <- range(NewDat_mds) * 1.2
plot(NewDat_mds, xlab = "Coordinate 1", 
     ylab = "Coordinate 2",
     xlim = lim, ylim = lim, type = "n")
text(NewDat_mds, labels = levels(NewDat_PCA_with_dep$region), 
     cex = 0.5)

# But here, as the two-dimensional fit may not
# explain what is needed to represent the observed
# distances, we shall investigate the solution 
# in a little more detail using the "minimum
# spanning tree".

# The minimum spanning tree is defined as follows. 
# Suppose n points are given (possibly in many
# dimensions). Then a tree spanning these points 
# (that is, a spanning tree) is any set of straight 
# line segments joining pairs of points such that
# - 0 closed loops occur,
# - every point is visited at least one time, and
# - the tree is connected (has paths between each
# pair of points)

# We use function mst() from package ape to plot
# minimum spanning tree on the two-dimensional
# scaling solution:

#
# run classic MDS function on distances
NewDat_mds <- cmdscale(NewDat_mah, k = nrow(NewDat_mah) - 1, 
         eig = TRUE)

#only 5 of the first 21 eigenvalues are > 0
# x <- firms_mds$points[,1]
# y <- firms_mds$points[,2]
# st <- mst(firms_mah)
# plot(x, y, xlab = "Coordinate 1", 
#      ylab = "Coordinate 2",
#      xlim = range(x)*1.2, type = "n")
# for (i in 1:nrow(firms_mah)) {
#   w1 <- which(st[i, ] == 1)
#   segments(x[i], y[i], x[w1], y[w1])
# }
# text(x, y, labels = colnames(firms_mah), 
#      cex = 1.5)
```


new trial
```{r}
 
# geo styling
g <- list(
  scope = 'france',
  showland = TRUE,
  landcolor = plotly::toRGB("grey86"),
  subunitcolor = plotly::toRGB("white"),
  countrycolor = plotly::toRGB("black"),
  showlakes = TRUE,
  lakecolor = plotly::toRGB("white"),
  showsubunits = TRUE,
  showcountries = TRUE,
  resolution = 50,
  projection = list(
    type = 'conic conformal',
    rotation = list(lon = 1)
  ),
  lonaxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(-5, 10),
    dtick = 5
  ),
  lataxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(40, 55),
    dtick = 5
  )
)
 
 
p <- plotly::plot_geo(geoNEW, lat = ~latitude, lon = ~longitude) %>%
  plotly::add_markers(
    # text = ~paste(town_name, paste("Firms tot:", total), 
    #               paste("Population:", total_population),
    #               paste("Mean salary:", sal_general), 
    #               paste("Salary ratio FvsM:", round(salary_ratio_FvsM, 4)),
    #               sep = "<br />"),
    color = ~cluster,  size = I(2), hoverinfo = "text", opacity = 0.8 #, colors = 'Purples'
  ) %>%
  plotly::colorbar(title = "Total population<br />in 2014") %>%
  plotly::layout(title = 'Geographic information about France', geo = g)
p
 
 
```

## Supervised

Function for interactive plotting:
```{r}
# This function is used to obtain interactive regression diagnostic plots
RegressionPlots <- function(fit, textlabels, robust=F){
  
  # number of units
  n = length(fit$fitted.values)
  
  # original response
  y = fit$y
  
  # Extract fitted values from lm() object
  Fitted.Values <-  fit$fitted.values
  
  # Extract residuals from lm() object
  Residuals <-  fit$residuals
  
  if (robust == F){
    # Extract standardized residuals from lm() object
    Standardized.Residuals <- MASS::stdres(fit)  
    
    # Calculate Leverage
    Leverage <- lm.influence(fit)$hat
    
    # text for the labels of the plots
    tt = "OLS"
  } else{
    # standardized residuals based on a robust scale estimate
    Standardized.Residuals <- fit$residuals/fit$scale
    
    # get the robust distances based on mahalanobis distance from the robust output 
    # (it has to be specified in the call to lmrob using: control = lmrob.control(compute.rd = T))
    Leverage <- fit$MD
    
    # text for the labels of the plots
    tt = "Robust"
  }
  
  # Extract fitted values for lm() object
  Theoretical.Quantiles <- qqnorm(Residuals, plot.it = F)$x
  
  # Create data frame 
  # Will be used as input to plot_ly
  regMat <- data.frame(Fitted.Values, 
                       Residuals, 
                       Standardized.Residuals, 
                       Theoretical.Quantiles,
                       Leverage, 
                       textlabels)
  
  # Plot using Plotly
  
  # text info
  t <- list(
    family = "sans serif",
    size = 14,
    color = toRGB("grey50"))
  
  
  # Plot 1: Fitted vs Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Residuals)
  
  plt1 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 

    layout(title = paste(tt, " Residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Residuals"))
  
  # Plot 2: Fitted vs Standardized Residuals
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, Standardized.Residuals)
  
  plt2 <- regMat %>% 
    plot_ly(x = Fitted.Values, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
            text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
    
    layout(title = paste(tt, " Standardized residuals vs Fitted Values"), plot_bgcolor = "#e6e6e6",
           xaxis = list(title = "Fitted Values"),
           yaxis = list(title = "Standardized residuals"))  

  # Plot 3: Residuals index
  plt3 <- regMat %>% 
    plot_ly(x = 1:n, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', 1:n,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_segments(x = -5, xend = n+5, y = 0, yend = 0, mode = "line", inherit = F, 
                 name = "Null line", line = list(width = 2)) %>%
    
    layout(title = paste(tt, " Standardized Residuals' index"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Index"),
             yaxis = list(title = "Standardized.Residuals")) 

  
  # Plot 4: QQ Pot
  plt4 <- regMat %>% 
    plot_ly(x = Theoretical.Quantiles, y = Standardized.Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Theoretical.Quantiles,
                         '</br> y: ', Standardized.Residuals)) %>% 
    
    add_trace(x = Theoretical.Quantiles, y = Theoretical.Quantiles, type = "scatter", 
              mode = "lines", name = "Theoretical", line = list(width = 2), 
              inherit = F) %>%
    
    layout(title = paste(tt, " Q-Q Plot"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Normal theoretical quantiles"),
             yaxis = list(title = "Data quantiles"))  

  
  # Plot5: Residuals vs Leverage
  # For scatter plot smoother
  LOESS <- loess.smooth(Leverage, Residuals)
  
  plt5 <- regMat %>% 
    plot_ly(x = Leverage, y = Residuals, 
            type = "scatter", mode = "markers", hoverinfo = "text", 
            name = "Data", marker = list(size = 10, opacity = 0.5), 
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Leverage,
                         '</br> y: ', Residuals)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              line = list(width = 2), inherit = F) %>%
    
    layout(title = paste(tt, " Leverage vs Residuals"), plot_bgcolor = "#e6e6e6", 
             xaxis = list(title = "Leverage values"),
             yaxis = list(title = "Residuals"))  


  # Plot 6: Fitted vs response
  # For scatter plot smoother
  LOESS <- loess.smooth(Fitted.Values, y)
  
  plt6 <- regMat %>% 
    plot_ly(x = Fitted.Values, y =y, 
            type = "scatter", mode = "markers", hoverinfo = "text",
            name = "Data", marker = list(size = 10, opacity = 0.5),
           text = paste('</br> Unit: ', 1:n,
                         '</br> Town: ', as.character(textlabels),
                         '</br> x: ', Fitted.Values,
                         '</br> y: ', y)) %>% 
    
    add_trace(x = LOESS$x, y = LOESS$y, type = "scatter", mode = "lines", name = "Smooth",
              inherit = F, line = list(width = 2)) %>% 
  
    layout(title = paste(tt, " Fitted Values vs response"), plot_bgcolor = "#e6e6e6",
             xaxis = list(title = "Fitted Values"),
             yaxis = list(title = "Response values"))

  # # Plot 7: MISSING: sqrt(abs(Residuals))
  # # For scatter plot smoother
  # LOESS2 <- loess.smooth(Fitted.Values, Root.Residuals)
  # 
  # plt3 <- regMat %>% 
  #   plot_ly(x = Fitted.Values, y = Root.Residuals, 
  #           type = "scatter", mode = "markers", hoverinfo = "text", 
  #           name = "Data", marker = list(size = 10, opacity = 0.5),
  #           text = paste('town: ', as.character(textlabels))) %>%
  #   
  #   add_trace(x = LOESS2$x, y = LOESS2$y, type = "scatter", mode = "lines", name = "Smooth",
  #             line = list(width = 2), inherit = F) %>% 
  #   
  #   layout(title = "Scale Location", plot_bgcolor = "#e6e6e6")
  plt = list(plt1, plt2, plt3, plt4, plt5, plt6)
  return(plt)
}

```


The focus of this part is on the delta between males and females salaries.
```{r linear model for delta M-F salary, fig.keep='all'}

# response variable
y = newDat$sal_Males - newDat$sal_Females
# original response variable histogram
ggplot(data=as.data.frame(y), aes(y)) +
  geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) +
  geom_density(col="black") +
  labs(x="Male-Female salary", y="Density") +
  ggtitle("Original response variable") +
  theme(plot.title = element_text(hjust = 0.5))

x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)
# merge variables
mod = cbind.data.frame(y, x)

# scatter plot matrix
# pairs(mod)
# correlations
corrplot(cor(mod), method = "number", title = "Correlation matrix",
         diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black")  # , mar=c(0,0,1.5,0)
plot(newDat$sal_M_26_50 - newDat$sal_F_26_50,y)
```

Model fitting using OLS:
```{r}

# OLS
ols = lm(y ~ ., data = mod, y=T)
summary(ols)

# diagnostic plots
# layout(matrix(1:6, nr = 2), height = c(2, 1))
RegressionPlots(ols, newDat$town_name, robust = F)

```

Model fitting using MM estimator:
```{r}

# robust fit using MM estimator
mm = lmrob(y~., data = mod, y=T, setting = "KS2014", control = lmrob.control(compute.rd = T))
summary(mm)
# outliers identified
outl = mm$rweights < 2e-05

# diagnostic plots
# layout(matrix(1:6, nr = 2), height = c(2, 1))
# plot(mm, cex = 0.6, id.n = 10, labels.id = newDat$town_name, sub.caption="MM")
RegressionPlots(mm, newDat$town_name, robust = T)
outl = mm$rweights < 2e-05

```

Specify the model for model selection:
```{r}

# copy of the previous options
y = newDat$sal_Males - newDat$sal_Females
x = cbind.data.frame(pop = newDat$total_population, 
                     firms = newDat$total,
                     # small_firms = newDat$small,
                     # small_firms = newDat$medium,
                     # small_firms = newDat$large,
                     ratio_pop_firms = newDat$total/newDat$total_population,
                     # delta_18_25 = newDat$sal_M_18_25 - newDat$sal_F_18_25,
                     # delta_26_50 = newDat$sal_M_26_50 - newDat$sal_F_26_50,
                     # delta_51plus = newDat$sal_M_51plus - newDat$sal_F_51plus,
                     delta_exec = newDat$sal_M_executive - newDat$sal_F_executive,
                     delta_midMan = newDat$sal_M_midManager - newDat$sal_F_midManager,
                     delta_empl = newDat$sal_M_employee - newDat$sal_F_employee,
                     delta_worker = newDat$sal_M_worker - newDat$sal_F_worker)

# merge variables
mod = cbind.data.frame(y, x)
# merge variables excluding ouliers
# mod = mod[!outl,]

# add transformations on the predictors
mod = cbind.data.frame(mod, mod[,2:ncol(mod)]^2, mod[,2:ncol(mod)]^3)
namesdataBS = names(mod)
lll = ((dim(mod)[2]-1)/3)
# for (i in 1:lll+1){
#   str = namesdataBS[i]
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1]
# }
namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll), sep="")
namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll), sep="")
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll))
names(mod) = namesdataBS

```


Use 10-folds cross validation on Elastic-net:
```{r}

# initialize list to contain the variables selected and the counter
var_selec = list()
i = 1

# Elastic net
x = as.matrix(mod[, 2:ncol(mod)])
y = mod[, 1]
par(mfrow =c(3,2))
par(oma=c(0,0,2,0))
alp = c(0, 0.2, 0.4, 0.6, 0.8, 1)
for (j in alp){
  # set.seed (3)
  cv.out = cv.glmnet(x, y, alpha = j, standardize.response = T ) # standardize.response = T , parallel = T 
  plot(cv.out)
  title(paste("alpha = ", j), line = 0.3)
  tmp_coeffs <- coef(cv.out, s = "lambda.min") # cv.out$lambda.1se 
  var_selec[[i]] = data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
  i = i + 1
}
mtext(expression("Best lambda for delta gender salary using elastic-net and 10-folds CV"), outer=TRUE,  cex=1, line=0.2)

# compare the results
print(var_selec)

# Manual evaluation
# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE',
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 

```

Add a second step OLS estimate on the variables identified by LASSO:
```{r}

for (i in 1:6){
  mod_red = mod[,colnames(mod) %in% var_selec[[i]]$name ] 
  mod_red = cbind.data.frame(y, mod_red)
  olsRed = lm(y ~ ., data = mod_red, y=T)
  print("--------------------------------------------")
  print(paste("Elastic net based on alpha = ", alp[i]))
  print("--------------------------------------------")
  print(summary(olsRed))
}

# # OLS
# ols = lm(y ~ ., data = mod, y=T)
# summary(ols)
# 
# # diagnostic plots
# # layout(matrix(1:6, nr = 2), height = c(2, 1))
# RegressionPlots(ols, newDat$town_name, robust = F)

```


Using best subset selection.

Function to plot the results
```{r}

plot.regsubsets2 <-  
  function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
     "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
  { 
    obj <- x 
    lsum <- summary(obj) 
    par(mar = c(7, 5, 6, 3) + 0.1) 
    nmodels <- length(lsum$rsq) 
    np <- obj$np 
    propscale <- FALSE 
    sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
                     nomatch = 0) 
    if (sscale == 0)  
      stop(paste("Unrecognised scale=", scale)) 
    if (propscale)  
      stop(paste("Proportional scaling only for probabilities")) 
    yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
    up <- switch(sscale, -1, -1, 1, 1) 
    index <- order(yscale * up) 
    colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
                                                           1e-04)), -log(pmax(yscale, 1e-04))) 
    image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
                       NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
          x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
          col = col) 
    laspar <- par("las") 
    on.exit(par(las = laspar)) 
    par(las = 2) 
    axis(1, at = 1:np, labels = labels, ...) # I modified this line 
    axis(2, at = 1:nmodels, labels = signif(yscale[index], 2), ...) 
    # axis(2,cex.axis=0.01)
    if (!is.null(main))  
      title(main = main) 
    box() 
    invisible(NULL) 
    
  } 
```

BSS results:
```{r}

# best subset selection
best.sub = regsubsets(y ~ ., data = mod, nvmax = ncol(mod))
best.sub.summary = summary(best.sub)
# manual plotting
par(mfrow =c(2,2))
# rsq
plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l")
ind_Rsq = which.max(best.sub.summary$rsq)
points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20)
# adjRsq
plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l")
ind_adjRsq = which.max(best.sub.summary$adjr2)
points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20)
# Cp
plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l")
ind_Cp = which.min(best.sub.summary$cp)
points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20)
# bic
plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l")
ind_bic = which.min(best.sub.summary$bic)
points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20)
mtext("Best subset selection for delta gender salary", outer=TRUE,  cex=1.2, line=-1.5)

# built-in plots
par(mfrow=c(1,1))
plot.regsubsets2(best.sub, scale = "r2", cex.axis = 0.6, main = "Best subset selection using R squared")
plot.regsubsets2(best.sub, scale = "adjr2", cex.axis = 0.6, main = "Best subset selection using adjusted R squared")
plot.regsubsets2(best.sub, scale = "Cp", cex.axis = 0.6, main = "Best subset selection using Mallows' Cp")
plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.6, main = "Best subset selection using BIC")

# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5)

# retrieve the model with min BIC
coefficients(best.sub, which.min(best.sub.summary$bic))
nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
nnn <- gsub(x = nnn,
            pattern = "`",
            replacement = "")
# apply OLS
mod_bic = mod[,colnames(mod) %in% nnn ] 
mod_bic = cbind.data.frame(y, mod_bic)
ols_bic = lm(y ~ ., data = mod_bic, y=T)
summary(ols_bic)

```



France map:
```{r}
# require dev version
# devtools::install_github('ropensci/plotly')
# library(plotly)

newDat$firms_over_pop = newDat$total/newDat$total_population
m <- list(colorbar = list(title = "Total Inches"))

# geo styling
g <- list(
  scope = 'france',
  showland = TRUE,
  landcolor = plotly::toRGB("grey86"),
  subunitcolor = plotly::toRGB("white"),
  countrycolor = plotly::toRGB("black"),
  showlakes = TRUE,
  lakecolor = plotly::toRGB("white"),
  showsubunits = TRUE,
  showcountries = TRUE,
  resolution = 50,
  projection = list(
    type = 'conic conformal',
    rotation = list(lon = 1)
  ),
  lonaxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(-5, 10),
    dtick = 5
  ),
  lataxis = list(
    showgrid = TRUE,
    gridwidth = 0.5,
    range = c(40, 55),
    dtick = 5
  )
)


p <- plotly::plot_geo(newDat, lat = ~latitude, lon = ~longitude) %>%
  plotly::add_markers(
    text = ~paste(town_name, paste("Firms tot:", total), 
                  paste("Population:", total_population),
                  paste("Mean salary:", sal_general), 
                  paste("Salary ratio FvsM:", round(salary_ratio_FvsM, 4)),
                  sep = "<br />"),
    color = ~firms_over_pop,  size = I(2), hoverinfo = "text", opacity = 0.8 #, colors = 'Purples'
  ) %>%
  plotly::colorbar(title = "Total population<br />in 2014") %>%
  plotly::layout(title = 'Geographic information about France', geo = g)
p

```



# FIRMS: old code

## PCA 

PCA on firms data:

```{r PCA}

# pairs(firms[, 3:8])
# 
# salary_NEW = salary[, 3:ncol(salary)]
# # colnames(salary_NEW) = 1:32
# corrplot(cor(salary_NEW), method = "circle", title = "Correlation matrix for salary", 
#          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
#          tl.col = "black", mar=c(0,0,1,0)) 
# 
# 
# # firms_clean <- firms[firms$micro < 20000 & firms$large < 200,]
# firms_clean <- firms[firms$micro > 1 & firms$micro < 2500 & 
#                      firms$small > 1 & firms$small < 800 & 
#                      firms$medium > 1 & firms$medium < 500 & 
#                      firms$large > 1 & firms$large < 200 &
#                      firms$null > 1 & firms$null < 2000 ,]
# myPr <- prcomp(firms_clean[, 4:8], scale = TRUE)
# #plot(scale(firms_clean$micro), scale(firms_clean$large))
# #mean(firms_clean$micro)
# #mean(firms_clean$large)
# myPr
# summary(myPr)
# plot(myPr, type = "l")
# # biplot(myPr, scale = 0)
# #extract PC scores...
# str(myPr)
# #myPr$x #checking principal component scores
# firms2 <- cbind(firms_clean, myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])
# 
# # using ggbiplot
# ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
#   ggtitle("Biplot for the firms's size") +
#   theme(plot.title = element_text(hjust = 0.5))
```

Modified PCA [Luca]:
```{r PCA 2}

# get Paris in order to exclude it in the following
Paris = which.max(firms$total)

# Original scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], gap=0, main = "Scatter matrix for firms")

# Correlation matrix
corrplot(cor(firms[-Paris, 3:8]), method = "number", title = "Correlation matrix for firms", 
         diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
         tl.col = "black", mar=c(0,0,1.5,0))

# Log scale
# Scatter matrix
# pairs(firms[-Paris, 3:8], log = "xy", gap=0, main = "Scatter matrix for firms in log scale")

# Correlation matrix
firmsLog = log(firms[, 3:8]) 
firmsLog[firmsLog == -Inf] = 0
corrplot(cor(firmsLog), method = "number", title = "Correlation matrix for firms  in log scale", 
          diag = F, tl.cex=1, #col = colorRampPalette(c("red","green","navyblue"))(100))
          tl.col = "black", mar=c(0,0,1.5,0))

# PCA
myPr <- prcomp(firms[-Paris, 3:8], scale = TRUE)
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0)
#extract PC scores...
str(myPr)
#myPr$x #checking principal component scores
# firms2 <- cbind(firms[-Paris, 3:8], myPr$x[, 1:2])
# head(firms2)
# #plot with ggplot...
# ggplot(firms2, aes(PC1, PC2)) +
#   stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
#   geom_point(shape = 21, col = "black")
# # correlations between variables and PCs...
# cor(firms_clean[, 4:8], firms2[,9:10])

# using ggbiplot
ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 5.5, varname.adjust = 1) +
  ggtitle("Biplot for the firms's size") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Cluster Analysis 

```{r clustering}
# firms_sampled <- firms[1:1000, ] # subsampling
# head(firms_sampled)
# firms_scaled <- scale(firms_sampled[, 3:8]) #scaling the data
# 
# head(firms_scaled)
# 
# firms_truncated <- firms_sampled[, 4:8]
# head(firms_truncated)
# plot(firms_sampled)
# # K-means clustering...
# 
# 
# fitK_scaled <- kmeans(firms_scaled, 4) 
# head(fitK_scaled)
# 
# fitK <- kmeans(firms_truncated, 5)
# head(fitK)
# str(fitK)
# plot(firms_sampled, col = fitK$cluster) #plotting data colored according to cluster membership
# 
# #choosing K---
# k <- list()
# for(i in 1:10){
#   k[[i]] <- kmeans(firms_truncated, i)
# }
# head(k)
# 
# betweenss_totalss <- list()
# for(i in 1:10){
#   betweenss_totalss[[i]] <- k[[i]]$betweenss/k[[i]]$totss
# }
# plot(1:10, betweenss_totalss, type ="b",
#      ylab = "Between SS / Total SS", xlab = "Clusters (k)") #calculating and plotting between SS to total SS ratio against number of clusters
# 
# for(i in 1:5) {
#   plot(firms, col = k[[i]]$cluster) #plotting data based on membership to clusters for k = 1 to 5 clusters
# }
# head(fitM)
# fitM <- Mclust(firms_truncated)
# plot(fitM)
# #Model-based clustering using mclust
# 
# head(clusters)
# plot(firms_sampled, col = clusters) # as we can see, it performs quite similar to the K-means 
# rect.hclust(fitH, k = 5, border = "red") # visualising dendrogam cut at k =5
# clusters <- cutree(fitH, 5)  # vector with cluster membership for each observation
# plot(fitH)
# d <- dist(firms_truncated)
# fitH <- hclust(d, "ward.D2")
# 
# #Hierarchical clustering---
# 

```

# SALARY: old code
## Linear models

First we perform some easy task
fitting a regression model to predict the salaries of people in age 26-50 using as regressor 51+ years:

```{r exLinMod, fig.keep='all'}
# # fit and show OLS estimate
# plot(salary$sal_26_50 ~ salary$sal_51plus)
# fit_LM_26_50 = lm(salary$sal_26_50 ~ salary$sal_51plus, data = salary) 
# abline(fit_LM_26_50, lwd=3, col="red")
# # diagnostics
# summary(fit_LM_26_50)
# plot(fit_LM_26_50)
# 
# # Same as before but adding polynomials which are evauated using 10-folds cross validation
# set.seed(1)
# # k-Fold Cross-Validation
# cv.err.K = rep(0, 5)
# cv.err.K = rbind(cv.err.K, cv.err.K)
# for (i in 1:5){
#   fit_LM_26_50.K = glm(sal_26_50 ~ poly(sal_51plus, i), data = salary)
#   cv.err.K[,i] = cv.glm(salary, fit_LM_26_50.K, K = 10)$delta[1]
# }
# # plotting results
# plot(cv.err.K[1,], type = 'l', col = 'red', xlab = "Polynomials' order", 
#      ylab = "10-folds CV", main = "CV and adjusted CV for different polynomials")
# lines(cv.err.K[2,], col = 'green')
# points(which.min(cv.err.K), cv.err.K[1, which.min(cv.err.K)], col = "red", cex=2, pch=20)
# legend('topright', legend = c('CV', 'Adj. CV'), col = c('red', 'green'), pch = 10)

```


Predicting salary for young people using Elastic NEt with CV and 10-folds CV. The predictors are most of the original variables and their 2nd, 3rd order polynomials and log tranformation:

```{r}
# # funtion to plot BSS
# plot.regsubsets2 <-  
#   function (x, labels = obj$xnames, main = NULL, scale = c("bic",  
#                                                            "Cp", "adjr2", "r2"), col = gray(seq(0, 0.9, length = 10)), ...)  
#   { 
#     obj <- x 
#     lsum <- summary(obj) 
#     par(mar = c(7, 5, 6, 3) + 0.1) 
#     nmodels <- length(lsum$rsq) 
#     np <- obj$np 
#     propscale <- FALSE 
#     sscale <- pmatch(scale[1], c("bic", "Cp", "adjr2", "r2"),  
#                      nomatch = 0) 
#     if (sscale == 0)  
#       stop(paste("Unrecognised scale=", scale)) 
#     if (propscale)  
#       stop(paste("Proportional scaling only for probabilities")) 
#     yscale <- switch(sscale, lsum$bic, lsum$cp, lsum$adjr2, lsum$rsq) 
#     up <- switch(sscale, -1, -1, 1, 1) 
#     index <- order(yscale * up) 
#     colorscale <- switch(sscale, yscale, yscale, -log(pmax(yscale,  
#                                                            1e-04)), -log(pmax(yscale, 1e-04))) 
#     image(z = t(ifelse(lsum$which[index, ], colorscale[index],  
#                        NA + max(colorscale) * 1.5)), xaxt = "n", yaxt = "n",  
#           x = (1:np), y = 1:nmodels, xlab = "", ylab = scale[1],  
#           col = col) 
#     laspar <- par("las") 
#     on.exit(par(las = laspar)) 
#     par(las = 2) 
#     axis(1, at = 1:np, labels = labels, ...) # I modified this line 
#     axis(2, at = 1:nmodels, labels = signif(yscale[index], 2)) 
#     if (!is.null(main))  
#       title(main = main) 
#     box() 
#     invisible(NULL) 
#   } 
# 
# set.seed(2018)
# 
# # load the data
# ind = salary$sal_18_25 < 11.5
# y = salary$sal_18_25[ind]
# x = cbind.data.frame(salary$sal_26_50, salary$sal_51plus, 
#                      salary$sal_general,  salary$sal_executive, salary$sal_midManager, 
#                      salary$sal_employee, salary$sal_worker, 
#                      salary$sal_Males, salary$sal_Females)
# x = x[ind, ]
# yx = cbind.data.frame(y, x)
# 
# # original response variable histogram
# ggplot(data=salary, aes(salary$sal_18_25)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # response variable histogram (excluding outliers)
# ggplot(data=salary[ind,], aes(y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 35) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Original response variable excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # after sampling to avoid serial correlation
# indd = sample(1:length(y), round(length(y)*0.4))
# yx = yx[indd,]
# ggplot(data=salary[ind,][indd,], aes(y[indd])) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 30) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Sampled response variable and excluding outliers") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# # correlations
# # corrplot(cor(yx), method = "number", title = "Correlation matrix for salary",
# #          diag = T, tl.cex=0.5, type="lower", #col = colorRampPalette(c("red","green","navyblue"))(100))
# #          tl.col = "black")  # , mar=c(0,0,1.5,0)
# 
# # scatter matrix
# # pairs(yx, gap=0, main = "Scatter matrix for some variables in salary")
# # manually remove outliers in mid mangager
# summary(yx)
# inddd = yx$`salary$sal_midManager` < 20
# yx = yx[inddd,]
# # final dataset
# pairs(yx, gap=0, main = "Scatter matrix for the variables in the model")
# 
# # create final dataframe
# require(leaps) 
# # assign names for predictors transformation
# namesdataBS = names(yx)
# yx = cbind.data.frame(yx, yx[,2:ncol(yx)]^2, yx[,2:ncol(yx)]^3, log(yx[,2:ncol(yx)]))
# dim(yx)
# lll = ((dim(yx)[2]-1)/4) 
# for (i in 1:lll+1){ 
#   str = namesdataBS[i] 
#   namesdataBS[i] = regmatches(str, regexpr("_", str), invert = TRUE)[[1]][-1] 
# } 
# namesdataBS[(lll+2):(lll*2+1)] = paste(namesdataBS[1:lll+1], rep("^2", lll)) 
# namesdataBS[(lll*2+2):(lll*3+1)] = paste(namesdataBS[1:lll+1], rep("^3", lll)) 
# namesdataBS[(lll*3+2):(lll*4+1)] = paste(namesdataBS[1:lll+1], rep("log", lll)) 
# names(yx) = namesdataBS 
# 
# # Elastic net
# x = as.matrix(yx[, 2:ncol(yx)])
# y = yx[, 1]
# par(mfrow =c(3,2))
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   cv.out = cv.glmnet(x, y, alpha = j)
#   plot(cv.out)
#   title(paste("alpha = ", j), line = 2.3)
# }
# mtext(expression("Best lambda for salary 18-25 using elastic and 10-folds CV"), outer=TRUE,  cex=1, line=-1.4) 
# 
# 
# # split the date leaving the 20% for CV
# par(mfrow =c(1,1))
# train = sample(1:nrow(x), floor(nrow(x)*0.8))
# test = -train
# y.test = y[test]
# x = as.matrix(x)
# itercol = 1
# for (j in c(0, 0.2, 0.4, 0.6, 0.8, 1)){
#   # set.seed (3)
#   lasso.mod = glmnet(x[train,], y[train], alpha = j, thresh = 1e-10)
#   err.i = rep("NA", length(lasso.mod$lambda))
#   for (i in 1:length(lasso.mod$lambda)){
#     lasso.pred = predict(lasso.mod, s = lasso.mod$lambda[i], newx = x[test,], alpha = j)
#     err.i[i] = mean((lasso.pred - y.test)^2)
#   }
#   if (itercol == 1){
#     plot(log(lasso.mod$lambda), err.i, xlab = 'log Lambda', ylab = 'test set MSE', 
#          main = 'Test MSE among different Lambdas', type = "b", col = itercol)
#   } else{
#     lines(log(lasso.mod$lambda), err.i, type = "b", col = itercol)
#   }
#   bestlam = which.min(err.i)
#   points(log(lasso.mod$lambda)[bestlam], err.i[bestlam], col = 3, cex=2, pch=20)
#   itercol = itercol +1
# }
# 
# 
# # best subset selection
# best.sub = regsubsets(y ~ ., data = yx, nvmax = ncol(yx)) 
# best.sub.summary = summary(best.sub) 
# # manual plotting 
# par(mfrow =c(2,2))
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for salary 18-25", outer=TRUE,  cex=1.2, line=-2.5) 
# # built-in plots 
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "adjr2") 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.9) 
# mtext("Best subset selection for salary 18-25 using BIC", outer=TRUE,  cex=1.4, line=-3.5) 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# names(coefficients(best.sub, 10))
# # nnn = names(coefficients(best.sub, which.min(best.sub.summary$bic)))
# 


```



ANOVA model for salary:
```{r}
# 
# # create response variable
# sal_y = c(salary$sal_M_18_25, salary$sal_M_26_50, salary$sal_M_51plus,
#           salary$sal_M_executive, salary$sal_M_midManager, salary$sal_M_employee, salary$sal_M_worker,
#           salary$sal_F_18_25, salary$sal_F_26_50, salary$sal_F_51plus,
#           salary$sal_F_executive, salary$sal_F_midManager, salary$sal_F_employee, salary$sal_F_worker)
# 
# n_sal_y = length(sal_y)             # length response variable
# n_cat = length(salary$sal_M_18_25)  # length of each category (i.e., original vectors)
# 
# # create sex dummy variable, 1 for males and 0 for females
# sal_sex = rep(0, n_sal_y)   # full regressors
# sal_sex[1:n_sal_y/2] = 1    # assign males
# 
# # create age dummy variables, 18-25 years old is the base case
# sal_age = cbind(rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # 26-50 y.o.
# sal_age[(n_cat+1):(n_cat*2), 1] = 1     # males
# sal_age[(n_cat*8+1):(n_cat*9), 1] = 1   # females
# # 51+ y.o.
# sal_age[(n_cat*2+1):(n_cat*3), 2] = 1   # males
# sal_age[(n_cat*9+1):(n_cat*10), 2] = 1  # females
# 
# # create job type dummy variables, worker is the base case
# sal_job = cbind(rep(0, n_sal_y), rep(0, n_sal_y), rep(0, n_sal_y)) # full regressors
# # executives
# sal_job[(n_cat*3+1):(n_cat*4), 1] = 1     # males
# sal_job[(n_cat*10+1):(n_cat*11), 1] = 1   # females
# # middle managers
# sal_job[(n_cat*4+1):(n_cat*5), 2] = 1     # males
# sal_job[(n_cat*11+1):(n_cat*12), 2] = 1   # females
# # employee
# sal_job[(n_cat*5+1):(n_cat*6), 3] =   1   # males
# sal_job[(n_cat*12+1):(n_cat*13), 3] = 1   # females
# 
# # final data set 
# data_ANOVA = cbind.data.frame(response = sal_y, sex = sal_sex, age = sal_age, job = sal_job)
# names(data_ANOVA)
# # show regressors' shape
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# # sub sample to avoid correlation
# set.seed(20)
# subs = sample(1:n_sal_y, size = round(n_sal_y*0.2))
# data_ANOVA = data_ANOVA[subs,]
# # show randomized data
# imagemat(data_ANOVA[,-1], xaxt = "n", main = "Factors for ANOVA")  
# axis(1, at=1:6, labels=c("Sex", "Age 26-50", "Age 51+", "Execut.", "Mid.Man.", "Empl.")) 
# box()
# 
# # plot response variable
# hist(data_ANOVA[,1], 30)
# hist(log(data_ANOVA[,1]), 30)
# hist(sqrt(data_ANOVA[,1]), 30)
# hist(data_ANOVA[,1]^-1, 30)
# sal_y = data_ANOVA[,1]^-1  # also suggested by Box-Cox transformation
# ggplot(data=data.frame(data_ANOVA[,1]), aes(sal_y)) + 
#   geom_histogram(aes(y =..density..), col="black", fill="blue", alpha = .3, bins = 40) + 
#   geom_density(col="black") + 
#   labs(x="salary^-1", y="Density") + 
#   ggtitle("Transformation of salary found using Box-Cox transformation") + 
#   theme(plot.title = element_text(hjust = 0.5)) 
# 
# sex = sal_sex[subs]
# age = sal_age[subs,]
# job = sal_job[subs,]
# # ANOVA model
# sal_ANOVA = lm(sal_y ~ sex + age + job + sex:age + sex:job)
# # names(sal_ANOVA$coefficients) = c("Males", "26-50", )
# summary(sal_ANOVA)
# anova((sal_ANOVA))
# 
# # box-cox transformation suggested to use y^-1
# boxcox(sal_ANOVA)
# title("Box-Cox transformation of the response")
# 
# # BSS
# best.sub = regsubsets((sal_y ~ sex + age + job + sex:age + sex:job), 
#                       data = data_ANOVA, nvmax = 12) 
# best.sub.summary = summary(best.sub) 
# # plots: manual plotting 
# par(mfrow =c(2,2)) 
# # rsq 
# plot(best.sub.summary$rsq , xlab="Number of Variables", ylab="Rsq", type="l") 
# ind_Rsq = which.max(best.sub.summary$rsq) 
# points(ind_Rsq, best.sub.summary$adjr2[ind_Rsq], col ="red", cex=2, pch=20) 
# # adjRsq 
# plot(best.sub.summary$adjr2 ,xlab="Number of Variables", ylab="Adjusted RSq", type="l") 
# ind_adjRsq = which.max(best.sub.summary$adjr2) 
# points(ind_adjRsq, best.sub.summary$adjr2[ind_adjRsq], col ="red", cex=2, pch=20) 
# # Cp 
# plot(best.sub.summary$cp ,xlab="Number of Variables", ylab="Cp", type="l") 
# ind_Cp = which.min(best.sub.summary$cp) 
# points(ind_Cp, best.sub.summary$cp[ind_adjRsq], col ="red", cex=2, pch=20) 
# # bic 
# plot(best.sub.summary$bic ,xlab="Number of Variables", ylab="bic", type="l") 
# ind_bic = which.min(best.sub.summary$bic) 
# points(ind_bic, best.sub.summary$bic[ind_bic], col ="red", cex=2, pch=20) 
# # mtext("My 'Title' in a strange place", line=15) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-2.5) 
# # existing plots
# par(mfrow=c(1,1)) 
# plot(best.sub, scale = "r2") 
# plot(best.sub, scale = "bic") 
# plot.regsubsets2(best.sub, scale = "bic", cex.axis = 0.7) 
# mtext("Best subset selection for ANOVA", outer=TRUE,  cex=1.2, line=-3.5) 
# plot(best.sub, scale = "Cp") 
# plot(best.sub, scale = "adjr2") 
# # retrieve the model with min BIC 
# coefficients(best.sub, which.min(best.sub.summary$bic))
# 
# 
# # group lasso
# # install.packages("gglasso")
# library(gglasso)
# # install.packages("RColorBrewer")
# library(RColorBrewer)
# # install.packages("zoo")
# library(zoo)
# grp = c(1,2,2,3,3,3,4,4,5,5,5)
# form <- model.matrix(sal_y ~ (sex + age + job)^2)
# head(form)
# form = form[,2:12]
# fit = gglasso(x=form,y=sal_y,group=grp,loss='ls')
# coef.mat=fit$beta
# 
# #Group1 enters the equation
# g1=max(which(coef.mat[1,]==0))
# #Group2 enters the equation
# g2=max(which(coef.mat[2,]==0))
# #Group3 enters the equation
# g3=max(which(coef.mat[4,]==0))
# #Group4 enters the equation
# g4=max(which(coef.mat[7,]==0))
# #Group5 enters the equation
# g5=max(which(coef.mat[9,]==0))
# #Coefficient Plot
# cols=brewer.pal(5,name="Set1")
# plot(fit$b0,main="Coefficient vs Step",
#      ylab="Intercept",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      xlim=c(-1,100),
#      ylim=c(0.076,max(fit$b0)+0.001),
#      type="l",lwd=4)
# grid()
# par(new=T)
# xx=c(g1,g2,g3,g4,g5)
# yy=c(fit$b0[g1],fit$b0[g2],fit$b0[g3],fit$b0[g4],fit$b0[g5])
# plot(x=xx,y=yy,pch=13,lwd=2,cex=2,col=cols[-1],
#      xlim=c(-1,100),ylim=c(0.076,max(fit$b0)+0.001),
#      xaxt='n',yaxt='n',xlab="",ylab="")
# lmda=round(fit$lambda[c(g1,g2,g3,g4,g5)],4)
# text(x=xx-0.005,y=yy+0.0001,labels=c("Group1","Group2","Group3","Group4","Group5","Group6"),pos=3,cex=0.7)
# text(x=xx-0.005,y=yy-0.0001,labels=paste("Lambda\n=",lmda),pos=1,cex=0.6)
# 
# # coefficient plot 2 (my version)
# cols=brewer.pal(5,name="Set1")
# plot(fit$beta[1,], main="Coefficients vs Lambda",
#      ylab="Coefficients",xlab="Step (decreasing Lambda)",
#      col=cols[1],
#      # xlim=c(-1,100),
#      ylim=c(min(fit$beta), max(fit$beta)),
#      type="l",lwd=4)
# for (j in 2:11){
#   lines(fit$beta[j,],
#         type="l",lwd=4, col=cols[j])
# }
# # plot legend once
# grid()
# par(new=T)
# legend('bottomleft', legend = paste("group ", 1:5), lty=1, col=cols[1:5], cex = 0.7,lwd=2)
# 
# #Cross Validation
# fit.cv=cv.gglasso(x=form,y=sal_y,group=grp,nfolds=10)
# plot(fit.cv, main="10-folds CV for groupwise Lasso in ANOVA")
# #Pick the best Lambda
# lmbda=fit.cv$lambda.1se
# (coefs=coef.gglasso(object=fit,s=lmbda))
# #At best lambda get coefficients and fitted values
# plt=sal_y-predict.gglasso(object=fit,newx=form,s=lmbda,type='link')
# plot(plt, ylab="residuals", xlab="index", main="Plot of residuals")
# abline(0, 0, col= "red")
# # matplot(plt,main="Predicted vs Actual",type='l',lwd=2,col=cols[c(1,2)]),
# #         ylab="Unemplyoment %",
# #         xlab="Time")
# grid()
# 


```


PCA for salary:
```{r}

myPr <- prcomp(salary[, 3:26], scale = TRUE)
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr, scale = 0, cex = 0.5)
str(myPr)
#myPr$x #checking principal component scores
salary2 <- cbind(salary, myPr$x[, 1:2])
head(salary2)
#plot with ggplot...
#require(ggplot2)
ggplot(salary2, aes(PC1, PC2)) + 
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) + 
  geom_point(shape = 21, col = "black")
# correlations between variables and PCs...
cor(salary[, 3:26], salary2[,27:28])

ggbiplot(myPr, obs.scale = 1, var.scale = 1, varname.size = 1, varname.adjust = 1) + 
  ggtitle("Biplot for the firms's size") + 
  theme(plot.title = element_text(hjust = 0.5)) 
  
```



# POP: old code


## Unsupervised learning 

Practise what has been done thoughout class.
```{r PCA population-Create clean dataset}
# # PCA trial 
# # create a clean dataset with only necessary variables inside.
# pca_pop_town <- ddply(population, .(CODGEO), function(population) {
#   data.frame(population$sex_ratio, population$dependency_ratio, population$aged_dependency_ratio, population$child_dependency_ratio)})
# 
# # rename the variables 
# names(pca_pop_town)[1:ncol(pca_pop_town)] <-
#   c("CODGEO",
#     "sex_ratio",
#     "dependency_ratio",
#     "aged_dep_ratio",
#     "child_dep_ratio")
# 
# summary(pca_pop_town)
```


```{r PCA population }
# PCA population
# set.seed(3)
# subs = sample(1:nrow(pca_pop_town), size = round(nrow(pca_pop_town)*0.2))
# summary(pca_pop_town)
# 
# pcapop <- prcomp(pca_pop_town[subs, 2:5])
# # pcapop$rotation
# 
# plot(pcapop, type = "l")
# biplot(pcapop, scale = 0, cex = 0.5)
# # fviz_pca_ind(pcapop) + labs(title="PCA", x="PC1", y="PC2")
# # scale_color_gradient2(low="blue", mid="white",high="red", midpoint=4)
# # fviz_pca_ind(pcapop, col.ind="contrib") + scale_color_gradient2(low="blue", mid="white", high="red", midpoint=4) + theme_minimal()
# str(pcapop)
# 
# ggbiplot(pcapop, obs.scale = 1, var.scale = 1, varname.size = 4.5, varname.adjust = 1) +
#   ggtitle("Biplot for dependency ratios") + 
#   theme(plot.title = element_text(hjust = 0.5))
# 
# #pcapop$x #checking principal component scores
# pca_pop_town2 <- cbind(pca_pop_town[subs,], pcapop$x[, 1:2])
# # head(pca_pop_town2)
```

```{r Plot PCA}
#plot with ggplot...
# ggplot(pca_pop_town2, aes(PC1, PC2)) +
#     stat_ellipse(geom = "polygon", col = "gray", alpha = 0.5) +
#     geom_point(shape = 21, col = "black")
```

```{r Proportion of variance explained & cumulative PVE}
# Proportion of variance explained & cumulative PVE
# pcapop$sdev
# pr.var=pcapop$sdev^2 
# pve=pr.var/sum(pr.var)
# pve
# 
# plot(pve, xlab="PCA", ylab="PVE", ylim=c(0,1), type='b')
# plot(cumsum(pve), xlab="PCA", ylab="Cumulative PVE", ylim=c(0,1), type='b')
```

```{r Kmean}
# kmean trial
# km.out=kmeans(pca_pop_town[subs,2:5], 5, nstart=50)
# plot(pca_pop_town[subs,2:5], col=(km.out$cluster), pch=20, cex=1)
```

```{r H-cluster}
#hierarchical clustering
# sd.data=scale(pca_pop_town)
# par(mfrow=c(1,3))
# data.dist=dist(sd.data)
# plot(hclust(data.dist), labels=pca_pop_town$CODGEO, main="complete", xlab="", sub="", ylab="")

# hc_pop=hclust(dist(pca_pop_town), method="complete")
# plot(hc_pop, labels=pca_pop_town$CODGEO, main="Complete Linkage", xlab="", ylab="", sub="", cex =.9)
```
